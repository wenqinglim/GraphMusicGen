{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31060737-6028-46dc-aa2a-133659cfbdae",
   "metadata": {},
   "source": [
    "# Train a model to generate structure of a piece (MELONS-inspired)\n",
    "\n",
    "1. Read structure dataset from POP909_structure\n",
    "2. Pre-process str into graph format\n",
    "3. Setup transformer model\n",
    "4. Train-val split, data loader\n",
    "5. Evaluate model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b181400b-6fb3-463b-966d-9ea578a735f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccf9ab62-b7d7-4a94-9fdb-1d2c3213741b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_path = \"POP909_structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d754b377-807f-495b-9476-c15a0ee36cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "label_paths = [\"human_label1\", \"human_label2\"]\n",
    "for folder in os.listdir(structure_path):\n",
    "    for label_path in label_paths:\n",
    "        try:\n",
    "            f = open(f\"{structure_path}/{folder}/{label_path}.txt\", \"r\")\n",
    "            # print(f.read())\n",
    "            labels.append(f.read())\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e56d4ea-6cce-430c-9881-7191eb709955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8A8A8B8C4C4b4b4x2A8B8C4C4C4C4X1o1\\n',\n",
       " 'i8A7A8B8C4C5b5b5A8B8C4C4C4C5o1\\n',\n",
       " 'i4A4A4B4B4C4C4C4D4x4B4B4C4C4C4D4X3\\n',\n",
       " 'i4A4A4B4B4C8C8x4B4B4C8C8X4\\n',\n",
       " 'i4A8A8B4C4b5x1b5A8B4x1C4C4\\n']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af654142-478d-4553-b81f-80db89181124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_string(s):\n",
    "    # This regex pattern matches a letter followed by one or more digits\n",
    "    pattern = re.compile(r'[a-zA-Z]\\d+')\n",
    "    # Find all matches in the string\n",
    "    matches = pattern.findall(s)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4269dc3a-ae35-4536-a820-94429e7e0598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i4', 'A4', 'A4', 'B4', 'B4', 'C8', 'C8', 'x4', 'B4', 'B4', 'C8', 'C8', 'X4']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_string(labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51f9ed7a-de88-430d-9841-bc9424cd349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_phrases = []\n",
    "for label in labels:\n",
    "    all_phrases.append(split_string(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32ef2418-1d51-47d2-8f52-c38c1e83798f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i8',\n",
       "  'A8',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'b4',\n",
       "  'b4',\n",
       "  'x2',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'X1',\n",
       "  'o1'],\n",
       " ['i8',\n",
       "  'A7',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C5',\n",
       "  'b5',\n",
       "  'b5',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C5',\n",
       "  'o1'],\n",
       " ['i4',\n",
       "  'A4',\n",
       "  'A4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'x4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'X3']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6fe7daf0-5a56-4074-8520-567520056d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_phrase_edge_type(prev_phrase, curr_phrase, prev_phrase_idx, curr_phrase_idx):\n",
    "    \"\"\"\n",
    "    Edge types:\n",
    "    1: Intro to Any\n",
    "    2: Any to Outro\n",
    "    3: Repeated phrase\n",
    "    4: Melody to Melody\n",
    "    5: Melody to Non-Melody\n",
    "    6: Non-Melody to Melody\n",
    "    7: Non-Melody to Non-Melody\n",
    "    \"\"\"\n",
    "    # print(prev_phrase_idx, curr_phrase_idx)\n",
    "    \n",
    "    prev_phrase_type = prev_phrase[0]\n",
    "    curr_phrase_type = curr_phrase[0]\n",
    "    \n",
    "    if prev_phrase == curr_phrase:\n",
    "            return 3\n",
    "    \n",
    "    if prev_phrase_idx + 1 == curr_phrase_idx:\n",
    "        # print(prev_phrase_type)\n",
    "    \n",
    "        if prev_phrase_type == \"i\":\n",
    "            return 1\n",
    "        elif curr_phrase_type == \"o\":\n",
    "            return 2\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.isupper():\n",
    "            return 4\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.islower():\n",
    "            return 5\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.isupper():\n",
    "            return 6\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.islower():\n",
    "            return 7\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "952d7a6c-f667-43f0-a2df-a32539748a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_phrase_edge_type(\"A4\", \"B4\", 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fad01669-06ee-49b9-a0dc-9e563baa9618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8',\n",
       " 'A8',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'b4',\n",
       " 'b4',\n",
       " 'x2',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'X1',\n",
       " 'o1']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ecf67c9-a9c3-4a07-a779-660fa860d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get max size of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20f6e135-ceef-4b6b-b9cc-696341e3ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(phrases):\n",
    "    # Create sequence of edges from phrase, where each item is a tuple (i, j, edge type, num bars in i, num bars in j)\n",
    "    # Start seq with START token\n",
    "    seq = [(40, 40, 8, 10, 10)]\n",
    "    max_phrase_len = 0\n",
    "    for i, phrase_from in enumerate(phrases):\n",
    "        for j, phrase_to in enumerate(phrases[i+1:]):\n",
    "            phrase_to_idx = j+i+1\n",
    "            edge_type = get_phrase_edge_type(phrase_from, phrase_to, i, phrase_to_idx)\n",
    "            if edge_type is not None:\n",
    "                phrase_from_len = int(phrase_from[1])\n",
    "                phrase_to_len = int(phrase_to[1])\n",
    "                max_phrase_len = max(max_phrase_len, max(phrase_from_len, phrase_to_len))\n",
    "                seq.append((i+1, phrase_to_idx+1, edge_type, phrase_from_len, phrase_to_len))\n",
    "    \n",
    "    # Append END token\n",
    "    seq.append((41, 41, 9, 11, 11))\n",
    "    return seq, max_phrase_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d27d96fc-78de-4fd5-a714-477437e814a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "max_phrase_len = 0\n",
    "max_num_nodes = 0\n",
    "for phrases in all_phrases:\n",
    "    # print(phrases)\n",
    "    num_nodes = len(phrases)\n",
    "    seq, max_phrase_len_indiv = create_sequence(phrases)\n",
    "    seqs.append(seq)\n",
    "    max_phrase_len = max(max_phrase_len, max_phrase_len_indiv)\n",
    "    max_num_nodes = max(max_num_nodes, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83fb7646-312c-4d18-84cd-d03c866f3fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8',\n",
       " 'A7',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'b5',\n",
       " 'b5',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'o1']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08eda1d0-6b73-417b-83cb-d53f4eec6e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 40, 8, 10, 10),\n",
       " (1, 2, 1, 8, 7),\n",
       " (2, 3, 4, 7, 8),\n",
       " (3, 4, 4, 8, 8),\n",
       " (3, 9, 3, 8, 8),\n",
       " (4, 5, 4, 8, 4),\n",
       " (4, 10, 3, 8, 8),\n",
       " (5, 6, 4, 4, 5),\n",
       " (5, 11, 3, 4, 4),\n",
       " (5, 12, 3, 4, 4),\n",
       " (5, 13, 3, 4, 4),\n",
       " (6, 7, 5, 5, 5),\n",
       " (6, 14, 3, 5, 5),\n",
       " (7, 8, 3, 5, 5),\n",
       " (8, 9, 6, 5, 8),\n",
       " (9, 10, 4, 8, 8),\n",
       " (10, 11, 4, 8, 4),\n",
       " (11, 12, 3, 4, 4),\n",
       " (11, 13, 3, 4, 4),\n",
       " (12, 13, 3, 4, 4),\n",
       " (13, 14, 4, 4, 5),\n",
       " (14, 15, 2, 5, 1),\n",
       " (41, 41, 9, 11, 11)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ede387e-42da-4182-975f-5f7ca62ade74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_phrase_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d79d2b30-a39d-4423-a0e0-d50c03f6161b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c0b370f-1c45-43ef-a8ac-69dcd642bfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150ab47-96b6-42b7-97a0-1c42e4a0eec5",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5849d65-260e-407d-afc6-407dd399785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def sequences_to_tensor(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Convert a list of sequences of different lengths to a padded tensor.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of list of tuples): List of sequences where each sequence is a list of tuples.\n",
    "        padding_value (int, optional): Value to use for padding. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_length, tuple_length)\n",
    "    \"\"\"\n",
    "    # Convert each sequence to a tensor\n",
    "    tensor_sequences = [torch.tensor(seq) for seq in sequences]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8ad429b-2c99-4f3a-a5ae-66a26e0cb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1818"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a881149-7308-4791-83b1-07b5803370e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded seq shape: torch.Size([1818, 152, 5])\n"
     ]
    }
   ],
   "source": [
    "padded_seq = sequences_to_tensor(seqs, padding_value=0)\n",
    "print(\"Padded seq shape:\", padded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30a435a3-e7a5-45c3-94f2-a8f1eee2df88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "77c686f5-f192-4832-ba0f-8bb75c267828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into Train and Test sets of size 1636 and 182 respectively.\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_test = round(len(seqs) * test_ratio)\n",
    "train_split, test_split = random_split(padded_seq, [len(seqs)-num_test, num_test])\n",
    "print(f\"Split data into Train and Test sets of size {len(train_split)} and {len(test_split)} respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79cff4a2-fc97-4370-a33c-028224190555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the custom dataset\n",
    "class TupleSequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, output_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.input_sequences[idx], self.output_sequences[idx]]\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "# Create the dataset\n",
    "dataset_train = TupleSequenceDataset(train_split, train_split)\n",
    "dataset_test = TupleSequenceDataset(test_split, test_split)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fbe783fb-8daf-46b4-9d34-0bba2b75f5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_token = [max_num_nodes+4, max_num_nodes+4, 10, max_phrase_len+3, max_phrase_len+3]\n",
    "# +1 to all to account for indices\n",
    "# +1 to max_num_nodes to account for START/END/PAD tokens\n",
    "# n_token[2] = 7+3 to account for START/END/PAD tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a446fdd-f302-4285-8d43-af446268bec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 43, 10, 12, 12]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd777-1572-428c-a42d-0968bc2d287d",
   "metadata": {},
   "source": [
    "## Autoregression transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "678c3f45-6f55-420b-aae3-ff12a76a8e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "576dc225-92c2-4b10-bc10-628ecbe7868b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/YatingMusic/compound-word-transformer/blob/main/workspace/uncond/cp-linear/main-cp.py\n",
    "# https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "# https://gist.github.com/danimelchor/bcad4d7f79b98464c4d4481d62d27622\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Get embeddings for edge tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, n_token, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # print(n_token)\n",
    "        self.lut = nn.Embedding(n_token, d_model, padding_idx=0)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(n_token)\n",
    "        # print(x.shape)\n",
    "        # print(torch.max(x))\n",
    "        # print(torch.min(x))\n",
    "        # print(self.d_model)\n",
    "        # print(self.lut(x))\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Get positional encodings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=20000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    def __init__(self, n_token):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "        \n",
    "        # --- params config --- #\n",
    "        self.n_token = n_token   \n",
    "        self.d_model = D_MODEL \n",
    "        self.d_feedforward = D_FEEDFW\n",
    "        self.n_layer = N_LAYER\n",
    "        self.dropout = 0.1\n",
    "        self.n_head = N_HEAD\n",
    "        self.d_head = D_MODEL // N_HEAD\n",
    "        self.d_inner = 1024\n",
    "        # self.loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.emb_sizes = [128, 128, 12, 16, 16]\n",
    "        \n",
    "        \n",
    "        # --- modules config --- #\n",
    "        # embeddings\n",
    "        print('>>>>>:', self.n_token)\n",
    "        self.emb_i = Embeddings(self.n_token[0], self.emb_sizes[0])\n",
    "        self.emb_j = Embeddings(self.n_token[1], self.emb_sizes[1])\n",
    "        self.emb_edge_type = Embeddings(self.n_token[2], self.emb_sizes[2])\n",
    "        self.emb_i_size = Embeddings(self.n_token[3], self.emb_sizes[3])\n",
    "        self.emb_j_size = Embeddings(self.n_token[4], self.emb_sizes[4])\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout)\n",
    "\n",
    "        # linear \n",
    "        self.in_linear = nn.Linear(np.sum(self.emb_sizes), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.n_head,\n",
    "            num_encoder_layers=self.n_layer,\n",
    "            num_decoder_layers=self.n_layer,\n",
    "            dim_feedforward=self.d_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        # individual output\n",
    "        self.proj_i    = nn.Linear(self.d_model, self.n_token[0])        \n",
    "        self.proj_j    = nn.Linear(self.d_model, self.n_token[1])\n",
    "        self.proj_edge_type  = nn.Linear(self.d_model, self.n_token[2])\n",
    "        self.proj_i_size     = nn.Linear(self.d_model, self.n_token[3])\n",
    "        self.proj_j_size    = nn.Linear(self.d_model, self.n_token[4])\n",
    "    \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        '''\n",
    "        linear transformer: b x s x f\n",
    "        x.shape=(bs, nf)\n",
    "        '''\n",
    "        \n",
    "        # print(f\"src shape: {src.shape}, target shape: {tgt.shape}\")\n",
    "    \n",
    "        # src embeddings\n",
    "        emb_i_src =    self.emb_i(src[..., 0])\n",
    "        emb_j_src =    self.emb_j(src[..., 1])\n",
    "        emb_edge_type_src =  self.emb_edge_type(src[..., 2])\n",
    "        emb_i_size_src =     self.emb_i_size(src[..., 3])\n",
    "        emb_j_size_src =    self.emb_j_size(src[..., 4])\n",
    "\n",
    "        embs_src = torch.cat(\n",
    "            [\n",
    "                emb_i_src,\n",
    "                emb_j_src,\n",
    "                emb_edge_type_src,\n",
    "                emb_i_size_src,\n",
    "                emb_j_size_src,\n",
    "            ], dim=-1)\n",
    "\n",
    "        emb_linear_src = self.in_linear(embs_src)\n",
    "        pos_emb_src = self.pos_emb(emb_linear_src)\n",
    "        \n",
    "        \n",
    "        # tgt embeddings\n",
    "        emb_i_tgt =    self.emb_i(tgt[..., 0])\n",
    "        emb_j_tgt =    self.emb_j(tgt[..., 1])\n",
    "        emb_edge_type_tgt =  self.emb_edge_type(tgt[..., 2])\n",
    "        emb_i_size_tgt =     self.emb_i_size(tgt[..., 3])\n",
    "        emb_j_size_tgt =    self.emb_j_size(tgt[..., 4])\n",
    "\n",
    "        embs_tgt = torch.cat(\n",
    "            [\n",
    "                emb_i_tgt,\n",
    "                emb_j_tgt,\n",
    "                emb_edge_type_tgt,\n",
    "                emb_i_size_tgt,\n",
    "                emb_j_size_tgt,\n",
    "            ], dim=-1)\n",
    "\n",
    "        emb_linear_tgt = self.in_linear(embs_tgt)\n",
    "        pos_emb_tgt = self.pos_emb(emb_linear_tgt)\n",
    "        \n",
    "        # target embeddings\n",
    "    \n",
    "        # transformer\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        # print(pos_emb_src.shape)\n",
    "        # print(pos_emb_tgt.shape)\n",
    "        pos_emb_src = pos_emb_src.permute(1,0,2)\n",
    "        pos_emb_tgt = pos_emb_tgt.permute(1,0,2)\n",
    "        # print(f\"Transformer input dim: {pos_emb_src.shape}, target dim: {pos_emb_tgt.shape}\")\n",
    "        transformer_out = self.transformer(pos_emb_src, pos_emb_tgt, \n",
    "                                           tgt_mask=tgt_mask, \n",
    "                                           src_key_padding_mask=src_pad_mask, \n",
    "                                           tgt_key_padding_mask=tgt_pad_mask)\n",
    "        # print(f\"Transformer output dim: {transformer_out.shape}\")\n",
    "        \n",
    "\n",
    "        y_i    = self.proj_i(transformer_out)\n",
    "        y_j    = self.proj_j(transformer_out)\n",
    "        y_edge_type  = self.proj_edge_type(transformer_out)\n",
    "        y_i_size    = self.proj_i_size(transformer_out)\n",
    "        y_j_size = self.proj_j_size(transformer_out)\n",
    "\n",
    "        return  y_i, y_j, y_edge_type, y_i_size, y_j_size\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a square matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ba4b897-6858-4431-bc7e-37b565021219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(predict, target, loss_func, loss_mask):\n",
    "        # predict = predict.permute(1, 2, 0)  \n",
    "        predict = predict.permute(2, 1, 0)  \n",
    "        # print(torch.min(target))\n",
    "        # print(torch.max(target))\n",
    "        # print(f\"Predict shape: {predict}, Target shape: {target}\")\n",
    "        loss = loss_func(predict, target)\n",
    "        # print(f\"Initial loss shape: {loss.shape}\")\n",
    "        # print(f\"Initial loss: {loss}\")\n",
    "        # loss = loss * loss_mask\n",
    "        # loss = torch.sum(loss) / torch.sum(loss_mask)\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e8f6bce-a686-4ec6-ab74-77e22874ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        X = batch_X.to(device)\n",
    "        y = batch_y.to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        # print(f\"X shape: {X.shape}\")\n",
    "        # print(f\"y_input shape: {y_input.shape}\")\n",
    "\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        # print(f\"mask shape: {tgt_mask.shape}\")        \n",
    "        \n",
    "        y_i, y_j, y_edge_type, y_i_size, y_j_size = model(X, y_input, tgt_mask)\n",
    "        \n",
    "        # print(f\"shape after model pred: {y_i.shape}, {y_j.shape}\")\n",
    "\n",
    "        # reshape (b, s, f) -> (b, f, s)\n",
    "        y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "        y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "        y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "        y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "        y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "        \n",
    "        # print(f\"shape after reshape: {y_i.shape}, {y_j.shape}\")\n",
    "        # print(f\"y_expected shape: {y_expected.shape}, y_expected_i: {y_expected[..., 0].shape}\")\n",
    "\n",
    "        # loss\n",
    "        loss_i = compute_loss(\n",
    "                y_i, y_expected[..., 0], loss_fn, tgt_mask)\n",
    "        loss_j = compute_loss(\n",
    "                y_j, y_expected[..., 1], loss_fn, tgt_mask)\n",
    "        loss_edge_type = compute_loss(\n",
    "                y_edge_type, y_expected[..., 2], loss_fn, tgt_mask)\n",
    "        loss_i_size = compute_loss(\n",
    "                y_i_size,  y_expected[..., 3], loss_fn, tgt_mask)\n",
    "        loss_j_size = compute_loss(\n",
    "                y_j_size, y_expected[..., 4], loss_fn, tgt_mask)\n",
    "\n",
    "        loss = (loss_i + loss_j + loss_edge_type + loss_i_size + loss_j_size) / 5\n",
    "        print(f\"Loss: {loss:.2f} | loss_i: {loss_i:.2f} loss_j: {loss_j:.2f} loss_edge_type: {loss_edge_type:.2f} loss_i_size: {loss_i_size:.2f} loss_j_size: {loss_j_size:.2f}\")\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            \n",
    "            X = batch_X.to(device)\n",
    "            y = batch_y.to(device)\n",
    "\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "            \n",
    "            \n",
    "            y_i, y_j, y_edge_type, y_i_size, y_j_size = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # reshape (b, s, f) -> (b, f, s)\n",
    "            y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "            y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "            y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "            y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "            y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "            \n",
    "\n",
    "            # loss\n",
    "            loss_i = compute_loss(\n",
    "                    y_i, y_expected[..., 0], loss_fn, tgt_mask)\n",
    "            loss_j = compute_loss(\n",
    "                    y_j, y_expected[..., 1], loss_fn, tgt_mask)\n",
    "            loss_edge_type = compute_loss(\n",
    "                    y_edge_type, y_expected[..., 2], loss_fn, tgt_mask)\n",
    "            loss_i_size = compute_loss(\n",
    "                    y_i_size,  y_expected[..., 3], loss_fn, tgt_mask)\n",
    "            loss_j_size = compute_loss(\n",
    "                    y_j_size, y_expected[..., 4], loss_fn, tgt_mask)\n",
    "\n",
    "            loss = (loss_i + loss_j + loss_edge_type + loss_i_size + loss_j_size) / 5\n",
    "            print(f\"Validation Loss: {loss:.2f} | loss_i: {loss_i:.2f} loss_j: {loss_j:.2f} loss_edge_type: {loss_edge_type:.2f} loss_i_size: {loss_i_size:.2f} loss_j_size: {loss_j_size:.2f}\")\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b5e44fc-974f-4521-adbc-3b1e4bdbe401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "63c9aa4b-c4e4-4888-9ad9-999b623103de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96c0ce03-9e6b-4241-86f5-8394a225b1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 43, 10, 12, 12]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d63502e-0558-41a8-b2f5-443c1f789317",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      ">>>>>: [43, 43, 10, 12, 12]\n",
      "Training and validating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Epoch 1 -------------------------\n",
      "Loss: 13744.65 | loss_i: 17835.79 loss_j: 17851.54 loss_edge_type: 10889.70 loss_i_size: 11724.36 loss_j_size: 10421.84\n",
      "Loss: 9111.38 | loss_i: 12914.35 loss_j: 14085.17 loss_edge_type: 5856.33 loss_i_size: 7045.30 loss_j_size: 5655.78\n",
      "Loss: 6970.24 | loss_i: 8855.09 loss_j: 11506.82 loss_edge_type: 4410.18 loss_i_size: 5550.74 loss_j_size: 4528.37\n",
      "Loss: 6284.02 | loss_i: 7062.00 loss_j: 9822.59 loss_edge_type: 4448.77 loss_i_size: 5456.15 loss_j_size: 4630.57\n",
      "Loss: 5593.20 | loss_i: 6038.51 loss_j: 8130.25 loss_edge_type: 4265.32 loss_i_size: 5065.07 loss_j_size: 4466.84\n",
      "Loss: 5258.57 | loss_i: 5762.73 loss_j: 7076.85 loss_edge_type: 4174.51 loss_i_size: 4878.73 loss_j_size: 4400.03\n",
      "Loss: 4918.01 | loss_i: 5503.80 loss_j: 6207.09 loss_edge_type: 4015.28 loss_i_size: 4621.25 loss_j_size: 4242.60\n",
      "Loss: 5029.06 | loss_i: 5908.73 loss_j: 6136.78 loss_edge_type: 4074.21 loss_i_size: 4624.12 loss_j_size: 4401.45\n",
      "Loss: 4588.01 | loss_i: 5541.54 loss_j: 5530.21 loss_edge_type: 3738.29 loss_i_size: 4194.37 loss_j_size: 3935.63\n",
      "Loss: 4649.44 | loss_i: 5748.63 loss_j: 5631.17 loss_edge_type: 3686.54 loss_i_size: 4228.88 loss_j_size: 3952.00\n",
      "Loss: 4471.61 | loss_i: 5581.05 loss_j: 5423.92 loss_edge_type: 3556.08 loss_i_size: 4047.52 loss_j_size: 3749.50\n",
      "Loss: 4256.20 | loss_i: 5266.92 loss_j: 5213.35 loss_edge_type: 3383.50 loss_i_size: 3850.42 loss_j_size: 3566.80\n",
      "Loss: 4480.66 | loss_i: 5532.25 loss_j: 5555.06 loss_edge_type: 3552.95 loss_i_size: 4022.95 loss_j_size: 3740.07\n",
      "Loss: 4200.35 | loss_i: 5133.12 loss_j: 5182.28 loss_edge_type: 3377.58 loss_i_size: 3786.20 loss_j_size: 3522.58\n",
      "Loss: 4047.26 | loss_i: 4898.83 loss_j: 4988.20 loss_edge_type: 3260.80 loss_i_size: 3674.58 loss_j_size: 3413.88\n",
      "Loss: 4116.14 | loss_i: 4994.79 loss_j: 5112.10 loss_edge_type: 3360.95 loss_i_size: 3654.47 loss_j_size: 3458.41\n",
      "Loss: 4012.35 | loss_i: 4838.20 loss_j: 4928.45 loss_edge_type: 3258.71 loss_i_size: 3587.06 loss_j_size: 3449.32\n",
      "Loss: 4237.95 | loss_i: 5210.07 loss_j: 5345.62 loss_edge_type: 3405.53 loss_i_size: 3703.04 loss_j_size: 3525.49\n",
      "Loss: 3728.91 | loss_i: 4573.40 loss_j: 4625.40 loss_edge_type: 3095.70 loss_i_size: 3217.61 loss_j_size: 3132.46\n",
      "Loss: 3865.57 | loss_i: 4805.54 loss_j: 4781.39 loss_edge_type: 3171.49 loss_i_size: 3336.43 loss_j_size: 3232.98\n",
      "Loss: 3915.02 | loss_i: 4897.85 loss_j: 4976.68 loss_edge_type: 3164.24 loss_i_size: 3304.96 loss_j_size: 3231.38\n",
      "Loss: 3690.49 | loss_i: 4639.72 loss_j: 4648.78 loss_edge_type: 3003.47 loss_i_size: 3137.23 loss_j_size: 3023.25\n",
      "Loss: 4148.48 | loss_i: 5328.54 loss_j: 5387.77 loss_edge_type: 3274.39 loss_i_size: 3456.38 loss_j_size: 3295.34\n",
      "Loss: 3392.25 | loss_i: 4303.68 loss_j: 4372.18 loss_edge_type: 2755.66 loss_i_size: 2854.85 loss_j_size: 2674.85\n",
      "Loss: 3551.48 | loss_i: 4630.98 loss_j: 4691.85 loss_edge_type: 2819.34 loss_i_size: 2910.31 loss_j_size: 2704.92\n",
      "Loss: 3625.63 | loss_i: 4733.56 loss_j: 4953.77 loss_edge_type: 2761.14 loss_i_size: 2945.04 loss_j_size: 2734.66\n",
      "Loss: 3278.95 | loss_i: 4344.83 loss_j: 4563.95 loss_edge_type: 2507.04 loss_i_size: 2597.49 loss_j_size: 2381.43\n",
      "Loss: 3074.54 | loss_i: 4117.98 loss_j: 4353.36 loss_edge_type: 2388.32 loss_i_size: 2340.01 loss_j_size: 2173.01\n",
      "Loss: 3010.95 | loss_i: 4156.43 loss_j: 4398.84 loss_edge_type: 2235.88 loss_i_size: 2226.12 loss_j_size: 2037.49\n",
      "Loss: 2947.17 | loss_i: 4015.64 loss_j: 4215.77 loss_edge_type: 2206.84 loss_i_size: 2221.51 loss_j_size: 2076.10\n",
      "Loss: 2887.62 | loss_i: 3926.62 loss_j: 4174.79 loss_edge_type: 2112.00 loss_i_size: 2196.34 loss_j_size: 2028.35\n",
      "Loss: 3081.11 | loss_i: 4132.18 loss_j: 4402.43 loss_edge_type: 2136.15 loss_i_size: 2442.53 loss_j_size: 2292.28\n",
      "Loss: 2953.30 | loss_i: 3977.18 loss_j: 4273.45 loss_edge_type: 2050.34 loss_i_size: 2310.08 loss_j_size: 2155.43\n",
      "Loss: 2637.68 | loss_i: 3501.69 loss_j: 3737.67 loss_edge_type: 1873.21 loss_i_size: 2095.98 loss_j_size: 1979.85\n",
      "Loss: 2535.99 | loss_i: 3268.50 loss_j: 3510.87 loss_edge_type: 1834.17 loss_i_size: 2082.17 loss_j_size: 1984.26\n",
      "Loss: 2559.44 | loss_i: 3350.09 loss_j: 3629.70 loss_edge_type: 1808.96 loss_i_size: 2026.92 loss_j_size: 1981.54\n",
      "Loss: 2443.91 | loss_i: 3271.24 loss_j: 3534.67 loss_edge_type: 1764.86 loss_i_size: 1834.98 loss_j_size: 1813.78\n",
      "Loss: 2385.53 | loss_i: 3109.50 loss_j: 3400.87 loss_edge_type: 1730.21 loss_i_size: 1863.94 loss_j_size: 1823.12\n",
      "Loss: 2555.45 | loss_i: 3407.38 loss_j: 3788.94 loss_edge_type: 1779.34 loss_i_size: 1919.42 loss_j_size: 1882.17\n",
      "Loss: 2376.11 | loss_i: 3155.71 loss_j: 3519.46 loss_edge_type: 1662.44 loss_i_size: 1799.20 loss_j_size: 1743.74\n",
      "Loss: 2281.01 | loss_i: 2984.15 loss_j: 3381.75 loss_edge_type: 1600.39 loss_i_size: 1739.34 loss_j_size: 1699.44\n",
      "Loss: 2193.22 | loss_i: 2863.05 loss_j: 3249.73 loss_edge_type: 1541.56 loss_i_size: 1693.08 loss_j_size: 1618.69\n",
      "Loss: 2280.67 | loss_i: 3058.05 loss_j: 3429.95 loss_edge_type: 1558.85 loss_i_size: 1701.01 loss_j_size: 1655.49\n",
      "Loss: 2350.34 | loss_i: 3213.04 loss_j: 3622.45 loss_edge_type: 1594.94 loss_i_size: 1681.62 loss_j_size: 1639.65\n",
      "Loss: 2424.20 | loss_i: 3275.46 loss_j: 3620.30 loss_edge_type: 1617.90 loss_i_size: 1812.63 loss_j_size: 1794.72\n",
      "Loss: 2370.21 | loss_i: 3239.63 loss_j: 3615.79 loss_edge_type: 1580.02 loss_i_size: 1723.91 loss_j_size: 1691.72\n",
      "Loss: 2397.30 | loss_i: 3219.86 loss_j: 3524.69 loss_edge_type: 1594.90 loss_i_size: 1838.90 loss_j_size: 1808.17\n",
      "Loss: 2032.08 | loss_i: 2634.76 loss_j: 2875.75 loss_edge_type: 1460.96 loss_i_size: 1612.94 loss_j_size: 1575.98\n",
      "Loss: 2106.84 | loss_i: 2711.21 loss_j: 2966.69 loss_edge_type: 1491.92 loss_i_size: 1705.60 loss_j_size: 1658.78\n",
      "Loss: 2003.68 | loss_i: 2600.76 loss_j: 2831.87 loss_edge_type: 1419.26 loss_i_size: 1601.86 loss_j_size: 1564.66\n",
      "Loss: 2344.37 | loss_i: 3234.99 loss_j: 3584.12 loss_edge_type: 1563.15 loss_i_size: 1653.89 loss_j_size: 1685.70\n",
      "Loss: 254.23 | loss_i: 319.80 loss_j: 345.26 loss_edge_type: 169.61 loss_i_size: 226.68 loss_j_size: 209.81\n",
      "Validation Loss: 2053.85 | loss_i: 2787.52 loss_j: 3077.61 loss_edge_type: 1365.68 loss_i_size: 1515.44 loss_j_size: 1522.99\n",
      "Validation Loss: 1909.95 | loss_i: 2527.59 loss_j: 2798.36 loss_edge_type: 1316.61 loss_i_size: 1457.49 loss_j_size: 1449.70\n",
      "Validation Loss: 2085.08 | loss_i: 2833.74 loss_j: 3114.63 loss_edge_type: 1368.84 loss_i_size: 1547.58 loss_j_size: 1560.59\n",
      "Validation Loss: 2039.43 | loss_i: 2675.41 loss_j: 2937.42 loss_edge_type: 1410.38 loss_i_size: 1606.31 loss_j_size: 1567.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1/20 [00:10<03:24, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2003.15 | loss_i: 2675.35 loss_j: 2981.33 loss_edge_type: 1340.34 loss_i_size: 1507.07 loss_j_size: 1511.68\n",
      "Validation Loss: 1512.19 | loss_i: 2107.72 loss_j: 2302.95 loss_edge_type: 994.89 loss_i_size: 1074.69 loss_j_size: 1080.71\n",
      "Training loss: 3724.0933\n",
      "Validation loss: 1933.9424\n",
      "\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Loss: 2000.33 | loss_i: 2578.58 loss_j: 2828.69 loss_edge_type: 1423.73 loss_i_size: 1597.49 loss_j_size: 1573.14\n",
      "Loss: 2013.62 | loss_i: 2613.37 loss_j: 2887.40 loss_edge_type: 1431.97 loss_i_size: 1578.58 loss_j_size: 1556.76\n",
      "Loss: 2083.43 | loss_i: 2751.30 loss_j: 3006.61 loss_edge_type: 1467.04 loss_i_size: 1591.96 loss_j_size: 1600.24\n",
      "Loss: 2045.18 | loss_i: 2608.40 loss_j: 2886.84 loss_edge_type: 1430.31 loss_i_size: 1667.51 loss_j_size: 1632.82\n",
      "Loss: 2356.58 | loss_i: 3145.39 loss_j: 3499.42 loss_edge_type: 1512.20 loss_i_size: 1823.39 loss_j_size: 1802.52\n",
      "Loss: 2110.55 | loss_i: 2773.82 loss_j: 3082.29 loss_edge_type: 1453.42 loss_i_size: 1625.47 loss_j_size: 1617.74\n",
      "Loss: 2062.80 | loss_i: 2750.93 loss_j: 3114.88 loss_edge_type: 1351.71 loss_i_size: 1543.69 loss_j_size: 1552.77\n",
      "Loss: 1858.17 | loss_i: 2475.15 loss_j: 2745.26 loss_edge_type: 1309.66 loss_i_size: 1382.39 loss_j_size: 1378.38\n",
      "Loss: 1955.81 | loss_i: 2552.15 loss_j: 2806.85 loss_edge_type: 1345.61 loss_i_size: 1536.82 loss_j_size: 1537.63\n",
      "Loss: 1852.92 | loss_i: 2361.12 loss_j: 2593.98 loss_edge_type: 1281.24 loss_i_size: 1521.01 loss_j_size: 1507.24\n",
      "Loss: 2111.57 | loss_i: 2811.91 loss_j: 3071.66 loss_edge_type: 1405.13 loss_i_size: 1630.44 loss_j_size: 1638.72\n",
      "Loss: 2133.90 | loss_i: 2798.32 loss_j: 3064.47 loss_edge_type: 1438.50 loss_i_size: 1692.39 loss_j_size: 1675.81\n",
      "Loss: 2042.73 | loss_i: 2706.31 loss_j: 2956.31 loss_edge_type: 1379.82 loss_i_size: 1604.53 loss_j_size: 1566.65\n",
      "Loss: 1656.29 | loss_i: 2168.89 loss_j: 2379.65 loss_edge_type: 1177.96 loss_i_size: 1284.47 loss_j_size: 1270.48\n",
      "Loss: 1719.33 | loss_i: 2222.40 loss_j: 2455.50 loss_edge_type: 1218.84 loss_i_size: 1366.51 loss_j_size: 1333.41\n",
      "Loss: 1802.08 | loss_i: 2314.95 loss_j: 2521.90 loss_edge_type: 1267.65 loss_i_size: 1456.24 loss_j_size: 1449.65\n",
      "Loss: 2001.11 | loss_i: 2735.89 loss_j: 3038.55 loss_edge_type: 1378.11 loss_i_size: 1431.53 loss_j_size: 1421.46\n",
      "Loss: 1973.57 | loss_i: 2689.07 loss_j: 2953.57 loss_edge_type: 1360.00 loss_i_size: 1429.33 loss_j_size: 1435.90\n",
      "Loss: 2014.55 | loss_i: 2670.59 loss_j: 2992.63 loss_edge_type: 1336.32 loss_i_size: 1530.44 loss_j_size: 1542.78\n",
      "Loss: 1903.86 | loss_i: 2556.98 loss_j: 2830.95 loss_edge_type: 1325.36 loss_i_size: 1394.31 loss_j_size: 1411.71\n",
      "Loss: 2079.15 | loss_i: 2841.75 loss_j: 3206.01 loss_edge_type: 1378.43 loss_i_size: 1491.09 loss_j_size: 1478.46\n",
      "Loss: 1789.56 | loss_i: 2359.88 loss_j: 2618.57 loss_edge_type: 1270.98 loss_i_size: 1354.06 loss_j_size: 1344.30\n",
      "Loss: 1988.01 | loss_i: 2680.72 loss_j: 2954.46 loss_edge_type: 1349.48 loss_i_size: 1477.99 loss_j_size: 1477.44\n",
      "Loss: 1762.67 | loss_i: 2251.87 loss_j: 2484.57 loss_edge_type: 1263.33 loss_i_size: 1404.40 loss_j_size: 1409.16\n",
      "Loss: 1868.33 | loss_i: 2469.83 loss_j: 2800.25 loss_edge_type: 1284.34 loss_i_size: 1393.98 loss_j_size: 1393.22\n",
      "Loss: 1689.62 | loss_i: 2076.11 loss_j: 2298.21 loss_edge_type: 1194.26 loss_i_size: 1436.51 loss_j_size: 1443.00\n",
      "Loss: 1789.91 | loss_i: 2241.57 loss_j: 2535.83 loss_edge_type: 1245.37 loss_i_size: 1470.64 loss_j_size: 1456.16\n",
      "Loss: 1665.53 | loss_i: 2113.58 loss_j: 2368.43 loss_edge_type: 1190.98 loss_i_size: 1339.67 loss_j_size: 1314.98\n",
      "Loss: 1628.58 | loss_i: 2099.55 loss_j: 2358.38 loss_edge_type: 1192.50 loss_i_size: 1254.45 loss_j_size: 1238.03\n",
      "Loss: 1731.12 | loss_i: 2209.95 loss_j: 2490.83 loss_edge_type: 1222.60 loss_i_size: 1381.10 loss_j_size: 1351.15\n",
      "Loss: 1974.86 | loss_i: 2693.12 loss_j: 3069.63 loss_edge_type: 1328.88 loss_i_size: 1405.18 loss_j_size: 1377.51\n",
      "Loss: 1501.45 | loss_i: 1803.73 loss_j: 2066.96 loss_edge_type: 1116.98 loss_i_size: 1268.38 loss_j_size: 1251.21\n",
      "Loss: 1783.20 | loss_i: 2328.99 loss_j: 2634.18 loss_edge_type: 1219.10 loss_i_size: 1381.07 loss_j_size: 1352.69\n",
      "Loss: 1525.66 | loss_i: 1898.21 loss_j: 2171.56 loss_edge_type: 1103.29 loss_i_size: 1220.06 loss_j_size: 1235.18\n",
      "Loss: 1602.08 | loss_i: 1953.20 loss_j: 2187.81 loss_edge_type: 1144.19 loss_i_size: 1346.78 loss_j_size: 1378.41\n",
      "Loss: 1508.10 | loss_i: 1902.53 loss_j: 2185.82 loss_edge_type: 1114.02 loss_i_size: 1172.25 loss_j_size: 1165.87\n",
      "Loss: 1653.70 | loss_i: 2027.13 loss_j: 2318.31 loss_edge_type: 1163.64 loss_i_size: 1381.88 loss_j_size: 1377.54\n",
      "Loss: 1551.60 | loss_i: 1867.73 loss_j: 2166.51 loss_edge_type: 1133.25 loss_i_size: 1291.72 loss_j_size: 1298.81\n",
      "Loss: 1517.79 | loss_i: 1939.81 loss_j: 2246.38 loss_edge_type: 1127.20 loss_i_size: 1131.22 loss_j_size: 1144.35\n",
      "Loss: 1535.40 | loss_i: 1768.48 loss_j: 2040.83 loss_edge_type: 1130.05 loss_i_size: 1376.00 loss_j_size: 1361.63\n",
      "Loss: 1563.86 | loss_i: 1870.74 loss_j: 2208.44 loss_edge_type: 1162.94 loss_i_size: 1274.76 loss_j_size: 1302.39\n",
      "Loss: 1540.05 | loss_i: 1900.59 loss_j: 2222.66 loss_edge_type: 1121.86 loss_i_size: 1221.71 loss_j_size: 1233.43\n",
      "Loss: 1497.99 | loss_i: 1719.10 loss_j: 2033.85 loss_edge_type: 1075.73 loss_i_size: 1342.20 loss_j_size: 1319.04\n",
      "Loss: 1453.69 | loss_i: 1686.79 loss_j: 2011.95 loss_edge_type: 1095.28 loss_i_size: 1237.76 loss_j_size: 1236.65\n",
      "Loss: 1489.62 | loss_i: 1768.98 loss_j: 2174.54 loss_edge_type: 1135.50 loss_i_size: 1178.67 loss_j_size: 1190.43\n",
      "Loss: 1729.18 | loss_i: 2212.18 loss_j: 2692.52 loss_edge_type: 1163.23 loss_i_size: 1280.56 loss_j_size: 1297.39\n",
      "Loss: 1481.27 | loss_i: 1667.81 loss_j: 2025.12 loss_edge_type: 1140.86 loss_i_size: 1268.06 loss_j_size: 1304.50\n",
      "Loss: 1636.63 | loss_i: 1881.92 loss_j: 2315.26 loss_edge_type: 1207.82 loss_i_size: 1385.46 loss_j_size: 1392.68\n",
      "Loss: 1473.98 | loss_i: 1683.09 loss_j: 2071.26 loss_edge_type: 1129.76 loss_i_size: 1234.85 loss_j_size: 1250.92\n",
      "Loss: 1435.04 | loss_i: 1660.03 loss_j: 2054.32 loss_edge_type: 1089.79 loss_i_size: 1173.30 loss_j_size: 1197.76\n",
      "Loss: 1272.83 | loss_i: 1399.96 loss_j: 1788.15 loss_edge_type: 1046.25 loss_i_size: 1058.81 loss_j_size: 1071.00\n",
      "Loss: 150.34 | loss_i: 146.45 loss_j: 184.06 loss_edge_type: 121.24 loss_i_size: 147.58 loss_j_size: 152.38\n",
      "Validation Loss: 1495.09 | loss_i: 1685.08 loss_j: 2167.25 loss_edge_type: 1057.06 loss_i_size: 1272.19 loss_j_size: 1293.90\n",
      "Validation Loss: 1224.32 | loss_i: 1290.45 loss_j: 1716.08 loss_edge_type: 945.62 loss_i_size: 1075.17 loss_j_size: 1094.28\n",
      "Validation Loss: 1480.98 | loss_i: 1722.40 loss_j: 2191.90 loss_edge_type: 1100.87 loss_i_size: 1180.27 loss_j_size: 1209.48\n",
      "Validation Loss: 1419.39 | loss_i: 1799.61 loss_j: 2274.20 loss_edge_type: 1073.52 loss_i_size: 978.53 loss_j_size: 971.06\n",
      "Validation Loss: 1265.38 | loss_i: 1384.87 loss_j: 1853.68 loss_edge_type: 1018.12 loss_i_size: 1031.36 loss_j_size: 1038.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 2/20 [00:21<03:14, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 894.02 | loss_i: 1014.61 loss_j: 1373.91 loss_edge_type: 699.36 loss_i_size: 691.42 loss_j_size: 690.83\n",
      "Training loss: 1741.7144\n",
      "Validation loss: 1296.5306\n",
      "\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Loss: 1409.45 | loss_i: 1605.73 loss_j: 2042.49 loss_edge_type: 1056.71 loss_i_size: 1167.36 loss_j_size: 1174.95\n",
      "Loss: 1218.62 | loss_i: 1378.04 loss_j: 1741.39 loss_edge_type: 982.25 loss_i_size: 980.48 loss_j_size: 1010.96\n",
      "Loss: 1465.40 | loss_i: 1612.79 loss_j: 2034.56 loss_edge_type: 1135.16 loss_i_size: 1262.25 loss_j_size: 1282.26\n",
      "Loss: 1421.88 | loss_i: 1637.91 loss_j: 2107.12 loss_edge_type: 1111.19 loss_i_size: 1110.89 loss_j_size: 1142.29\n",
      "Loss: 1300.35 | loss_i: 1418.76 loss_j: 1795.84 loss_edge_type: 1021.84 loss_i_size: 1111.00 loss_j_size: 1154.33\n",
      "Loss: 1801.58 | loss_i: 2231.51 loss_j: 2867.50 loss_edge_type: 1234.06 loss_i_size: 1334.43 loss_j_size: 1340.42\n",
      "Loss: 1463.15 | loss_i: 1682.30 loss_j: 2115.07 loss_edge_type: 1073.02 loss_i_size: 1222.82 loss_j_size: 1222.55\n",
      "Loss: 1296.75 | loss_i: 1417.45 loss_j: 1791.69 loss_edge_type: 999.50 loss_i_size: 1127.10 loss_j_size: 1148.02\n",
      "Loss: 1359.59 | loss_i: 1512.56 loss_j: 1961.27 loss_edge_type: 1058.34 loss_i_size: 1120.67 loss_j_size: 1145.11\n",
      "Loss: 1280.05 | loss_i: 1322.40 loss_j: 1736.04 loss_edge_type: 1019.52 loss_i_size: 1144.32 loss_j_size: 1177.94\n",
      "Loss: 1260.68 | loss_i: 1304.57 loss_j: 1737.18 loss_edge_type: 1002.50 loss_i_size: 1118.87 loss_j_size: 1140.29\n",
      "Loss: 1289.48 | loss_i: 1380.21 loss_j: 1792.95 loss_edge_type: 976.29 loss_i_size: 1149.58 loss_j_size: 1148.35\n",
      "Loss: 1253.32 | loss_i: 1354.53 loss_j: 1768.82 loss_edge_type: 1001.62 loss_i_size: 1069.81 loss_j_size: 1071.82\n",
      "Loss: 1294.97 | loss_i: 1470.24 loss_j: 1942.30 loss_edge_type: 1020.09 loss_i_size: 1016.94 loss_j_size: 1025.27\n",
      "Loss: 1285.17 | loss_i: 1280.10 loss_j: 1707.91 loss_edge_type: 1019.60 loss_i_size: 1199.85 loss_j_size: 1218.37\n",
      "Loss: 1666.07 | loss_i: 1972.20 loss_j: 2599.16 loss_edge_type: 1188.90 loss_i_size: 1293.97 loss_j_size: 1276.14\n",
      "Loss: 1244.52 | loss_i: 1244.37 loss_j: 1658.83 loss_edge_type: 1015.02 loss_i_size: 1136.23 loss_j_size: 1168.16\n",
      "Loss: 1248.58 | loss_i: 1303.88 loss_j: 1757.64 loss_edge_type: 1029.71 loss_i_size: 1061.73 loss_j_size: 1089.95\n",
      "Loss: 1302.80 | loss_i: 1328.37 loss_j: 1825.79 loss_edge_type: 1009.91 loss_i_size: 1160.48 loss_j_size: 1189.43\n",
      "Loss: 1178.24 | loss_i: 1234.67 loss_j: 1635.87 loss_edge_type: 951.03 loss_i_size: 1022.95 loss_j_size: 1046.68\n",
      "Loss: 1207.26 | loss_i: 1211.55 loss_j: 1642.92 loss_edge_type: 987.83 loss_i_size: 1076.19 loss_j_size: 1117.83\n",
      "Loss: 1135.94 | loss_i: 1158.75 loss_j: 1602.26 loss_edge_type: 944.85 loss_i_size: 979.83 loss_j_size: 993.99\n",
      "Loss: 1204.35 | loss_i: 1127.42 loss_j: 1573.54 loss_edge_type: 989.21 loss_i_size: 1148.96 loss_j_size: 1182.62\n",
      "Loss: 1270.85 | loss_i: 1440.14 loss_j: 1961.34 loss_edge_type: 1025.21 loss_i_size: 960.26 loss_j_size: 967.32\n",
      "Loss: 1348.15 | loss_i: 1411.64 loss_j: 1932.75 loss_edge_type: 1093.60 loss_i_size: 1133.60 loss_j_size: 1169.15\n",
      "Loss: 1278.43 | loss_i: 1347.79 loss_j: 1901.49 loss_edge_type: 1029.18 loss_i_size: 1061.72 loss_j_size: 1051.99\n",
      "Loss: 1230.70 | loss_i: 1219.56 loss_j: 1684.29 loss_edge_type: 976.97 loss_i_size: 1119.67 loss_j_size: 1152.99\n",
      "Loss: 1123.67 | loss_i: 1109.93 loss_j: 1555.69 loss_edge_type: 934.33 loss_i_size: 991.58 loss_j_size: 1026.81\n",
      "Loss: 1290.42 | loss_i: 1301.31 loss_j: 1854.94 loss_edge_type: 1007.84 loss_i_size: 1121.47 loss_j_size: 1166.54\n",
      "Loss: 1178.22 | loss_i: 1154.39 loss_j: 1596.82 loss_edge_type: 955.56 loss_i_size: 1080.09 loss_j_size: 1104.23\n",
      "Loss: 1318.46 | loss_i: 1394.33 loss_j: 1999.90 loss_edge_type: 1047.37 loss_i_size: 1065.08 loss_j_size: 1085.64\n",
      "Loss: 1173.34 | loss_i: 1129.20 loss_j: 1590.45 loss_edge_type: 970.37 loss_i_size: 1076.85 loss_j_size: 1099.84\n",
      "Loss: 1224.78 | loss_i: 1211.36 loss_j: 1716.51 loss_edge_type: 1011.47 loss_i_size: 1074.40 loss_j_size: 1110.14\n",
      "Loss: 1271.75 | loss_i: 1308.10 loss_j: 1908.44 loss_edge_type: 996.54 loss_i_size: 1055.51 loss_j_size: 1090.15\n",
      "Loss: 1108.23 | loss_i: 1034.50 loss_j: 1553.16 loss_edge_type: 962.71 loss_i_size: 979.48 loss_j_size: 1011.29\n",
      "Loss: 1206.54 | loss_i: 1209.80 loss_j: 1627.37 loss_edge_type: 1012.82 loss_i_size: 1071.84 loss_j_size: 1110.85\n",
      "Loss: 1207.07 | loss_i: 1220.88 loss_j: 1691.68 loss_edge_type: 961.83 loss_i_size: 1065.17 loss_j_size: 1095.81\n",
      "Loss: 1468.26 | loss_i: 1600.54 loss_j: 2315.63 loss_edge_type: 1103.69 loss_i_size: 1147.83 loss_j_size: 1173.62\n",
      "Loss: 1132.71 | loss_i: 1063.05 loss_j: 1489.55 loss_edge_type: 948.65 loss_i_size: 1063.45 loss_j_size: 1098.83\n",
      "Loss: 1219.33 | loss_i: 1216.10 loss_j: 1818.48 loss_edge_type: 1025.16 loss_i_size: 988.27 loss_j_size: 1048.64\n",
      "Loss: 1351.46 | loss_i: 1315.43 loss_j: 1981.07 loss_edge_type: 1065.49 loss_i_size: 1178.83 loss_j_size: 1216.46\n",
      "Loss: 1064.23 | loss_i: 995.14 loss_j: 1460.61 loss_edge_type: 917.01 loss_i_size: 964.91 loss_j_size: 983.46\n",
      "Loss: 1076.99 | loss_i: 1000.63 loss_j: 1442.62 loss_edge_type: 906.53 loss_i_size: 1001.72 loss_j_size: 1033.44\n",
      "Loss: 1121.73 | loss_i: 1049.41 loss_j: 1499.07 loss_edge_type: 908.79 loss_i_size: 1059.00 loss_j_size: 1092.38\n",
      "Loss: 1007.96 | loss_i: 902.94 loss_j: 1254.61 loss_edge_type: 858.43 loss_i_size: 993.19 loss_j_size: 1030.65\n",
      "Loss: 1049.53 | loss_i: 879.37 loss_j: 1327.00 loss_edge_type: 903.30 loss_i_size: 1056.22 loss_j_size: 1081.77\n",
      "Loss: 1104.30 | loss_i: 1069.01 loss_j: 1585.52 loss_edge_type: 918.01 loss_i_size: 962.30 loss_j_size: 986.67\n",
      "Loss: 1170.72 | loss_i: 1114.44 loss_j: 1651.98 loss_edge_type: 968.42 loss_i_size: 1034.41 loss_j_size: 1084.36\n",
      "Loss: 1123.68 | loss_i: 1067.38 loss_j: 1550.84 loss_edge_type: 965.42 loss_i_size: 998.16 loss_j_size: 1036.61\n",
      "Loss: 1154.40 | loss_i: 1046.38 loss_j: 1594.30 loss_edge_type: 979.96 loss_i_size: 1055.03 loss_j_size: 1096.34\n",
      "Loss: 959.74 | loss_i: 886.65 loss_j: 1306.80 loss_edge_type: 828.23 loss_i_size: 868.38 loss_j_size: 908.63\n",
      "Loss: 98.20 | loss_i: 68.90 loss_j: 95.21 loss_edge_type: 83.94 loss_i_size: 120.31 loss_j_size: 122.65\n",
      "Validation Loss: 981.82 | loss_i: 873.52 loss_j: 1397.44 loss_edge_type: 839.96 loss_i_size: 883.08 loss_j_size: 915.06\n",
      "Validation Loss: 1077.49 | loss_i: 987.94 loss_j: 1543.34 loss_edge_type: 915.34 loss_i_size: 942.99 loss_j_size: 997.85\n",
      "Validation Loss: 1521.58 | loss_i: 1687.97 loss_j: 2513.41 loss_edge_type: 1128.34 loss_i_size: 1113.27 loss_j_size: 1164.89\n",
      "Validation Loss: 988.59 | loss_i: 857.46 loss_j: 1352.58 loss_edge_type: 865.58 loss_i_size: 908.22 loss_j_size: 959.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:31<03:00, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 929.49 | loss_i: 764.36 loss_j: 1215.70 loss_edge_type: 825.81 loss_i_size: 905.39 loss_j_size: 936.20\n",
      "Validation Loss: 635.86 | loss_i: 531.56 loss_j: 850.88 loss_edge_type: 560.56 loss_i_size: 607.61 loss_j_size: 628.67\n",
      "Training loss: 1229.2701\n",
      "Validation loss: 1022.4707\n",
      "\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Loss: 1134.87 | loss_i: 1087.86 loss_j: 1580.31 loss_edge_type: 962.41 loss_i_size: 1002.49 loss_j_size: 1041.30\n",
      "Loss: 1244.70 | loss_i: 1202.80 loss_j: 1784.46 loss_edge_type: 994.47 loss_i_size: 1086.10 loss_j_size: 1155.65\n",
      "Loss: 1190.17 | loss_i: 1063.70 loss_j: 1570.12 loss_edge_type: 1005.65 loss_i_size: 1133.76 loss_j_size: 1177.64\n",
      "Loss: 1089.65 | loss_i: 1028.81 loss_j: 1528.42 loss_edge_type: 932.83 loss_i_size: 956.37 loss_j_size: 1001.80\n",
      "Loss: 1052.72 | loss_i: 974.21 loss_j: 1474.25 loss_edge_type: 897.92 loss_i_size: 947.97 loss_j_size: 969.28\n",
      "Loss: 1137.24 | loss_i: 1043.17 loss_j: 1610.05 loss_edge_type: 953.11 loss_i_size: 1027.61 loss_j_size: 1052.26\n",
      "Loss: 1132.48 | loss_i: 1126.09 loss_j: 1696.07 loss_edge_type: 943.03 loss_i_size: 937.63 loss_j_size: 959.57\n",
      "Loss: 1025.57 | loss_i: 888.25 loss_j: 1356.54 loss_edge_type: 857.87 loss_i_size: 1000.38 loss_j_size: 1024.80\n",
      "Loss: 1373.07 | loss_i: 1440.79 loss_j: 2197.92 loss_edge_type: 1049.47 loss_i_size: 1066.76 loss_j_size: 1110.42\n",
      "Loss: 1146.83 | loss_i: 1076.24 loss_j: 1711.56 loss_edge_type: 934.95 loss_i_size: 989.45 loss_j_size: 1021.98\n",
      "Loss: 1208.13 | loss_i: 1089.09 loss_j: 1691.86 loss_edge_type: 1061.50 loss_i_size: 1076.34 loss_j_size: 1121.88\n",
      "Loss: 1166.24 | loss_i: 1033.47 loss_j: 1616.00 loss_edge_type: 954.05 loss_i_size: 1084.26 loss_j_size: 1143.41\n",
      "Loss: 1057.22 | loss_i: 931.05 loss_j: 1488.99 loss_edge_type: 895.07 loss_i_size: 960.73 loss_j_size: 1010.26\n",
      "Loss: 1098.83 | loss_i: 1019.52 loss_j: 1636.60 loss_edge_type: 896.03 loss_i_size: 955.47 loss_j_size: 986.54\n",
      "Loss: 1009.98 | loss_i: 878.17 loss_j: 1372.10 loss_edge_type: 870.58 loss_i_size: 954.77 loss_j_size: 974.27\n",
      "Loss: 1090.65 | loss_i: 991.62 loss_j: 1580.83 loss_edge_type: 898.38 loss_i_size: 971.28 loss_j_size: 1011.17\n",
      "Loss: 1074.89 | loss_i: 846.34 loss_j: 1273.50 loss_edge_type: 928.82 loss_i_size: 1138.76 loss_j_size: 1187.04\n",
      "Loss: 914.25 | loss_i: 764.37 loss_j: 1174.73 loss_edge_type: 868.14 loss_i_size: 867.51 loss_j_size: 896.51\n",
      "Loss: 956.84 | loss_i: 821.45 loss_j: 1266.10 loss_edge_type: 903.55 loss_i_size: 876.73 loss_j_size: 916.36\n",
      "Loss: 948.77 | loss_i: 814.05 loss_j: 1260.84 loss_edge_type: 855.96 loss_i_size: 889.39 loss_j_size: 923.63\n",
      "Loss: 909.49 | loss_i: 788.57 loss_j: 1189.23 loss_edge_type: 828.23 loss_i_size: 853.01 loss_j_size: 888.38\n",
      "Loss: 944.02 | loss_i: 789.75 loss_j: 1245.90 loss_edge_type: 849.00 loss_i_size: 895.02 loss_j_size: 940.42\n",
      "Loss: 966.30 | loss_i: 966.43 loss_j: 1386.28 loss_edge_type: 850.46 loss_i_size: 798.04 loss_j_size: 830.29\n",
      "Loss: 975.49 | loss_i: 804.44 loss_j: 1264.43 loss_edge_type: 859.50 loss_i_size: 964.30 loss_j_size: 984.76\n",
      "Loss: 953.22 | loss_i: 795.93 loss_j: 1187.69 loss_edge_type: 853.92 loss_i_size: 943.67 loss_j_size: 984.89\n",
      "Loss: 856.50 | loss_i: 692.82 loss_j: 1113.59 loss_edge_type: 784.31 loss_i_size: 821.41 loss_j_size: 870.35\n",
      "Loss: 942.34 | loss_i: 819.82 loss_j: 1212.41 loss_edge_type: 801.35 loss_i_size: 923.22 loss_j_size: 954.89\n",
      "Loss: 1302.40 | loss_i: 1557.31 loss_j: 2357.04 loss_edge_type: 994.29 loss_i_size: 772.23 loss_j_size: 831.14\n",
      "Loss: 1180.32 | loss_i: 1144.32 loss_j: 1728.51 loss_edge_type: 997.70 loss_i_size: 995.04 loss_j_size: 1036.00\n",
      "Loss: 1050.86 | loss_i: 922.57 loss_j: 1483.04 loss_edge_type: 851.80 loss_i_size: 975.82 loss_j_size: 1021.06\n",
      "Loss: 1047.76 | loss_i: 876.71 loss_j: 1393.55 loss_edge_type: 914.71 loss_i_size: 1005.52 loss_j_size: 1048.31\n",
      "Loss: 889.08 | loss_i: 661.90 loss_j: 1035.59 loss_edge_type: 794.33 loss_i_size: 958.68 loss_j_size: 994.91\n",
      "Loss: 1145.77 | loss_i: 1018.63 loss_j: 1586.06 loss_edge_type: 918.04 loss_i_size: 1077.59 loss_j_size: 1128.51\n",
      "Loss: 930.37 | loss_i: 801.92 loss_j: 1217.09 loss_edge_type: 843.45 loss_i_size: 877.05 loss_j_size: 912.32\n",
      "Loss: 1083.96 | loss_i: 964.19 loss_j: 1565.49 loss_edge_type: 915.82 loss_i_size: 968.09 loss_j_size: 1006.23\n",
      "Loss: 992.60 | loss_i: 824.45 loss_j: 1299.39 loss_edge_type: 867.35 loss_i_size: 958.00 loss_j_size: 1013.83\n",
      "Loss: 920.70 | loss_i: 792.86 loss_j: 1208.70 loss_edge_type: 857.52 loss_i_size: 839.32 loss_j_size: 905.08\n",
      "Loss: 1041.75 | loss_i: 1004.99 loss_j: 1572.58 loss_edge_type: 936.56 loss_i_size: 828.14 loss_j_size: 866.49\n",
      "Loss: 1026.04 | loss_i: 806.71 loss_j: 1279.51 loss_edge_type: 864.22 loss_i_size: 1073.97 loss_j_size: 1105.78\n",
      "Loss: 878.04 | loss_i: 702.76 loss_j: 1097.77 loss_edge_type: 828.95 loss_i_size: 865.34 loss_j_size: 895.39\n",
      "Loss: 780.64 | loss_i: 614.75 loss_j: 915.84 loss_edge_type: 748.67 loss_i_size: 787.70 loss_j_size: 836.23\n",
      "Loss: 969.06 | loss_i: 826.07 loss_j: 1264.49 loss_edge_type: 850.22 loss_i_size: 925.08 loss_j_size: 979.43\n",
      "Loss: 1032.97 | loss_i: 925.25 loss_j: 1489.74 loss_edge_type: 867.91 loss_i_size: 923.98 loss_j_size: 957.98\n",
      "Loss: 1264.67 | loss_i: 1076.57 loss_j: 1693.93 loss_edge_type: 1029.35 loss_i_size: 1249.05 loss_j_size: 1274.44\n",
      "Loss: 997.82 | loss_i: 953.09 loss_j: 1430.97 loss_edge_type: 889.59 loss_i_size: 839.06 loss_j_size: 876.39\n",
      "Loss: 988.19 | loss_i: 863.32 loss_j: 1393.09 loss_edge_type: 888.33 loss_i_size: 869.80 loss_j_size: 926.39\n",
      "Loss: 911.84 | loss_i: 753.94 loss_j: 1214.45 loss_edge_type: 791.08 loss_i_size: 865.69 loss_j_size: 934.04\n",
      "Loss: 852.57 | loss_i: 705.01 loss_j: 1128.88 loss_edge_type: 812.78 loss_i_size: 780.19 loss_j_size: 835.99\n",
      "Loss: 895.75 | loss_i: 765.79 loss_j: 1206.24 loss_edge_type: 814.97 loss_i_size: 830.55 loss_j_size: 861.23\n",
      "Loss: 1039.99 | loss_i: 882.39 loss_j: 1399.70 loss_edge_type: 907.54 loss_i_size: 990.96 loss_j_size: 1019.37\n",
      "Loss: 867.30 | loss_i: 630.07 loss_j: 1025.54 loss_edge_type: 759.57 loss_i_size: 945.30 loss_j_size: 976.01\n",
      "Loss: 109.23 | loss_i: 81.65 loss_j: 149.25 loss_edge_type: 94.55 loss_i_size: 107.10 loss_j_size: 113.60\n",
      "Validation Loss: 1042.73 | loss_i: 955.97 loss_j: 1578.53 loss_edge_type: 837.82 loss_i_size: 906.40 loss_j_size: 934.93\n",
      "Validation Loss: 1228.58 | loss_i: 1249.38 loss_j: 1979.05 loss_edge_type: 994.16 loss_i_size: 934.61 loss_j_size: 985.69\n",
      "Validation Loss: 761.24 | loss_i: 547.36 loss_j: 955.18 loss_edge_type: 696.72 loss_i_size: 772.66 loss_j_size: 834.30\n",
      "Validation Loss: 844.47 | loss_i: 647.44 loss_j: 1075.77 loss_edge_type: 828.57 loss_i_size: 808.86 loss_j_size: 861.74\n",
      "Validation Loss: 1010.12 | loss_i: 898.32 loss_j: 1498.75 loss_edge_type: 829.47 loss_i_size: 883.49 loss_j_size: 940.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4/20 [00:42<02:47, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 524.05 | loss_i: 369.86 loss_j: 642.91 loss_edge_type: 492.91 loss_i_size: 541.52 loss_j_size: 573.06\n",
      "Training loss: 1015.3873\n",
      "Validation loss: 901.8653\n",
      "\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Loss: 952.32 | loss_i: 760.72 loss_j: 1211.03 loss_edge_type: 811.82 loss_i_size: 965.51 loss_j_size: 1012.52\n",
      "Loss: 957.85 | loss_i: 872.84 loss_j: 1329.18 loss_edge_type: 830.94 loss_i_size: 865.49 loss_j_size: 890.81\n",
      "Loss: 788.57 | loss_i: 597.84 loss_j: 987.76 loss_edge_type: 741.61 loss_i_size: 782.37 loss_j_size: 833.25\n",
      "Loss: 1012.01 | loss_i: 979.29 loss_j: 1439.47 loss_edge_type: 872.90 loss_i_size: 876.86 loss_j_size: 891.55\n",
      "Loss: 918.26 | loss_i: 749.28 loss_j: 1210.15 loss_edge_type: 842.79 loss_i_size: 870.88 loss_j_size: 918.21\n",
      "Loss: 850.06 | loss_i: 649.71 loss_j: 1080.58 loss_edge_type: 807.55 loss_i_size: 838.34 loss_j_size: 874.15\n",
      "Loss: 1088.04 | loss_i: 947.92 loss_j: 1531.19 loss_edge_type: 899.92 loss_i_size: 1007.55 loss_j_size: 1053.61\n",
      "Loss: 1082.88 | loss_i: 901.75 loss_j: 1451.47 loss_edge_type: 915.66 loss_i_size: 1061.36 loss_j_size: 1084.18\n",
      "Loss: 878.01 | loss_i: 786.52 loss_j: 1206.39 loss_edge_type: 797.11 loss_i_size: 777.98 loss_j_size: 822.03\n",
      "Loss: 867.06 | loss_i: 768.09 loss_j: 1169.67 loss_edge_type: 786.26 loss_i_size: 782.51 loss_j_size: 828.79\n",
      "Loss: 828.61 | loss_i: 638.32 loss_j: 1062.33 loss_edge_type: 772.08 loss_i_size: 817.18 loss_j_size: 853.13\n",
      "Loss: 1066.78 | loss_i: 931.76 loss_j: 1571.25 loss_edge_type: 934.40 loss_i_size: 925.78 loss_j_size: 970.73\n",
      "Loss: 1040.96 | loss_i: 926.62 loss_j: 1445.56 loss_edge_type: 879.78 loss_i_size: 944.64 loss_j_size: 1008.19\n",
      "Loss: 830.30 | loss_i: 614.42 loss_j: 973.35 loss_edge_type: 768.10 loss_i_size: 876.40 loss_j_size: 919.24\n",
      "Loss: 864.88 | loss_i: 671.84 loss_j: 1070.39 loss_edge_type: 795.33 loss_i_size: 869.25 loss_j_size: 917.57\n",
      "Loss: 944.19 | loss_i: 761.02 loss_j: 1262.85 loss_edge_type: 838.60 loss_i_size: 900.97 loss_j_size: 957.50\n",
      "Loss: 829.50 | loss_i: 651.69 loss_j: 1050.18 loss_edge_type: 752.66 loss_i_size: 805.45 loss_j_size: 887.54\n",
      "Loss: 993.91 | loss_i: 860.19 loss_j: 1381.83 loss_edge_type: 850.72 loss_i_size: 914.45 loss_j_size: 962.38\n",
      "Loss: 966.13 | loss_i: 744.02 loss_j: 1263.92 loss_edge_type: 859.47 loss_i_size: 948.62 loss_j_size: 1014.63\n",
      "Loss: 930.72 | loss_i: 735.59 loss_j: 1187.58 loss_edge_type: 843.13 loss_i_size: 920.04 loss_j_size: 967.25\n",
      "Loss: 965.83 | loss_i: 773.26 loss_j: 1275.05 loss_edge_type: 857.84 loss_i_size: 937.38 loss_j_size: 985.63\n",
      "Loss: 955.92 | loss_i: 876.68 loss_j: 1426.81 loss_edge_type: 883.77 loss_i_size: 769.27 loss_j_size: 823.06\n",
      "Loss: 768.80 | loss_i: 643.36 loss_j: 1002.72 loss_edge_type: 713.52 loss_i_size: 719.97 loss_j_size: 764.43\n",
      "Loss: 868.11 | loss_i: 718.30 loss_j: 1161.71 loss_edge_type: 804.57 loss_i_size: 795.12 loss_j_size: 860.86\n",
      "Loss: 990.69 | loss_i: 792.42 loss_j: 1378.78 loss_edge_type: 872.35 loss_i_size: 933.54 loss_j_size: 976.37\n",
      "Loss: 885.23 | loss_i: 716.36 loss_j: 1160.26 loss_edge_type: 811.16 loss_i_size: 841.56 loss_j_size: 896.80\n",
      "Loss: 1002.78 | loss_i: 860.28 loss_j: 1412.76 loss_edge_type: 886.31 loss_i_size: 901.26 loss_j_size: 953.28\n",
      "Loss: 917.58 | loss_i: 765.33 loss_j: 1251.76 loss_edge_type: 828.35 loss_i_size: 837.45 loss_j_size: 904.99\n",
      "Loss: 844.14 | loss_i: 644.56 loss_j: 1011.86 loss_edge_type: 760.36 loss_i_size: 878.87 loss_j_size: 925.06\n",
      "Loss: 909.09 | loss_i: 709.83 loss_j: 1158.30 loss_edge_type: 807.45 loss_i_size: 905.18 loss_j_size: 964.71\n",
      "Loss: 1043.86 | loss_i: 995.47 loss_j: 1570.82 loss_edge_type: 895.70 loss_i_size: 849.98 loss_j_size: 907.31\n",
      "Loss: 943.29 | loss_i: 811.85 loss_j: 1307.42 loss_edge_type: 826.13 loss_i_size: 860.11 loss_j_size: 910.95\n",
      "Loss: 890.19 | loss_i: 717.36 loss_j: 1219.96 loss_edge_type: 786.36 loss_i_size: 834.68 loss_j_size: 892.58\n",
      "Loss: 746.86 | loss_i: 562.91 loss_j: 904.15 loss_edge_type: 732.03 loss_i_size: 746.45 loss_j_size: 788.78\n",
      "Loss: 851.96 | loss_i: 647.99 loss_j: 1072.43 loss_edge_type: 815.79 loss_i_size: 844.66 loss_j_size: 878.91\n",
      "Loss: 1115.78 | loss_i: 916.77 loss_j: 1495.70 loss_edge_type: 913.15 loss_i_size: 1100.84 loss_j_size: 1152.42\n",
      "Loss: 846.99 | loss_i: 679.76 loss_j: 1134.44 loss_edge_type: 783.80 loss_i_size: 792.55 loss_j_size: 844.41\n",
      "Loss: 969.53 | loss_i: 772.01 loss_j: 1217.82 loss_edge_type: 873.70 loss_i_size: 964.91 loss_j_size: 1019.21\n",
      "Loss: 844.74 | loss_i: 718.43 loss_j: 1149.86 loss_edge_type: 787.40 loss_i_size: 755.69 loss_j_size: 812.32\n",
      "Loss: 1047.40 | loss_i: 1009.58 loss_j: 1656.66 loss_edge_type: 872.84 loss_i_size: 823.20 loss_j_size: 874.74\n",
      "Loss: 940.99 | loss_i: 839.96 loss_j: 1411.19 loss_edge_type: 821.20 loss_i_size: 783.21 loss_j_size: 849.40\n",
      "Loss: 871.44 | loss_i: 696.84 loss_j: 1115.71 loss_edge_type: 783.81 loss_i_size: 864.39 loss_j_size: 896.44\n",
      "Loss: 922.98 | loss_i: 751.03 loss_j: 1269.43 loss_edge_type: 825.63 loss_i_size: 856.67 loss_j_size: 912.15\n",
      "Loss: 845.93 | loss_i: 606.54 loss_j: 1036.28 loss_edge_type: 747.97 loss_i_size: 890.04 loss_j_size: 948.83\n",
      "Loss: 852.20 | loss_i: 699.83 loss_j: 1095.90 loss_edge_type: 793.02 loss_i_size: 808.07 loss_j_size: 864.16\n",
      "Loss: 973.09 | loss_i: 864.22 loss_j: 1365.10 loss_edge_type: 793.07 loss_i_size: 900.42 loss_j_size: 942.65\n",
      "Loss: 898.30 | loss_i: 761.31 loss_j: 1303.95 loss_edge_type: 839.81 loss_i_size: 771.41 loss_j_size: 815.02\n",
      "Loss: 921.11 | loss_i: 756.60 loss_j: 1278.45 loss_edge_type: 788.08 loss_i_size: 864.29 loss_j_size: 918.13\n",
      "Loss: 985.02 | loss_i: 824.84 loss_j: 1456.76 loss_edge_type: 878.65 loss_i_size: 855.35 loss_j_size: 909.49\n",
      "Loss: 869.23 | loss_i: 655.17 loss_j: 1069.46 loss_edge_type: 801.42 loss_i_size: 876.08 loss_j_size: 943.99\n",
      "Loss: 865.75 | loss_i: 735.59 loss_j: 1185.47 loss_edge_type: 823.94 loss_i_size: 762.16 loss_j_size: 821.60\n",
      "Loss: 98.72 | loss_i: 68.68 loss_j: 115.20 loss_edge_type: 101.25 loss_i_size: 102.49 loss_j_size: 105.96\n",
      "Validation Loss: 869.28 | loss_i: 662.32 loss_j: 1098.00 loss_edge_type: 788.23 loss_i_size: 866.76 loss_j_size: 931.11\n",
      "Validation Loss: 1055.93 | loss_i: 955.13 loss_j: 1576.30 loss_edge_type: 922.43 loss_i_size: 889.70 loss_j_size: 936.10\n",
      "Validation Loss: 819.72 | loss_i: 600.96 loss_j: 1048.80 loss_edge_type: 772.39 loss_i_size: 813.45 loss_j_size: 863.02\n",
      "Validation Loss: 782.86 | loss_i: 547.08 loss_j: 927.47 loss_edge_type: 750.42 loss_i_size: 816.22 loss_j_size: 873.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:53<02:39, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 906.20 | loss_i: 790.80 loss_j: 1378.13 loss_edge_type: 777.41 loss_i_size: 748.38 loss_j_size: 836.26\n",
      "Validation Loss: 583.43 | loss_i: 573.14 loss_j: 969.62 loss_edge_type: 527.19 loss_i_size: 409.41 loss_j_size: 437.81\n",
      "Training loss: 906.6265\n",
      "Validation loss: 836.2372\n",
      "\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Loss: 915.14 | loss_i: 715.28 loss_j: 1216.66 loss_edge_type: 854.83 loss_i_size: 864.76 loss_j_size: 924.15\n",
      "Loss: 830.26 | loss_i: 625.92 loss_j: 1043.02 loss_edge_type: 784.36 loss_i_size: 815.94 loss_j_size: 882.07\n",
      "Loss: 897.33 | loss_i: 694.63 loss_j: 1231.44 loss_edge_type: 766.37 loss_i_size: 868.12 loss_j_size: 926.08\n",
      "Loss: 772.17 | loss_i: 624.76 loss_j: 990.15 loss_edge_type: 731.50 loss_i_size: 734.28 loss_j_size: 780.14\n",
      "Loss: 728.63 | loss_i: 483.91 loss_j: 766.57 loss_edge_type: 709.74 loss_i_size: 813.73 loss_j_size: 869.21\n",
      "Loss: 1000.43 | loss_i: 788.64 loss_j: 1388.87 loss_edge_type: 857.33 loss_i_size: 960.57 loss_j_size: 1006.72\n",
      "Loss: 831.49 | loss_i: 627.12 loss_j: 1043.74 loss_edge_type: 757.06 loss_i_size: 832.77 loss_j_size: 896.77\n",
      "Loss: 913.04 | loss_i: 769.06 loss_j: 1217.54 loss_edge_type: 835.58 loss_i_size: 843.09 loss_j_size: 899.92\n",
      "Loss: 1014.89 | loss_i: 822.30 loss_j: 1315.18 loss_edge_type: 903.96 loss_i_size: 984.06 loss_j_size: 1048.96\n",
      "Loss: 874.75 | loss_i: 646.18 loss_j: 1091.71 loss_edge_type: 807.62 loss_i_size: 890.17 loss_j_size: 938.10\n",
      "Loss: 802.30 | loss_i: 684.01 loss_j: 1148.27 loss_edge_type: 766.30 loss_i_size: 680.91 loss_j_size: 732.02\n",
      "Loss: 765.29 | loss_i: 564.76 loss_j: 906.71 loss_edge_type: 722.38 loss_i_size: 795.40 loss_j_size: 837.19\n",
      "Loss: 855.75 | loss_i: 639.84 loss_j: 1045.65 loss_edge_type: 746.43 loss_i_size: 887.17 loss_j_size: 959.64\n",
      "Loss: 1099.67 | loss_i: 1035.39 loss_j: 1750.55 loss_edge_type: 885.43 loss_i_size: 884.82 loss_j_size: 942.13\n",
      "Loss: 853.39 | loss_i: 689.78 loss_j: 1154.42 loss_edge_type: 783.70 loss_i_size: 790.54 loss_j_size: 848.48\n",
      "Loss: 889.28 | loss_i: 732.08 loss_j: 1217.86 loss_edge_type: 830.33 loss_i_size: 806.62 loss_j_size: 859.50\n",
      "Loss: 816.26 | loss_i: 600.19 loss_j: 1025.88 loss_edge_type: 764.53 loss_i_size: 822.16 loss_j_size: 868.52\n",
      "Loss: 816.09 | loss_i: 691.73 loss_j: 1104.09 loss_edge_type: 763.10 loss_i_size: 733.01 loss_j_size: 788.50\n",
      "Loss: 931.22 | loss_i: 682.46 loss_j: 1213.48 loss_edge_type: 830.03 loss_i_size: 929.36 loss_j_size: 1000.75\n",
      "Loss: 933.00 | loss_i: 704.12 loss_j: 1167.27 loss_edge_type: 855.42 loss_i_size: 941.88 loss_j_size: 996.29\n",
      "Loss: 782.69 | loss_i: 642.26 loss_j: 1033.34 loss_edge_type: 715.93 loss_i_size: 738.31 loss_j_size: 783.60\n",
      "Loss: 743.76 | loss_i: 552.14 loss_j: 881.43 loss_edge_type: 699.31 loss_i_size: 761.08 loss_j_size: 824.83\n",
      "Loss: 856.82 | loss_i: 737.32 loss_j: 1203.05 loss_edge_type: 818.08 loss_i_size: 732.00 loss_j_size: 793.65\n",
      "Loss: 812.58 | loss_i: 581.98 loss_j: 1003.16 loss_edge_type: 735.51 loss_i_size: 848.41 loss_j_size: 893.82\n",
      "Loss: 784.39 | loss_i: 632.56 loss_j: 1002.97 loss_edge_type: 772.18 loss_i_size: 734.46 loss_j_size: 779.79\n",
      "Loss: 813.66 | loss_i: 666.65 loss_j: 1120.39 loss_edge_type: 780.67 loss_i_size: 731.66 loss_j_size: 768.94\n",
      "Loss: 1112.26 | loss_i: 1069.58 loss_j: 1796.29 loss_edge_type: 931.38 loss_i_size: 857.52 loss_j_size: 906.51\n",
      "Loss: 907.68 | loss_i: 665.27 loss_j: 1186.54 loss_edge_type: 843.49 loss_i_size: 890.69 loss_j_size: 952.39\n",
      "Loss: 758.36 | loss_i: 583.65 loss_j: 923.39 loss_edge_type: 722.91 loss_i_size: 748.39 loss_j_size: 813.45\n",
      "Loss: 685.20 | loss_i: 483.23 loss_j: 762.27 loss_edge_type: 663.35 loss_i_size: 731.17 loss_j_size: 785.98\n",
      "Loss: 869.57 | loss_i: 698.29 loss_j: 1201.70 loss_edge_type: 820.93 loss_i_size: 784.41 loss_j_size: 842.54\n",
      "Loss: 828.81 | loss_i: 686.01 loss_j: 1097.42 loss_edge_type: 767.04 loss_i_size: 764.99 loss_j_size: 828.56\n",
      "Loss: 828.01 | loss_i: 615.89 loss_j: 1112.26 loss_edge_type: 788.64 loss_i_size: 768.54 loss_j_size: 854.72\n",
      "Loss: 955.37 | loss_i: 775.18 loss_j: 1296.42 loss_edge_type: 890.50 loss_i_size: 866.83 loss_j_size: 947.94\n",
      "Loss: 827.79 | loss_i: 661.05 loss_j: 1142.41 loss_edge_type: 752.94 loss_i_size: 761.83 loss_j_size: 820.74\n",
      "Loss: 792.94 | loss_i: 598.66 loss_j: 1000.61 loss_edge_type: 747.42 loss_i_size: 784.18 loss_j_size: 833.84\n",
      "Loss: 714.73 | loss_i: 556.71 loss_j: 900.98 loss_edge_type: 668.96 loss_i_size: 685.10 loss_j_size: 761.92\n",
      "Loss: 934.62 | loss_i: 807.95 loss_j: 1299.72 loss_edge_type: 843.96 loss_i_size: 838.79 loss_j_size: 882.68\n",
      "Loss: 1053.14 | loss_i: 908.25 loss_j: 1500.84 loss_edge_type: 916.70 loss_i_size: 922.25 loss_j_size: 1017.64\n",
      "Loss: 875.04 | loss_i: 663.94 loss_j: 1139.26 loss_edge_type: 778.73 loss_i_size: 870.18 loss_j_size: 923.10\n",
      "Loss: 880.81 | loss_i: 667.32 loss_j: 1121.83 loss_edge_type: 775.32 loss_i_size: 907.93 loss_j_size: 931.65\n",
      "Loss: 826.41 | loss_i: 706.85 loss_j: 1156.67 loss_edge_type: 754.29 loss_i_size: 730.29 loss_j_size: 783.92\n",
      "Loss: 842.43 | loss_i: 618.49 loss_j: 1001.55 loss_edge_type: 806.04 loss_i_size: 863.21 loss_j_size: 922.87\n",
      "Loss: 788.14 | loss_i: 628.21 loss_j: 1068.96 loss_edge_type: 725.52 loss_i_size: 719.03 loss_j_size: 798.97\n",
      "Loss: 858.22 | loss_i: 683.59 loss_j: 1235.76 loss_edge_type: 758.11 loss_i_size: 779.06 loss_j_size: 834.57\n",
      "Loss: 724.69 | loss_i: 546.47 loss_j: 920.94 loss_edge_type: 710.67 loss_i_size: 695.20 loss_j_size: 750.19\n",
      "Loss: 794.26 | loss_i: 693.02 loss_j: 1084.39 loss_edge_type: 753.81 loss_i_size: 682.85 loss_j_size: 757.23\n",
      "Loss: 692.95 | loss_i: 480.75 loss_j: 810.17 loss_edge_type: 695.29 loss_i_size: 701.07 loss_j_size: 777.48\n",
      "Loss: 774.41 | loss_i: 568.83 loss_j: 1015.67 loss_edge_type: 702.77 loss_i_size: 764.10 loss_j_size: 820.66\n",
      "Loss: 789.01 | loss_i: 593.82 loss_j: 1015.16 loss_edge_type: 686.76 loss_i_size: 796.43 loss_j_size: 852.88\n",
      "Loss: 774.28 | loss_i: 582.07 loss_j: 985.35 loss_edge_type: 718.97 loss_i_size: 763.14 loss_j_size: 821.86\n",
      "Loss: 144.86 | loss_i: 140.09 loss_j: 250.18 loss_edge_type: 113.28 loss_i_size: 105.40 loss_j_size: 115.35\n",
      "Validation Loss: 606.76 | loss_i: 396.93 loss_j: 669.68 loss_edge_type: 617.59 loss_i_size: 640.28 loss_j_size: 709.29\n",
      "Validation Loss: 742.69 | loss_i: 644.95 loss_j: 1102.46 loss_edge_type: 691.48 loss_i_size: 606.40 loss_j_size: 668.15\n",
      "Validation Loss: 739.08 | loss_i: 569.76 loss_j: 956.22 loss_edge_type: 680.92 loss_i_size: 710.27 loss_j_size: 778.25\n",
      "Validation Loss: 1110.38 | loss_i: 1102.02 loss_j: 1919.81 loss_edge_type: 914.80 loss_i_size: 764.08 loss_j_size: 851.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [01:03<02:29, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 779.13 | loss_i: 581.89 loss_j: 1028.69 loss_edge_type: 732.22 loss_i_size: 747.83 loss_j_size: 805.01\n",
      "Validation Loss: 574.85 | loss_i: 401.28 loss_j: 717.43 loss_edge_type: 480.21 loss_i_size: 614.72 loss_j_size: 660.62\n",
      "Training loss: 834.0047\n",
      "Validation loss: 758.8141\n",
      "\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Loss: 759.92 | loss_i: 540.91 loss_j: 875.01 loss_edge_type: 681.82 loss_i_size: 818.93 loss_j_size: 882.96\n",
      "Loss: 847.55 | loss_i: 686.93 loss_j: 1175.42 loss_edge_type: 785.14 loss_i_size: 760.62 loss_j_size: 829.64\n",
      "Loss: 842.85 | loss_i: 672.57 loss_j: 1167.07 loss_edge_type: 788.00 loss_i_size: 761.60 loss_j_size: 825.03\n",
      "Loss: 721.57 | loss_i: 507.87 loss_j: 902.90 loss_edge_type: 685.36 loss_i_size: 715.12 loss_j_size: 796.58\n",
      "Loss: 664.19 | loss_i: 445.91 loss_j: 778.49 loss_edge_type: 665.52 loss_i_size: 687.08 loss_j_size: 743.92\n",
      "Loss: 860.44 | loss_i: 693.58 loss_j: 1110.81 loss_edge_type: 757.04 loss_i_size: 838.66 loss_j_size: 902.11\n",
      "Loss: 708.57 | loss_i: 517.22 loss_j: 815.60 loss_edge_type: 650.04 loss_i_size: 737.91 loss_j_size: 822.09\n",
      "Loss: 810.25 | loss_i: 641.46 loss_j: 1046.71 loss_edge_type: 769.78 loss_i_size: 751.12 loss_j_size: 842.17\n",
      "Loss: 921.87 | loss_i: 713.28 loss_j: 1174.47 loss_edge_type: 888.36 loss_i_size: 893.35 loss_j_size: 939.86\n",
      "Loss: 823.49 | loss_i: 650.48 loss_j: 1154.25 loss_edge_type: 774.69 loss_i_size: 740.63 loss_j_size: 797.39\n",
      "Loss: 812.50 | loss_i: 631.80 loss_j: 1115.76 loss_edge_type: 741.56 loss_i_size: 755.15 loss_j_size: 818.23\n",
      "Loss: 855.93 | loss_i: 668.74 loss_j: 1101.44 loss_edge_type: 807.37 loss_i_size: 816.44 loss_j_size: 885.65\n",
      "Loss: 767.61 | loss_i: 546.39 loss_j: 944.98 loss_edge_type: 737.49 loss_i_size: 776.43 loss_j_size: 832.75\n",
      "Loss: 747.29 | loss_i: 545.21 loss_j: 913.99 loss_edge_type: 717.91 loss_i_size: 756.79 loss_j_size: 802.54\n",
      "Loss: 758.85 | loss_i: 546.77 loss_j: 886.15 loss_edge_type: 741.88 loss_i_size: 769.71 loss_j_size: 849.75\n",
      "Loss: 750.17 | loss_i: 515.71 loss_j: 901.49 loss_edge_type: 729.94 loss_i_size: 777.39 loss_j_size: 826.33\n",
      "Loss: 851.63 | loss_i: 668.48 loss_j: 1148.01 loss_edge_type: 783.82 loss_i_size: 793.97 loss_j_size: 863.85\n",
      "Loss: 731.91 | loss_i: 478.24 loss_j: 845.83 loss_edge_type: 714.18 loss_i_size: 778.83 loss_j_size: 842.49\n",
      "Loss: 842.65 | loss_i: 665.25 loss_j: 1087.84 loss_edge_type: 758.13 loss_i_size: 806.97 loss_j_size: 895.05\n",
      "Loss: 1248.22 | loss_i: 1064.09 loss_j: 1810.32 loss_edge_type: 1017.22 loss_i_size: 1145.35 loss_j_size: 1204.12\n",
      "Loss: 780.74 | loss_i: 594.14 loss_j: 1008.22 loss_edge_type: 723.30 loss_i_size: 754.62 loss_j_size: 823.41\n",
      "Loss: 797.29 | loss_i: 671.88 loss_j: 1032.05 loss_edge_type: 720.62 loss_i_size: 754.71 loss_j_size: 807.19\n",
      "Loss: 812.49 | loss_i: 599.63 loss_j: 954.76 loss_edge_type: 760.99 loss_i_size: 836.63 loss_j_size: 910.44\n",
      "Loss: 792.31 | loss_i: 644.65 loss_j: 1069.75 loss_edge_type: 785.97 loss_i_size: 698.69 loss_j_size: 762.49\n",
      "Loss: 832.94 | loss_i: 604.60 loss_j: 1134.50 loss_edge_type: 752.62 loss_i_size: 799.82 loss_j_size: 873.17\n",
      "Loss: 755.52 | loss_i: 574.94 loss_j: 974.96 loss_edge_type: 702.67 loss_i_size: 730.17 loss_j_size: 794.86\n",
      "Loss: 792.37 | loss_i: 561.67 loss_j: 923.30 loss_edge_type: 722.87 loss_i_size: 833.71 loss_j_size: 920.30\n",
      "Loss: 764.63 | loss_i: 546.82 loss_j: 892.40 loss_edge_type: 690.11 loss_i_size: 821.76 loss_j_size: 872.07\n",
      "Loss: 811.23 | loss_i: 586.42 loss_j: 1012.69 loss_edge_type: 767.27 loss_i_size: 799.87 loss_j_size: 889.90\n",
      "Loss: 592.90 | loss_i: 411.62 loss_j: 646.49 loss_edge_type: 598.83 loss_i_size: 622.19 loss_j_size: 685.39\n",
      "Loss: 719.55 | loss_i: 462.89 loss_j: 799.75 loss_edge_type: 709.34 loss_i_size: 781.53 loss_j_size: 844.23\n",
      "Loss: 641.40 | loss_i: 471.47 loss_j: 823.76 loss_edge_type: 652.93 loss_i_size: 602.94 loss_j_size: 655.91\n",
      "Loss: 724.21 | loss_i: 536.00 loss_j: 935.47 loss_edge_type: 691.30 loss_i_size: 700.48 loss_j_size: 757.80\n",
      "Loss: 747.08 | loss_i: 556.23 loss_j: 922.30 loss_edge_type: 724.03 loss_i_size: 720.44 loss_j_size: 812.41\n",
      "Loss: 739.30 | loss_i: 495.62 loss_j: 852.11 loss_edge_type: 718.21 loss_i_size: 781.03 loss_j_size: 849.53\n",
      "Loss: 921.27 | loss_i: 836.33 loss_j: 1492.55 loss_edge_type: 821.82 loss_i_size: 689.80 loss_j_size: 765.88\n",
      "Loss: 676.38 | loss_i: 462.16 loss_j: 793.94 loss_edge_type: 669.68 loss_i_size: 690.98 loss_j_size: 765.15\n",
      "Loss: 766.88 | loss_i: 661.01 loss_j: 1120.88 loss_edge_type: 735.46 loss_i_size: 625.39 loss_j_size: 691.66\n",
      "Loss: 822.14 | loss_i: 652.86 loss_j: 1117.31 loss_edge_type: 765.67 loss_i_size: 755.45 loss_j_size: 819.42\n",
      "Loss: 860.05 | loss_i: 705.75 loss_j: 1216.01 loss_edge_type: 785.98 loss_i_size: 755.50 loss_j_size: 837.02\n",
      "Loss: 745.61 | loss_i: 557.41 loss_j: 942.48 loss_edge_type: 763.26 loss_i_size: 703.09 loss_j_size: 761.82\n",
      "Loss: 747.59 | loss_i: 540.18 loss_j: 971.49 loss_edge_type: 723.53 loss_i_size: 716.08 loss_j_size: 786.64\n",
      "Loss: 829.39 | loss_i: 708.26 loss_j: 1251.27 loss_edge_type: 764.12 loss_i_size: 672.16 loss_j_size: 751.14\n",
      "Loss: 728.61 | loss_i: 616.33 loss_j: 1036.96 loss_edge_type: 681.54 loss_i_size: 618.03 loss_j_size: 690.19\n",
      "Loss: 705.47 | loss_i: 486.18 loss_j: 849.51 loss_edge_type: 654.25 loss_i_size: 739.13 loss_j_size: 798.29\n",
      "Loss: 1112.94 | loss_i: 1036.73 loss_j: 1769.80 loss_edge_type: 933.43 loss_i_size: 875.51 loss_j_size: 949.22\n",
      "Loss: 772.88 | loss_i: 545.25 loss_j: 987.12 loss_edge_type: 715.50 loss_i_size: 772.37 loss_j_size: 844.16\n",
      "Loss: 739.00 | loss_i: 632.23 loss_j: 957.03 loss_edge_type: 723.51 loss_i_size: 649.52 loss_j_size: 732.73\n",
      "Loss: 731.82 | loss_i: 478.44 loss_j: 851.54 loss_edge_type: 685.23 loss_i_size: 789.57 loss_j_size: 854.33\n",
      "Loss: 844.40 | loss_i: 608.62 loss_j: 1091.45 loss_edge_type: 799.14 loss_i_size: 816.99 loss_j_size: 905.79\n",
      "Loss: 663.30 | loss_i: 474.32 loss_j: 803.19 loss_edge_type: 632.27 loss_i_size: 668.89 loss_j_size: 737.84\n",
      "Loss: 133.81 | loss_i: 125.44 loss_j: 258.02 loss_edge_type: 123.34 loss_i_size: 76.84 loss_j_size: 85.40\n",
      "Validation Loss: 647.56 | loss_i: 544.40 loss_j: 895.42 loss_edge_type: 620.18 loss_i_size: 548.93 loss_j_size: 628.88\n",
      "Validation Loss: 781.51 | loss_i: 585.41 loss_j: 1087.08 loss_edge_type: 705.81 loss_i_size: 728.43 loss_j_size: 800.84\n",
      "Validation Loss: 747.84 | loss_i: 526.43 loss_j: 931.47 loss_edge_type: 696.89 loss_i_size: 757.60 loss_j_size: 826.81\n",
      "Validation Loss: 740.51 | loss_i: 572.38 loss_j: 1062.30 loss_edge_type: 686.40 loss_i_size: 653.19 loss_j_size: 728.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:14<02:17, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 901.81 | loss_i: 778.42 loss_j: 1299.89 loss_edge_type: 809.02 loss_i_size: 758.45 loss_j_size: 863.29\n",
      "Validation Loss: 463.18 | loss_i: 390.43 loss_j: 647.67 loss_edge_type: 437.32 loss_i_size: 393.63 loss_j_size: 446.85\n",
      "Training loss: 777.5570\n",
      "Validation loss: 713.7368\n",
      "\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Loss: 735.52 | loss_i: 537.69 loss_j: 916.94 loss_edge_type: 682.90 loss_i_size: 740.18 loss_j_size: 799.88\n",
      "Loss: 896.25 | loss_i: 781.76 loss_j: 1434.46 loss_edge_type: 772.31 loss_i_size: 702.24 loss_j_size: 790.48\n",
      "Loss: 835.81 | loss_i: 638.14 loss_j: 1117.23 loss_edge_type: 748.21 loss_i_size: 808.12 loss_j_size: 867.36\n",
      "Loss: 699.22 | loss_i: 507.00 loss_j: 880.57 loss_edge_type: 667.75 loss_i_size: 690.79 loss_j_size: 749.98\n",
      "Loss: 764.67 | loss_i: 558.58 loss_j: 984.61 loss_edge_type: 719.58 loss_i_size: 751.88 loss_j_size: 808.71\n",
      "Loss: 1073.60 | loss_i: 1063.36 loss_j: 1929.82 loss_edge_type: 845.46 loss_i_size: 722.35 loss_j_size: 807.03\n",
      "Loss: 780.86 | loss_i: 544.81 loss_j: 950.51 loss_edge_type: 749.00 loss_i_size: 794.03 loss_j_size: 865.94\n",
      "Loss: 706.46 | loss_i: 543.32 loss_j: 980.76 loss_edge_type: 710.10 loss_i_size: 630.62 loss_j_size: 667.53\n",
      "Loss: 915.44 | loss_i: 604.83 loss_j: 1057.03 loss_edge_type: 794.24 loss_i_size: 1042.29 loss_j_size: 1078.81\n",
      "Loss: 746.98 | loss_i: 554.70 loss_j: 923.64 loss_edge_type: 728.83 loss_i_size: 728.87 loss_j_size: 798.85\n",
      "Loss: 720.99 | loss_i: 533.44 loss_j: 885.26 loss_edge_type: 708.96 loss_i_size: 699.13 loss_j_size: 778.17\n",
      "Loss: 721.83 | loss_i: 495.69 loss_j: 837.54 loss_edge_type: 698.93 loss_i_size: 741.41 loss_j_size: 835.56\n",
      "Loss: 654.33 | loss_i: 453.58 loss_j: 790.72 loss_edge_type: 634.17 loss_i_size: 660.59 loss_j_size: 732.60\n",
      "Loss: 781.54 | loss_i: 572.89 loss_j: 972.81 loss_edge_type: 741.70 loss_i_size: 770.90 loss_j_size: 849.42\n",
      "Loss: 844.02 | loss_i: 727.32 loss_j: 1319.66 loss_edge_type: 814.02 loss_i_size: 636.80 loss_j_size: 722.31\n",
      "Loss: 788.30 | loss_i: 594.48 loss_j: 1003.10 loss_edge_type: 744.69 loss_i_size: 755.55 loss_j_size: 843.69\n",
      "Loss: 761.30 | loss_i: 568.02 loss_j: 963.30 loss_edge_type: 725.82 loss_i_size: 740.13 loss_j_size: 809.22\n",
      "Loss: 691.24 | loss_i: 495.65 loss_j: 808.88 loss_edge_type: 666.70 loss_i_size: 704.37 loss_j_size: 780.61\n",
      "Loss: 630.86 | loss_i: 464.94 loss_j: 809.30 loss_edge_type: 651.03 loss_i_size: 574.57 loss_j_size: 654.45\n",
      "Loss: 765.24 | loss_i: 549.86 loss_j: 961.53 loss_edge_type: 722.43 loss_i_size: 757.63 loss_j_size: 834.76\n",
      "Loss: 719.06 | loss_i: 535.21 loss_j: 984.85 loss_edge_type: 694.76 loss_i_size: 653.09 loss_j_size: 727.41\n",
      "Loss: 662.64 | loss_i: 452.94 loss_j: 760.95 loss_edge_type: 657.98 loss_i_size: 674.56 loss_j_size: 766.79\n",
      "Loss: 716.78 | loss_i: 454.28 loss_j: 752.66 loss_edge_type: 689.99 loss_i_size: 813.54 loss_j_size: 873.43\n",
      "Loss: 690.94 | loss_i: 442.96 loss_j: 796.83 loss_edge_type: 662.98 loss_i_size: 737.81 loss_j_size: 814.13\n",
      "Loss: 835.14 | loss_i: 737.21 loss_j: 1196.74 loss_edge_type: 762.68 loss_i_size: 699.84 loss_j_size: 779.26\n",
      "Loss: 844.74 | loss_i: 665.42 loss_j: 1146.30 loss_edge_type: 795.60 loss_i_size: 775.99 loss_j_size: 840.41\n",
      "Loss: 702.58 | loss_i: 502.34 loss_j: 841.91 loss_edge_type: 665.95 loss_i_size: 709.74 loss_j_size: 792.98\n",
      "Loss: 657.61 | loss_i: 479.02 loss_j: 813.96 loss_edge_type: 653.66 loss_i_size: 625.59 loss_j_size: 715.85\n",
      "Loss: 663.71 | loss_i: 441.24 loss_j: 729.66 loss_edge_type: 640.47 loss_i_size: 698.12 loss_j_size: 809.07\n",
      "Loss: 801.65 | loss_i: 618.56 loss_j: 1110.16 loss_edge_type: 717.69 loss_i_size: 734.22 loss_j_size: 827.63\n",
      "Loss: 674.91 | loss_i: 456.12 loss_j: 727.23 loss_edge_type: 661.31 loss_i_size: 714.37 loss_j_size: 815.51\n",
      "Loss: 777.78 | loss_i: 646.77 loss_j: 1065.63 loss_edge_type: 680.19 loss_i_size: 713.31 loss_j_size: 783.00\n",
      "Loss: 780.08 | loss_i: 642.35 loss_j: 1203.29 loss_edge_type: 740.08 loss_i_size: 623.10 loss_j_size: 691.60\n",
      "Loss: 687.76 | loss_i: 486.94 loss_j: 845.54 loss_edge_type: 700.64 loss_i_size: 671.26 loss_j_size: 734.41\n",
      "Loss: 732.81 | loss_i: 538.98 loss_j: 942.50 loss_edge_type: 709.60 loss_i_size: 685.12 loss_j_size: 787.87\n",
      "Loss: 833.71 | loss_i: 557.26 loss_j: 939.46 loss_edge_type: 761.54 loss_i_size: 928.50 loss_j_size: 981.79\n",
      "Loss: 746.92 | loss_i: 545.61 loss_j: 935.23 loss_edge_type: 706.61 loss_i_size: 723.94 loss_j_size: 823.20\n",
      "Loss: 739.02 | loss_i: 506.42 loss_j: 879.66 loss_edge_type: 731.44 loss_i_size: 754.87 loss_j_size: 822.70\n",
      "Loss: 732.41 | loss_i: 537.06 loss_j: 929.72 loss_edge_type: 695.97 loss_i_size: 704.54 loss_j_size: 794.77\n",
      "Loss: 693.58 | loss_i: 454.72 loss_j: 822.07 loss_edge_type: 662.66 loss_i_size: 720.89 loss_j_size: 807.56\n",
      "Loss: 722.77 | loss_i: 455.63 loss_j: 755.27 loss_edge_type: 682.85 loss_i_size: 809.91 loss_j_size: 910.19\n",
      "Loss: 808.56 | loss_i: 625.21 loss_j: 1107.96 loss_edge_type: 760.65 loss_i_size: 740.20 loss_j_size: 808.78\n",
      "Loss: 666.98 | loss_i: 445.68 loss_j: 816.23 loss_edge_type: 617.46 loss_i_size: 679.55 loss_j_size: 776.00\n",
      "Loss: 631.82 | loss_i: 447.12 loss_j: 764.68 loss_edge_type: 638.97 loss_i_size: 602.67 loss_j_size: 705.64\n",
      "Loss: 708.17 | loss_i: 518.63 loss_j: 931.38 loss_edge_type: 694.45 loss_i_size: 652.19 loss_j_size: 744.21\n",
      "Loss: 780.47 | loss_i: 569.88 loss_j: 1066.54 loss_edge_type: 726.41 loss_i_size: 720.55 loss_j_size: 818.96\n",
      "Loss: 757.88 | loss_i: 559.62 loss_j: 997.90 loss_edge_type: 694.74 loss_i_size: 725.04 loss_j_size: 812.07\n",
      "Loss: 906.12 | loss_i: 604.09 loss_j: 1062.82 loss_edge_type: 821.75 loss_i_size: 976.88 loss_j_size: 1065.06\n",
      "Loss: 664.92 | loss_i: 496.72 loss_j: 890.51 loss_edge_type: 636.07 loss_i_size: 607.25 loss_j_size: 694.03\n",
      "Loss: 529.09 | loss_i: 321.25 loss_j: 532.04 loss_edge_type: 555.21 loss_i_size: 570.16 loss_j_size: 666.79\n",
      "Loss: 700.53 | loss_i: 463.78 loss_j: 781.79 loss_edge_type: 700.22 loss_i_size: 745.44 loss_j_size: 811.44\n",
      "Loss: 125.47 | loss_i: 98.35 loss_j: 173.29 loss_edge_type: 113.85 loss_i_size: 111.17 loss_j_size: 130.66\n",
      "Validation Loss: 756.90 | loss_i: 558.18 loss_j: 1089.85 loss_edge_type: 699.68 loss_i_size: 667.22 loss_j_size: 769.59\n",
      "Validation Loss: 946.83 | loss_i: 856.21 loss_j: 1382.60 loss_edge_type: 796.11 loss_i_size: 803.00 loss_j_size: 896.25\n",
      "Validation Loss: 691.68 | loss_i: 447.81 loss_j: 830.46 loss_edge_type: 670.11 loss_i_size: 717.11 loss_j_size: 792.91\n",
      "Validation Loss: 761.27 | loss_i: 673.17 loss_j: 1107.99 loss_edge_type: 681.09 loss_i_size: 630.26 loss_j_size: 713.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:24<02:06, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 626.27 | loss_i: 438.91 loss_j: 842.24 loss_edge_type: 613.34 loss_i_size: 586.05 loss_j_size: 650.81\n",
      "Validation Loss: 348.71 | loss_i: 211.83 loss_j: 370.53 loss_edge_type: 350.50 loss_i_size: 368.62 loss_j_size: 442.07\n",
      "Training loss: 734.8290\n",
      "Validation loss: 688.6114\n",
      "\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Loss: 696.67 | loss_i: 506.16 loss_j: 900.81 loss_edge_type: 673.95 loss_i_size: 673.82 loss_j_size: 728.60\n",
      "Loss: 706.95 | loss_i: 478.62 loss_j: 838.98 loss_edge_type: 682.32 loss_i_size: 728.24 loss_j_size: 806.59\n",
      "Loss: 862.83 | loss_i: 713.88 loss_j: 1266.57 loss_edge_type: 778.55 loss_i_size: 735.74 loss_j_size: 819.41\n",
      "Loss: 601.39 | loss_i: 412.21 loss_j: 664.92 loss_edge_type: 581.46 loss_i_size: 647.46 loss_j_size: 700.88\n",
      "Loss: 1055.47 | loss_i: 870.92 loss_j: 1501.31 loss_edge_type: 893.19 loss_i_size: 979.72 loss_j_size: 1032.20\n",
      "Loss: 681.32 | loss_i: 420.51 loss_j: 766.43 loss_edge_type: 661.42 loss_i_size: 733.55 loss_j_size: 824.71\n",
      "Loss: 741.02 | loss_i: 541.31 loss_j: 929.42 loss_edge_type: 752.80 loss_i_size: 703.22 loss_j_size: 778.36\n",
      "Loss: 644.92 | loss_i: 452.69 loss_j: 764.87 loss_edge_type: 633.99 loss_i_size: 642.98 loss_j_size: 730.08\n",
      "Loss: 648.08 | loss_i: 436.73 loss_j: 801.92 loss_edge_type: 662.87 loss_i_size: 631.99 loss_j_size: 706.86\n",
      "Loss: 623.32 | loss_i: 375.04 loss_j: 636.72 loss_edge_type: 642.13 loss_i_size: 693.31 loss_j_size: 769.39\n",
      "Loss: 768.90 | loss_i: 558.95 loss_j: 1021.90 loss_edge_type: 715.37 loss_i_size: 739.74 loss_j_size: 808.53\n",
      "Loss: 635.26 | loss_i: 442.52 loss_j: 763.06 loss_edge_type: 624.90 loss_i_size: 629.47 loss_j_size: 716.36\n",
      "Loss: 666.50 | loss_i: 451.85 loss_j: 791.83 loss_edge_type: 657.20 loss_i_size: 666.83 loss_j_size: 764.79\n",
      "Loss: 772.77 | loss_i: 568.51 loss_j: 989.87 loss_edge_type: 714.80 loss_i_size: 751.66 loss_j_size: 839.02\n",
      "Loss: 726.88 | loss_i: 528.30 loss_j: 950.80 loss_edge_type: 707.99 loss_i_size: 681.90 loss_j_size: 765.41\n",
      "Loss: 664.07 | loss_i: 467.20 loss_j: 763.25 loss_edge_type: 642.35 loss_i_size: 675.13 loss_j_size: 772.45\n",
      "Loss: 640.08 | loss_i: 428.81 loss_j: 768.25 loss_edge_type: 620.95 loss_i_size: 651.08 loss_j_size: 731.30\n",
      "Loss: 634.85 | loss_i: 435.08 loss_j: 749.88 loss_edge_type: 646.49 loss_i_size: 628.88 loss_j_size: 713.93\n",
      "Loss: 758.68 | loss_i: 601.75 loss_j: 1126.93 loss_edge_type: 690.25 loss_i_size: 649.78 loss_j_size: 724.71\n",
      "Loss: 664.91 | loss_i: 449.77 loss_j: 801.85 loss_edge_type: 639.42 loss_i_size: 681.10 loss_j_size: 752.42\n",
      "Loss: 766.20 | loss_i: 544.43 loss_j: 938.73 loss_edge_type: 716.57 loss_i_size: 773.21 loss_j_size: 858.07\n",
      "Loss: 877.98 | loss_i: 781.09 loss_j: 1408.45 loss_edge_type: 752.80 loss_i_size: 679.54 loss_j_size: 768.03\n",
      "Loss: 611.15 | loss_i: 355.49 loss_j: 609.78 loss_edge_type: 608.23 loss_i_size: 704.51 loss_j_size: 777.72\n",
      "Loss: 643.56 | loss_i: 447.29 loss_j: 769.31 loss_edge_type: 653.37 loss_i_size: 634.07 loss_j_size: 713.77\n",
      "Loss: 690.10 | loss_i: 501.81 loss_j: 864.25 loss_edge_type: 672.87 loss_i_size: 660.57 loss_j_size: 750.99\n",
      "Loss: 554.14 | loss_i: 332.16 loss_j: 518.55 loss_edge_type: 542.38 loss_i_size: 642.78 loss_j_size: 734.82\n",
      "Loss: 802.21 | loss_i: 604.56 loss_j: 1048.44 loss_edge_type: 773.56 loss_i_size: 739.22 loss_j_size: 845.29\n",
      "Loss: 690.47 | loss_i: 454.61 loss_j: 854.62 loss_edge_type: 666.27 loss_i_size: 695.06 loss_j_size: 781.81\n",
      "Loss: 731.89 | loss_i: 507.61 loss_j: 871.17 loss_edge_type: 682.23 loss_i_size: 759.10 loss_j_size: 839.32\n",
      "Loss: 762.61 | loss_i: 619.62 loss_j: 1054.38 loss_edge_type: 725.30 loss_i_size: 662.30 loss_j_size: 751.46\n",
      "Loss: 690.33 | loss_i: 481.67 loss_j: 858.00 loss_edge_type: 672.52 loss_i_size: 676.36 loss_j_size: 763.08\n",
      "Loss: 659.89 | loss_i: 523.10 loss_j: 864.55 loss_edge_type: 650.80 loss_i_size: 585.05 loss_j_size: 675.96\n",
      "Loss: 774.10 | loss_i: 502.56 loss_j: 959.66 loss_edge_type: 689.55 loss_i_size: 795.02 loss_j_size: 923.69\n",
      "Loss: 686.16 | loss_i: 497.40 loss_j: 821.95 loss_edge_type: 680.04 loss_i_size: 664.96 loss_j_size: 766.43\n",
      "Loss: 647.79 | loss_i: 465.30 loss_j: 804.85 loss_edge_type: 652.47 loss_i_size: 622.08 loss_j_size: 694.26\n",
      "Loss: 733.51 | loss_i: 538.79 loss_j: 923.23 loss_edge_type: 683.57 loss_i_size: 709.60 loss_j_size: 812.37\n",
      "Loss: 610.03 | loss_i: 462.59 loss_j: 740.82 loss_edge_type: 666.17 loss_i_size: 563.03 loss_j_size: 617.55\n",
      "Loss: 929.92 | loss_i: 728.41 loss_j: 1417.89 loss_edge_type: 810.57 loss_i_size: 793.35 loss_j_size: 899.41\n",
      "Loss: 623.82 | loss_i: 427.59 loss_j: 748.28 loss_edge_type: 626.52 loss_i_size: 612.03 loss_j_size: 704.69\n",
      "Loss: 682.78 | loss_i: 504.39 loss_j: 891.71 loss_edge_type: 670.76 loss_i_size: 623.75 loss_j_size: 723.29\n",
      "Loss: 659.37 | loss_i: 435.34 loss_j: 727.26 loss_edge_type: 607.78 loss_i_size: 724.64 loss_j_size: 801.85\n",
      "Loss: 776.88 | loss_i: 570.71 loss_j: 1025.41 loss_edge_type: 715.91 loss_i_size: 738.50 loss_j_size: 833.86\n",
      "Loss: 719.95 | loss_i: 512.08 loss_j: 892.33 loss_edge_type: 723.54 loss_i_size: 684.44 loss_j_size: 787.38\n",
      "Loss: 649.39 | loss_i: 419.45 loss_j: 793.04 loss_edge_type: 647.88 loss_i_size: 650.03 loss_j_size: 736.54\n",
      "Loss: 776.78 | loss_i: 553.28 loss_j: 1034.83 loss_edge_type: 774.78 loss_i_size: 708.82 loss_j_size: 812.17\n",
      "Loss: 782.19 | loss_i: 657.07 loss_j: 1214.93 loss_edge_type: 695.48 loss_i_size: 634.57 loss_j_size: 708.91\n",
      "Loss: 791.07 | loss_i: 565.78 loss_j: 1021.96 loss_edge_type: 737.83 loss_i_size: 768.02 loss_j_size: 861.75\n",
      "Loss: 726.23 | loss_i: 505.43 loss_j: 921.18 loss_edge_type: 682.94 loss_i_size: 710.98 loss_j_size: 810.60\n",
      "Loss: 752.03 | loss_i: 488.79 loss_j: 892.42 loss_edge_type: 707.46 loss_i_size: 796.29 loss_j_size: 875.21\n",
      "Loss: 721.72 | loss_i: 485.76 loss_j: 880.26 loss_edge_type: 695.47 loss_i_size: 726.23 loss_j_size: 820.86\n",
      "Loss: 674.19 | loss_i: 438.24 loss_j: 777.45 loss_edge_type: 656.23 loss_i_size: 710.22 loss_j_size: 788.82\n",
      "Loss: 85.27 | loss_i: 62.82 loss_j: 95.97 loss_edge_type: 88.31 loss_i_size: 84.81 loss_j_size: 94.46\n",
      "Validation Loss: 826.39 | loss_i: 714.16 loss_j: 1260.73 loss_edge_type: 720.01 loss_i_size: 674.98 loss_j_size: 762.08\n",
      "Validation Loss: 696.92 | loss_i: 544.66 loss_j: 949.09 loss_edge_type: 663.06 loss_i_size: 605.65 loss_j_size: 722.13\n",
      "Validation Loss: 561.57 | loss_i: 352.43 loss_j: 639.41 loss_edge_type: 572.26 loss_i_size: 583.79 loss_j_size: 659.95\n",
      "Validation Loss: 766.57 | loss_i: 659.81 loss_j: 1166.55 loss_edge_type: 672.63 loss_i_size: 614.76 loss_j_size: 719.12\n",
      "Validation Loss: 649.41 | loss_i: 417.46 loss_j: 777.34 loss_edge_type: 606.82 loss_i_size: 669.01 loss_j_size: 776.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:35<01:55, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 424.41 | loss_i: 272.07 loss_j: 515.84 loss_edge_type: 427.47 loss_i_size: 432.96 loss_j_size: 473.71\n",
      "Training loss: 701.5112\n",
      "Validation loss: 654.2121\n",
      "\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Loss: 730.29 | loss_i: 504.20 loss_j: 910.00 loss_edge_type: 691.38 loss_i_size: 732.64 loss_j_size: 813.25\n",
      "Loss: 650.61 | loss_i: 460.37 loss_j: 837.32 loss_edge_type: 609.71 loss_i_size: 634.33 loss_j_size: 711.31\n",
      "Loss: 653.66 | loss_i: 448.48 loss_j: 849.58 loss_edge_type: 638.21 loss_i_size: 629.63 loss_j_size: 702.39\n",
      "Loss: 725.74 | loss_i: 505.02 loss_j: 895.42 loss_edge_type: 681.02 loss_i_size: 741.91 loss_j_size: 805.35\n",
      "Loss: 646.78 | loss_i: 432.32 loss_j: 741.31 loss_edge_type: 650.35 loss_i_size: 661.19 loss_j_size: 748.72\n",
      "Loss: 727.65 | loss_i: 543.71 loss_j: 917.48 loss_edge_type: 715.87 loss_i_size: 694.45 loss_j_size: 766.74\n",
      "Loss: 779.84 | loss_i: 560.51 loss_j: 959.80 loss_edge_type: 748.22 loss_i_size: 777.50 loss_j_size: 853.16\n",
      "Loss: 706.16 | loss_i: 588.52 loss_j: 1094.35 loss_edge_type: 640.75 loss_i_size: 574.59 loss_j_size: 632.58\n",
      "Loss: 813.48 | loss_i: 558.55 loss_j: 1057.60 loss_edge_type: 741.62 loss_i_size: 803.89 loss_j_size: 905.74\n",
      "Loss: 639.42 | loss_i: 390.98 loss_j: 711.19 loss_edge_type: 632.59 loss_i_size: 693.87 loss_j_size: 768.44\n",
      "Loss: 728.88 | loss_i: 513.11 loss_j: 861.64 loss_edge_type: 687.29 loss_i_size: 752.72 loss_j_size: 829.63\n",
      "Loss: 653.67 | loss_i: 411.07 loss_j: 771.84 loss_edge_type: 625.10 loss_i_size: 678.79 loss_j_size: 781.55\n",
      "Loss: 553.45 | loss_i: 364.63 loss_j: 616.79 loss_edge_type: 582.47 loss_i_size: 559.00 loss_j_size: 644.36\n",
      "Loss: 560.93 | loss_i: 390.29 loss_j: 646.48 loss_edge_type: 587.71 loss_i_size: 561.36 loss_j_size: 618.81\n",
      "Loss: 711.24 | loss_i: 447.51 loss_j: 817.46 loss_edge_type: 681.42 loss_i_size: 758.79 loss_j_size: 851.04\n",
      "Loss: 681.32 | loss_i: 529.59 loss_j: 861.74 loss_edge_type: 673.83 loss_i_size: 630.41 loss_j_size: 711.06\n",
      "Loss: 728.73 | loss_i: 525.31 loss_j: 951.87 loss_edge_type: 699.82 loss_i_size: 678.61 loss_j_size: 788.02\n",
      "Loss: 699.36 | loss_i: 466.60 loss_j: 879.59 loss_edge_type: 663.22 loss_i_size: 700.71 loss_j_size: 786.65\n",
      "Loss: 676.50 | loss_i: 492.11 loss_j: 876.13 loss_edge_type: 670.78 loss_i_size: 621.59 loss_j_size: 721.90\n",
      "Loss: 754.99 | loss_i: 484.25 loss_j: 895.65 loss_edge_type: 700.40 loss_i_size: 794.38 loss_j_size: 900.29\n",
      "Loss: 690.12 | loss_i: 483.92 loss_j: 847.70 loss_edge_type: 673.10 loss_i_size: 675.91 loss_j_size: 769.99\n",
      "Loss: 852.21 | loss_i: 728.89 loss_j: 1359.56 loss_edge_type: 768.61 loss_i_size: 652.97 loss_j_size: 751.01\n",
      "Loss: 946.40 | loss_i: 823.05 loss_j: 1467.16 loss_edge_type: 824.70 loss_i_size: 762.51 loss_j_size: 854.59\n",
      "Loss: 567.68 | loss_i: 412.27 loss_j: 702.54 loss_edge_type: 598.01 loss_i_size: 515.23 loss_j_size: 610.32\n",
      "Loss: 548.50 | loss_i: 323.80 loss_j: 555.41 loss_edge_type: 547.66 loss_i_size: 620.65 loss_j_size: 694.95\n",
      "Loss: 713.19 | loss_i: 500.25 loss_j: 925.69 loss_edge_type: 674.21 loss_i_size: 684.41 loss_j_size: 781.39\n",
      "Loss: 704.47 | loss_i: 480.96 loss_j: 837.36 loss_edge_type: 651.27 loss_i_size: 737.38 loss_j_size: 815.40\n",
      "Loss: 592.70 | loss_i: 380.98 loss_j: 645.47 loss_edge_type: 608.78 loss_i_size: 623.41 loss_j_size: 704.88\n",
      "Loss: 626.32 | loss_i: 401.98 loss_j: 742.65 loss_edge_type: 629.18 loss_i_size: 636.88 loss_j_size: 720.92\n",
      "Loss: 772.55 | loss_i: 589.17 loss_j: 1091.89 loss_edge_type: 705.43 loss_i_size: 694.73 loss_j_size: 781.54\n",
      "Loss: 713.25 | loss_i: 485.93 loss_j: 877.16 loss_edge_type: 690.32 loss_i_size: 700.29 loss_j_size: 812.57\n",
      "Loss: 720.02 | loss_i: 600.70 loss_j: 957.05 loss_edge_type: 670.65 loss_i_size: 623.13 loss_j_size: 748.59\n",
      "Loss: 659.06 | loss_i: 421.87 loss_j: 795.81 loss_edge_type: 680.72 loss_i_size: 649.61 loss_j_size: 747.27\n",
      "Loss: 677.53 | loss_i: 462.40 loss_j: 848.60 loss_edge_type: 698.79 loss_i_size: 643.43 loss_j_size: 734.44\n",
      "Loss: 636.25 | loss_i: 428.75 loss_j: 817.97 loss_edge_type: 648.70 loss_i_size: 604.30 loss_j_size: 681.53\n",
      "Loss: 594.50 | loss_i: 422.41 loss_j: 751.10 loss_edge_type: 605.76 loss_i_size: 558.09 loss_j_size: 635.13\n",
      "Loss: 740.09 | loss_i: 610.30 loss_j: 1094.20 loss_edge_type: 651.83 loss_i_size: 624.56 loss_j_size: 719.54\n",
      "Loss: 708.29 | loss_i: 443.27 loss_j: 804.23 loss_edge_type: 678.42 loss_i_size: 751.14 loss_j_size: 864.41\n",
      "Loss: 671.89 | loss_i: 473.20 loss_j: 831.09 loss_edge_type: 662.29 loss_i_size: 649.22 loss_j_size: 743.65\n",
      "Loss: 686.44 | loss_i: 463.63 loss_j: 866.54 loss_edge_type: 663.41 loss_i_size: 656.37 loss_j_size: 782.28\n",
      "Loss: 635.25 | loss_i: 405.73 loss_j: 711.70 loss_edge_type: 602.30 loss_i_size: 674.40 loss_j_size: 782.11\n",
      "Loss: 782.96 | loss_i: 504.49 loss_j: 899.44 loss_edge_type: 738.84 loss_i_size: 832.11 loss_j_size: 939.91\n",
      "Loss: 664.11 | loss_i: 469.93 loss_j: 849.01 loss_edge_type: 657.29 loss_i_size: 615.35 loss_j_size: 728.99\n",
      "Loss: 674.18 | loss_i: 413.88 loss_j: 752.85 loss_edge_type: 658.84 loss_i_size: 719.64 loss_j_size: 825.67\n",
      "Loss: 665.96 | loss_i: 406.72 loss_j: 795.32 loss_edge_type: 655.52 loss_i_size: 697.94 loss_j_size: 774.32\n",
      "Loss: 595.22 | loss_i: 400.39 loss_j: 701.22 loss_edge_type: 572.26 loss_i_size: 601.28 loss_j_size: 700.95\n",
      "Loss: 617.88 | loss_i: 398.83 loss_j: 746.69 loss_edge_type: 603.22 loss_i_size: 625.46 loss_j_size: 715.17\n",
      "Loss: 682.90 | loss_i: 483.44 loss_j: 843.85 loss_edge_type: 680.97 loss_i_size: 656.37 loss_j_size: 749.85\n",
      "Loss: 778.91 | loss_i: 542.89 loss_j: 986.04 loss_edge_type: 722.64 loss_i_size: 779.53 loss_j_size: 863.43\n",
      "Loss: 616.10 | loss_i: 398.01 loss_j: 721.42 loss_edge_type: 643.06 loss_i_size: 611.53 loss_j_size: 706.48\n",
      "Loss: 576.67 | loss_i: 365.87 loss_j: 579.77 loss_edge_type: 615.27 loss_i_size: 613.25 loss_j_size: 709.19\n",
      "Loss: 70.45 | loss_i: 65.75 loss_j: 114.22 loss_edge_type: 87.60 loss_i_size: 37.82 loss_j_size: 46.84\n",
      "Validation Loss: 601.43 | loss_i: 401.93 loss_j: 744.90 loss_edge_type: 637.58 loss_i_size: 555.81 loss_j_size: 666.94\n",
      "Validation Loss: 603.91 | loss_i: 413.49 loss_j: 784.52 loss_edge_type: 603.19 loss_i_size: 548.55 loss_j_size: 669.78\n",
      "Validation Loss: 603.99 | loss_i: 443.89 loss_j: 776.29 loss_edge_type: 607.24 loss_i_size: 552.47 loss_j_size: 640.08\n",
      "Validation Loss: 651.22 | loss_i: 490.07 loss_j: 847.12 loss_edge_type: 649.14 loss_i_size: 586.79 loss_j_size: 682.96\n",
      "Validation Loss: 864.06 | loss_i: 800.98 loss_j: 1416.68 loss_edge_type: 732.06 loss_i_size: 637.37 loss_j_size: 733.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [01:45<01:44, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 463.39 | loss_i: 280.25 loss_j: 486.03 loss_edge_type: 443.83 loss_i_size: 512.51 loss_j_size: 594.35\n",
      "Training loss: 673.1681\n",
      "Validation loss: 631.3339\n",
      "\n",
      "------------------------- Epoch 11 -------------------------\n",
      "Loss: 637.21 | loss_i: 395.11 loss_j: 655.07 loss_edge_type: 622.81 loss_i_size: 691.23 loss_j_size: 821.85\n",
      "Loss: 659.33 | loss_i: 463.66 loss_j: 770.50 loss_edge_type: 659.12 loss_i_size: 667.52 loss_j_size: 735.88\n",
      "Loss: 666.89 | loss_i: 441.03 loss_j: 786.82 loss_edge_type: 646.66 loss_i_size: 680.06 loss_j_size: 779.89\n",
      "Loss: 721.03 | loss_i: 447.11 loss_j: 793.78 loss_edge_type: 678.09 loss_i_size: 788.16 loss_j_size: 897.99\n",
      "Loss: 761.25 | loss_i: 563.98 loss_j: 1084.30 loss_edge_type: 728.87 loss_i_size: 673.01 loss_j_size: 756.07\n",
      "Loss: 698.48 | loss_i: 511.02 loss_j: 838.03 loss_edge_type: 694.92 loss_i_size: 667.10 loss_j_size: 781.34\n",
      "Loss: 706.46 | loss_i: 447.81 loss_j: 853.18 loss_edge_type: 694.28 loss_i_size: 727.93 loss_j_size: 809.09\n",
      "Loss: 575.29 | loss_i: 342.83 loss_j: 595.74 loss_edge_type: 586.86 loss_i_size: 629.94 loss_j_size: 721.08\n",
      "Loss: 637.04 | loss_i: 416.08 loss_j: 761.12 loss_edge_type: 631.66 loss_i_size: 644.65 loss_j_size: 731.68\n",
      "Loss: 650.76 | loss_i: 493.24 loss_j: 861.09 loss_edge_type: 671.00 loss_i_size: 572.67 loss_j_size: 655.78\n",
      "Loss: 644.85 | loss_i: 500.58 loss_j: 901.82 loss_edge_type: 631.79 loss_i_size: 547.40 loss_j_size: 642.68\n",
      "Loss: 605.90 | loss_i: 397.17 loss_j: 731.30 loss_edge_type: 600.60 loss_i_size: 603.79 loss_j_size: 696.65\n",
      "Loss: 695.09 | loss_i: 526.74 loss_j: 951.50 loss_edge_type: 669.62 loss_i_size: 630.10 loss_j_size: 697.48\n",
      "Loss: 535.99 | loss_i: 337.07 loss_j: 552.93 loss_edge_type: 547.88 loss_i_size: 581.39 loss_j_size: 660.67\n",
      "Loss: 648.01 | loss_i: 442.82 loss_j: 778.13 loss_edge_type: 625.37 loss_i_size: 656.58 loss_j_size: 737.14\n",
      "Loss: 597.01 | loss_i: 384.55 loss_j: 675.90 loss_edge_type: 589.32 loss_i_size: 618.18 loss_j_size: 717.13\n",
      "Loss: 608.27 | loss_i: 390.02 loss_j: 756.33 loss_edge_type: 631.28 loss_i_size: 583.27 loss_j_size: 680.42\n",
      "Loss: 590.77 | loss_i: 391.83 loss_j: 756.08 loss_edge_type: 602.53 loss_i_size: 548.58 loss_j_size: 654.84\n",
      "Loss: 606.40 | loss_i: 353.22 loss_j: 659.26 loss_edge_type: 577.85 loss_i_size: 689.56 loss_j_size: 752.08\n",
      "Loss: 599.40 | loss_i: 432.59 loss_j: 793.34 loss_edge_type: 612.05 loss_i_size: 531.08 loss_j_size: 627.95\n",
      "Loss: 644.65 | loss_i: 416.69 loss_j: 730.83 loss_edge_type: 671.66 loss_i_size: 655.91 loss_j_size: 748.16\n",
      "Loss: 874.77 | loss_i: 642.39 loss_j: 1259.97 loss_edge_type: 809.78 loss_i_size: 775.09 loss_j_size: 886.61\n",
      "Loss: 554.93 | loss_i: 330.91 loss_j: 572.16 loss_edge_type: 578.12 loss_i_size: 608.92 loss_j_size: 684.55\n",
      "Loss: 655.48 | loss_i: 396.69 loss_j: 734.68 loss_edge_type: 592.56 loss_i_size: 740.86 loss_j_size: 812.59\n",
      "Loss: 605.52 | loss_i: 341.78 loss_j: 650.09 loss_edge_type: 606.50 loss_i_size: 670.72 loss_j_size: 758.49\n",
      "Loss: 621.17 | loss_i: 368.61 loss_j: 707.11 loss_edge_type: 623.18 loss_i_size: 647.52 loss_j_size: 759.44\n",
      "Loss: 658.33 | loss_i: 511.35 loss_j: 942.56 loss_edge_type: 624.76 loss_i_size: 562.61 loss_j_size: 650.38\n",
      "Loss: 620.68 | loss_i: 429.68 loss_j: 741.73 loss_edge_type: 620.65 loss_i_size: 615.23 loss_j_size: 696.10\n",
      "Loss: 589.47 | loss_i: 347.40 loss_j: 586.76 loss_edge_type: 605.18 loss_i_size: 647.74 loss_j_size: 760.28\n",
      "Loss: 855.59 | loss_i: 781.11 loss_j: 1358.08 loss_edge_type: 765.31 loss_i_size: 646.41 loss_j_size: 727.05\n",
      "Loss: 657.69 | loss_i: 472.90 loss_j: 917.26 loss_edge_type: 591.28 loss_i_size: 602.94 loss_j_size: 704.07\n",
      "Loss: 723.13 | loss_i: 491.00 loss_j: 896.89 loss_edge_type: 682.39 loss_i_size: 714.42 loss_j_size: 830.95\n",
      "Loss: 753.94 | loss_i: 531.06 loss_j: 1014.50 loss_edge_type: 716.11 loss_i_size: 713.21 loss_j_size: 794.84\n",
      "Loss: 712.03 | loss_i: 511.31 loss_j: 946.92 loss_edge_type: 669.56 loss_i_size: 681.55 loss_j_size: 750.78\n",
      "Loss: 753.15 | loss_i: 499.42 loss_j: 993.38 loss_edge_type: 703.57 loss_i_size: 737.14 loss_j_size: 832.24\n",
      "Loss: 700.89 | loss_i: 478.70 loss_j: 939.02 loss_edge_type: 678.29 loss_i_size: 649.76 loss_j_size: 758.65\n",
      "Loss: 581.83 | loss_i: 326.13 loss_j: 668.57 loss_edge_type: 607.00 loss_i_size: 606.00 loss_j_size: 701.46\n",
      "Loss: 760.98 | loss_i: 505.67 loss_j: 920.86 loss_edge_type: 760.32 loss_i_size: 763.14 loss_j_size: 854.90\n",
      "Loss: 567.38 | loss_i: 398.89 loss_j: 698.65 loss_edge_type: 580.60 loss_i_size: 544.70 loss_j_size: 614.06\n",
      "Loss: 763.70 | loss_i: 579.33 loss_j: 1007.83 loss_edge_type: 697.61 loss_i_size: 704.10 loss_j_size: 829.62\n",
      "Loss: 676.66 | loss_i: 486.14 loss_j: 910.09 loss_edge_type: 655.27 loss_i_size: 612.96 loss_j_size: 718.82\n",
      "Loss: 599.43 | loss_i: 405.81 loss_j: 699.98 loss_edge_type: 628.37 loss_i_size: 588.90 loss_j_size: 674.09\n",
      "Loss: 683.62 | loss_i: 463.37 loss_j: 892.24 loss_edge_type: 709.22 loss_i_size: 622.76 loss_j_size: 730.50\n",
      "Loss: 627.37 | loss_i: 393.36 loss_j: 718.95 loss_edge_type: 619.30 loss_i_size: 651.15 loss_j_size: 754.11\n",
      "Loss: 526.88 | loss_i: 331.77 loss_j: 540.35 loss_edge_type: 563.45 loss_i_size: 552.19 loss_j_size: 646.67\n",
      "Loss: 683.35 | loss_i: 448.08 loss_j: 790.37 loss_edge_type: 702.39 loss_i_size: 689.49 loss_j_size: 786.45\n",
      "Loss: 656.87 | loss_i: 468.92 loss_j: 808.67 loss_edge_type: 658.44 loss_i_size: 627.24 loss_j_size: 721.07\n",
      "Loss: 684.70 | loss_i: 490.79 loss_j: 954.52 loss_edge_type: 701.36 loss_i_size: 590.66 loss_j_size: 686.16\n",
      "Loss: 626.41 | loss_i: 389.46 loss_j: 711.14 loss_edge_type: 641.77 loss_i_size: 649.87 loss_j_size: 739.83\n",
      "Loss: 681.64 | loss_i: 415.45 loss_j: 862.24 loss_edge_type: 632.67 loss_i_size: 695.74 loss_j_size: 802.11\n",
      "Loss: 559.18 | loss_i: 317.05 loss_j: 573.08 loss_edge_type: 538.32 loss_i_size: 640.34 loss_j_size: 727.09\n",
      "Loss: 76.24 | loss_i: 44.45 loss_j: 69.66 loss_edge_type: 84.05 loss_i_size: 79.30 loss_j_size: 103.73\n",
      "Validation Loss: 545.88 | loss_i: 387.60 loss_j: 738.53 loss_edge_type: 546.48 loss_i_size: 487.63 loss_j_size: 569.16\n",
      "Validation Loss: 586.21 | loss_i: 407.79 loss_j: 702.65 loss_edge_type: 553.33 loss_i_size: 579.36 loss_j_size: 687.90\n",
      "Validation Loss: 722.29 | loss_i: 610.58 loss_j: 1038.85 loss_edge_type: 665.91 loss_i_size: 590.76 loss_j_size: 705.33\n",
      "Validation Loss: 623.76 | loss_i: 415.92 loss_j: 788.11 loss_edge_type: 598.41 loss_i_size: 609.09 loss_j_size: 707.26\n",
      "Validation Loss: 729.94 | loss_i: 585.39 loss_j: 1106.61 loss_edge_type: 706.86 loss_i_size: 574.16 loss_j_size: 676.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [01:55<01:34, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 395.21 | loss_i: 226.21 loss_j: 402.75 loss_edge_type: 420.42 loss_i_size: 423.38 loss_j_size: 503.31\n",
      "Training loss: 645.8287\n",
      "Validation loss: 600.5478\n",
      "\n",
      "------------------------- Epoch 12 -------------------------\n",
      "Loss: 580.10 | loss_i: 361.28 loss_j: 644.20 loss_edge_type: 603.96 loss_i_size: 597.93 loss_j_size: 693.14\n",
      "Loss: 633.04 | loss_i: 469.22 loss_j: 912.66 loss_edge_type: 646.21 loss_i_size: 519.85 loss_j_size: 617.26\n",
      "Loss: 901.91 | loss_i: 740.33 loss_j: 1356.67 loss_edge_type: 746.01 loss_i_size: 794.28 loss_j_size: 872.25\n",
      "Loss: 582.30 | loss_i: 338.17 loss_j: 620.93 loss_edge_type: 581.45 loss_i_size: 641.20 loss_j_size: 729.77\n",
      "Loss: 644.01 | loss_i: 369.31 loss_j: 703.64 loss_edge_type: 605.93 loss_i_size: 726.26 loss_j_size: 814.93\n",
      "Loss: 727.13 | loss_i: 506.72 loss_j: 870.51 loss_edge_type: 719.76 loss_i_size: 716.17 loss_j_size: 822.49\n",
      "Loss: 764.88 | loss_i: 531.67 loss_j: 955.88 loss_edge_type: 747.96 loss_i_size: 747.35 loss_j_size: 841.53\n",
      "Loss: 688.03 | loss_i: 435.52 loss_j: 850.59 loss_edge_type: 665.50 loss_i_size: 694.40 loss_j_size: 794.14\n",
      "Loss: 595.68 | loss_i: 384.46 loss_j: 724.29 loss_edge_type: 602.15 loss_i_size: 590.81 loss_j_size: 676.69\n",
      "Loss: 608.94 | loss_i: 378.78 loss_j: 689.80 loss_edge_type: 628.59 loss_i_size: 618.32 loss_j_size: 729.21\n",
      "Loss: 595.82 | loss_i: 340.55 loss_j: 660.41 loss_edge_type: 607.44 loss_i_size: 634.03 loss_j_size: 736.67\n",
      "Loss: 619.85 | loss_i: 396.46 loss_j: 774.95 loss_edge_type: 624.78 loss_i_size: 606.90 loss_j_size: 696.15\n",
      "Loss: 520.99 | loss_i: 315.39 loss_j: 512.37 loss_edge_type: 559.77 loss_i_size: 553.38 loss_j_size: 664.04\n",
      "Loss: 667.47 | loss_i: 481.20 loss_j: 926.15 loss_edge_type: 645.69 loss_i_size: 605.03 loss_j_size: 679.29\n",
      "Loss: 709.16 | loss_i: 504.47 loss_j: 937.05 loss_edge_type: 690.27 loss_i_size: 648.42 loss_j_size: 765.58\n",
      "Loss: 563.12 | loss_i: 330.33 loss_j: 563.06 loss_edge_type: 597.07 loss_i_size: 608.62 loss_j_size: 716.52\n",
      "Loss: 764.42 | loss_i: 529.54 loss_j: 1045.40 loss_edge_type: 713.68 loss_i_size: 714.25 loss_j_size: 819.23\n",
      "Loss: 577.36 | loss_i: 379.76 loss_j: 666.15 loss_edge_type: 595.36 loss_i_size: 575.39 loss_j_size: 670.12\n",
      "Loss: 553.31 | loss_i: 388.93 loss_j: 696.81 loss_edge_type: 541.08 loss_i_size: 524.75 loss_j_size: 614.97\n",
      "Loss: 593.79 | loss_i: 410.62 loss_j: 757.49 loss_edge_type: 589.41 loss_i_size: 562.30 loss_j_size: 649.11\n",
      "Loss: 654.80 | loss_i: 423.67 loss_j: 769.49 loss_edge_type: 674.63 loss_i_size: 641.42 loss_j_size: 764.77\n",
      "Loss: 691.15 | loss_i: 421.86 loss_j: 805.78 loss_edge_type: 654.95 loss_i_size: 734.41 loss_j_size: 838.74\n",
      "Loss: 492.46 | loss_i: 277.12 loss_j: 512.61 loss_edge_type: 521.98 loss_i_size: 526.26 loss_j_size: 624.31\n",
      "Loss: 590.36 | loss_i: 419.86 loss_j: 716.14 loss_edge_type: 615.44 loss_i_size: 557.49 loss_j_size: 642.86\n",
      "Loss: 623.69 | loss_i: 391.34 loss_j: 712.91 loss_edge_type: 652.30 loss_i_size: 622.17 loss_j_size: 739.73\n",
      "Loss: 546.85 | loss_i: 359.06 loss_j: 618.85 loss_edge_type: 567.32 loss_i_size: 547.61 loss_j_size: 641.40\n",
      "Loss: 545.31 | loss_i: 302.56 loss_j: 582.70 loss_edge_type: 559.92 loss_i_size: 587.91 loss_j_size: 693.45\n",
      "Loss: 588.73 | loss_i: 356.94 loss_j: 682.54 loss_edge_type: 597.72 loss_i_size: 620.45 loss_j_size: 686.02\n",
      "Loss: 646.70 | loss_i: 427.75 loss_j: 765.53 loss_edge_type: 627.15 loss_i_size: 653.02 loss_j_size: 760.06\n",
      "Loss: 633.83 | loss_i: 434.92 loss_j: 781.79 loss_edge_type: 624.79 loss_i_size: 610.43 loss_j_size: 717.24\n",
      "Loss: 790.75 | loss_i: 648.38 loss_j: 1271.74 loss_edge_type: 691.13 loss_i_size: 625.03 loss_j_size: 717.46\n",
      "Loss: 720.09 | loss_i: 558.99 loss_j: 965.49 loss_edge_type: 681.93 loss_i_size: 642.70 loss_j_size: 751.34\n",
      "Loss: 502.34 | loss_i: 338.41 loss_j: 616.28 loss_edge_type: 526.80 loss_i_size: 470.45 loss_j_size: 559.78\n",
      "Loss: 635.06 | loss_i: 436.31 loss_j: 832.97 loss_edge_type: 674.21 loss_i_size: 567.45 loss_j_size: 664.34\n",
      "Loss: 632.94 | loss_i: 426.50 loss_j: 770.39 loss_edge_type: 630.24 loss_i_size: 608.12 loss_j_size: 729.47\n",
      "Loss: 592.81 | loss_i: 348.81 loss_j: 639.24 loss_edge_type: 569.20 loss_i_size: 656.83 loss_j_size: 749.99\n",
      "Loss: 616.37 | loss_i: 385.68 loss_j: 794.10 loss_edge_type: 624.80 loss_i_size: 601.41 loss_j_size: 675.86\n",
      "Loss: 524.05 | loss_i: 280.60 loss_j: 542.51 loss_edge_type: 555.51 loss_i_size: 572.06 loss_j_size: 669.55\n",
      "Loss: 774.27 | loss_i: 487.23 loss_j: 932.40 loss_edge_type: 716.94 loss_i_size: 808.14 loss_j_size: 926.64\n",
      "Loss: 662.53 | loss_i: 453.62 loss_j: 851.47 loss_edge_type: 649.54 loss_i_size: 625.97 loss_j_size: 732.04\n",
      "Loss: 623.33 | loss_i: 363.37 loss_j: 707.16 loss_edge_type: 617.35 loss_i_size: 674.71 loss_j_size: 754.03\n",
      "Loss: 617.73 | loss_i: 431.74 loss_j: 805.53 loss_edge_type: 619.19 loss_i_size: 566.26 loss_j_size: 665.91\n",
      "Loss: 554.86 | loss_i: 343.83 loss_j: 595.54 loss_edge_type: 604.93 loss_i_size: 578.37 loss_j_size: 651.63\n",
      "Loss: 571.35 | loss_i: 357.67 loss_j: 642.15 loss_edge_type: 577.81 loss_i_size: 592.21 loss_j_size: 686.93\n",
      "Loss: 597.84 | loss_i: 351.16 loss_j: 644.59 loss_edge_type: 611.57 loss_i_size: 642.87 loss_j_size: 739.03\n",
      "Loss: 616.09 | loss_i: 403.73 loss_j: 787.30 loss_edge_type: 601.62 loss_i_size: 599.44 loss_j_size: 688.35\n",
      "Loss: 559.07 | loss_i: 359.41 loss_j: 666.82 loss_edge_type: 570.65 loss_i_size: 547.95 loss_j_size: 650.52\n",
      "Loss: 696.01 | loss_i: 504.89 loss_j: 992.31 loss_edge_type: 672.82 loss_i_size: 598.08 loss_j_size: 711.94\n",
      "Loss: 566.94 | loss_i: 410.60 loss_j: 714.37 loss_edge_type: 583.90 loss_i_size: 516.08 loss_j_size: 609.74\n",
      "Loss: 518.77 | loss_i: 281.86 loss_j: 518.87 loss_edge_type: 565.90 loss_i_size: 563.96 loss_j_size: 663.26\n",
      "Loss: 695.95 | loss_i: 526.44 loss_j: 935.70 loss_edge_type: 671.42 loss_i_size: 622.18 loss_j_size: 724.01\n",
      "Loss: 65.40 | loss_i: 53.20 loss_j: 98.86 loss_edge_type: 62.27 loss_i_size: 49.76 loss_j_size: 62.89\n",
      "Validation Loss: 789.68 | loss_i: 717.10 loss_j: 1253.24 loss_edge_type: 693.10 loss_i_size: 572.95 loss_j_size: 711.98\n",
      "Validation Loss: 561.93 | loss_i: 358.85 loss_j: 735.62 loss_edge_type: 567.72 loss_i_size: 535.41 loss_j_size: 612.04\n",
      "Validation Loss: 634.35 | loss_i: 450.61 loss_j: 891.08 loss_edge_type: 612.16 loss_i_size: 546.58 loss_j_size: 671.31\n",
      "Validation Loss: 549.32 | loss_i: 306.85 loss_j: 582.74 loss_edge_type: 538.64 loss_i_size: 603.96 loss_j_size: 714.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:06<01:23, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 492.06 | loss_i: 282.67 loss_j: 553.83 loss_edge_type: 525.12 loss_i_size: 493.98 loss_j_size: 604.72\n",
      "Validation Loss: 397.89 | loss_i: 283.75 loss_j: 504.54 loss_edge_type: 407.82 loss_i_size: 373.54 loss_j_size: 419.81\n",
      "Training loss: 616.1484\n",
      "Validation loss: 570.8720\n",
      "\n",
      "------------------------- Epoch 13 -------------------------\n",
      "Loss: 487.39 | loss_i: 297.02 loss_j: 498.48 loss_edge_type: 522.34 loss_i_size: 511.29 loss_j_size: 607.84\n",
      "Loss: 662.87 | loss_i: 443.54 loss_j: 853.66 loss_edge_type: 672.55 loss_i_size: 618.36 loss_j_size: 726.26\n",
      "Loss: 645.83 | loss_i: 398.99 loss_j: 771.19 loss_edge_type: 642.33 loss_i_size: 663.42 loss_j_size: 753.24\n",
      "Loss: 588.60 | loss_i: 366.16 loss_j: 626.06 loss_edge_type: 575.11 loss_i_size: 648.54 loss_j_size: 727.14\n",
      "Loss: 510.64 | loss_i: 279.62 loss_j: 524.39 loss_edge_type: 543.04 loss_i_size: 538.05 loss_j_size: 668.08\n",
      "Loss: 577.35 | loss_i: 352.12 loss_j: 714.31 loss_edge_type: 603.49 loss_i_size: 559.78 loss_j_size: 657.05\n",
      "Loss: 590.41 | loss_i: 396.29 loss_j: 733.67 loss_edge_type: 652.41 loss_i_size: 539.70 loss_j_size: 629.98\n",
      "Loss: 611.70 | loss_i: 398.11 loss_j: 739.63 loss_edge_type: 666.55 loss_i_size: 578.20 loss_j_size: 676.03\n",
      "Loss: 764.56 | loss_i: 561.04 loss_j: 1191.84 loss_edge_type: 696.18 loss_i_size: 621.37 loss_j_size: 752.36\n",
      "Loss: 569.42 | loss_i: 366.33 loss_j: 651.34 loss_edge_type: 599.66 loss_i_size: 550.17 loss_j_size: 679.59\n",
      "Loss: 549.35 | loss_i: 310.32 loss_j: 589.24 loss_edge_type: 598.75 loss_i_size: 584.48 loss_j_size: 663.96\n",
      "Loss: 748.39 | loss_i: 493.15 loss_j: 934.57 loss_edge_type: 739.43 loss_i_size: 729.13 loss_j_size: 845.68\n",
      "Loss: 758.71 | loss_i: 473.25 loss_j: 919.39 loss_edge_type: 730.73 loss_i_size: 781.46 loss_j_size: 888.71\n",
      "Loss: 645.24 | loss_i: 510.96 loss_j: 923.55 loss_edge_type: 647.74 loss_i_size: 516.20 loss_j_size: 627.76\n",
      "Loss: 588.63 | loss_i: 361.29 loss_j: 646.26 loss_edge_type: 560.10 loss_i_size: 649.19 loss_j_size: 726.30\n",
      "Loss: 656.48 | loss_i: 400.42 loss_j: 786.94 loss_edge_type: 630.32 loss_i_size: 672.42 loss_j_size: 792.32\n",
      "Loss: 587.92 | loss_i: 384.58 loss_j: 696.66 loss_edge_type: 633.01 loss_i_size: 539.85 loss_j_size: 685.47\n",
      "Loss: 695.96 | loss_i: 484.66 loss_j: 935.67 loss_edge_type: 682.02 loss_i_size: 649.40 loss_j_size: 728.03\n",
      "Loss: 505.41 | loss_i: 301.66 loss_j: 564.38 loss_edge_type: 579.58 loss_i_size: 503.04 loss_j_size: 578.37\n",
      "Loss: 640.73 | loss_i: 470.71 loss_j: 912.60 loss_edge_type: 615.94 loss_i_size: 562.74 loss_j_size: 641.66\n",
      "Loss: 551.33 | loss_i: 310.24 loss_j: 590.02 loss_edge_type: 584.26 loss_i_size: 592.19 loss_j_size: 679.93\n",
      "Loss: 674.90 | loss_i: 554.01 loss_j: 1008.33 loss_edge_type: 632.96 loss_i_size: 537.10 loss_j_size: 642.07\n",
      "Loss: 588.97 | loss_i: 372.18 loss_j: 653.92 loss_edge_type: 618.47 loss_i_size: 610.02 loss_j_size: 690.27\n",
      "Loss: 569.24 | loss_i: 353.39 loss_j: 623.35 loss_edge_type: 575.63 loss_i_size: 589.73 loss_j_size: 704.12\n",
      "Loss: 688.34 | loss_i: 408.40 loss_j: 889.63 loss_edge_type: 652.51 loss_i_size: 696.76 loss_j_size: 794.40\n",
      "Loss: 627.21 | loss_i: 463.91 loss_j: 819.25 loss_edge_type: 618.22 loss_i_size: 555.77 loss_j_size: 678.91\n",
      "Loss: 577.03 | loss_i: 360.86 loss_j: 664.93 loss_edge_type: 616.26 loss_i_size: 576.04 loss_j_size: 667.06\n",
      "Loss: 778.43 | loss_i: 525.67 loss_j: 1003.68 loss_edge_type: 784.46 loss_i_size: 733.38 loss_j_size: 844.95\n",
      "Loss: 630.42 | loss_i: 380.59 loss_j: 708.03 loss_edge_type: 606.67 loss_i_size: 682.02 loss_j_size: 774.80\n",
      "Loss: 618.17 | loss_i: 421.44 loss_j: 751.66 loss_edge_type: 629.02 loss_i_size: 596.67 loss_j_size: 692.07\n",
      "Loss: 519.64 | loss_i: 286.00 loss_j: 586.25 loss_edge_type: 525.98 loss_i_size: 557.16 loss_j_size: 642.79\n",
      "Loss: 538.68 | loss_i: 298.09 loss_j: 552.97 loss_edge_type: 572.97 loss_i_size: 592.50 loss_j_size: 676.85\n",
      "Loss: 621.75 | loss_i: 332.44 loss_j: 646.92 loss_edge_type: 613.94 loss_i_size: 720.74 loss_j_size: 794.74\n",
      "Loss: 622.06 | loss_i: 434.76 loss_j: 780.92 loss_edge_type: 592.64 loss_i_size: 592.47 loss_j_size: 709.51\n",
      "Loss: 688.26 | loss_i: 514.76 loss_j: 928.10 loss_edge_type: 675.37 loss_i_size: 606.18 loss_j_size: 716.90\n",
      "Loss: 542.58 | loss_i: 338.56 loss_j: 619.00 loss_edge_type: 542.78 loss_i_size: 554.95 loss_j_size: 657.62\n",
      "Loss: 622.78 | loss_i: 399.03 loss_j: 797.34 loss_edge_type: 617.62 loss_i_size: 609.58 loss_j_size: 690.34\n",
      "Loss: 632.29 | loss_i: 437.15 loss_j: 765.67 loss_edge_type: 607.03 loss_i_size: 625.89 loss_j_size: 725.70\n",
      "Loss: 703.15 | loss_i: 448.63 loss_j: 915.13 loss_edge_type: 649.06 loss_i_size: 707.64 loss_j_size: 795.30\n",
      "Loss: 482.82 | loss_i: 317.32 loss_j: 565.53 loss_edge_type: 525.71 loss_i_size: 467.92 loss_j_size: 537.60\n",
      "Loss: 552.85 | loss_i: 337.34 loss_j: 621.57 loss_edge_type: 540.41 loss_i_size: 579.73 loss_j_size: 685.23\n",
      "Loss: 485.98 | loss_i: 268.23 loss_j: 515.30 loss_edge_type: 496.23 loss_i_size: 526.58 loss_j_size: 623.54\n",
      "Loss: 620.80 | loss_i: 386.73 loss_j: 740.33 loss_edge_type: 605.40 loss_i_size: 643.38 loss_j_size: 728.15\n",
      "Loss: 526.46 | loss_i: 316.30 loss_j: 661.25 loss_edge_type: 516.97 loss_i_size: 528.33 loss_j_size: 609.46\n",
      "Loss: 591.20 | loss_i: 361.76 loss_j: 678.92 loss_edge_type: 623.18 loss_i_size: 608.89 loss_j_size: 683.24\n",
      "Loss: 565.32 | loss_i: 334.70 loss_j: 636.03 loss_edge_type: 572.85 loss_i_size: 595.89 loss_j_size: 687.10\n",
      "Loss: 579.99 | loss_i: 366.28 loss_j: 697.70 loss_edge_type: 610.99 loss_i_size: 560.35 loss_j_size: 664.63\n",
      "Loss: 671.91 | loss_i: 490.00 loss_j: 896.78 loss_edge_type: 646.45 loss_i_size: 606.81 loss_j_size: 719.50\n",
      "Loss: 488.14 | loss_i: 269.06 loss_j: 464.13 loss_edge_type: 519.54 loss_i_size: 545.05 loss_j_size: 642.94\n",
      "Loss: 514.48 | loss_i: 272.93 loss_j: 536.60 loss_edge_type: 523.92 loss_i_size: 580.60 loss_j_size: 658.37\n",
      "Loss: 499.24 | loss_i: 301.75 loss_j: 566.57 loss_edge_type: 520.79 loss_i_size: 503.51 loss_j_size: 603.59\n",
      "Loss: 57.28 | loss_i: 30.33 loss_j: 56.55 loss_edge_type: 59.81 loss_i_size: 60.06 loss_j_size: 79.67\n",
      "Validation Loss: 593.56 | loss_i: 421.05 loss_j: 760.47 loss_edge_type: 602.58 loss_i_size: 543.52 loss_j_size: 640.20\n",
      "Validation Loss: 650.13 | loss_i: 545.94 loss_j: 981.27 loss_edge_type: 590.46 loss_i_size: 499.11 loss_j_size: 633.85\n",
      "Validation Loss: 507.25 | loss_i: 285.16 loss_j: 584.59 loss_edge_type: 547.19 loss_i_size: 523.98 loss_j_size: 595.34\n",
      "Validation Loss: 636.11 | loss_i: 528.89 loss_j: 953.99 loss_edge_type: 579.03 loss_i_size: 510.68 loss_j_size: 607.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:17<01:13, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 488.40 | loss_i: 255.73 loss_j: 493.72 loss_edge_type: 529.63 loss_i_size: 521.85 loss_j_size: 641.08\n",
      "Validation Loss: 405.18 | loss_i: 275.06 loss_j: 543.39 loss_edge_type: 406.97 loss_i_size: 371.38 loss_j_size: 429.10\n",
      "Training loss: 592.6405\n",
      "Validation loss: 546.7721\n",
      "\n",
      "------------------------- Epoch 14 -------------------------\n",
      "Loss: 536.28 | loss_i: 314.24 loss_j: 587.51 loss_edge_type: 545.69 loss_i_size: 571.83 loss_j_size: 662.10\n",
      "Loss: 647.04 | loss_i: 400.73 loss_j: 737.00 loss_edge_type: 677.86 loss_i_size: 673.49 loss_j_size: 746.10\n",
      "Loss: 550.44 | loss_i: 379.75 loss_j: 654.52 loss_edge_type: 546.91 loss_i_size: 547.00 loss_j_size: 623.99\n",
      "Loss: 534.66 | loss_i: 309.71 loss_j: 557.24 loss_edge_type: 546.96 loss_i_size: 579.23 loss_j_size: 680.12\n",
      "Loss: 516.77 | loss_i: 280.74 loss_j: 543.18 loss_edge_type: 515.36 loss_i_size: 566.16 loss_j_size: 678.40\n",
      "Loss: 449.86 | loss_i: 240.38 loss_j: 406.52 loss_edge_type: 506.21 loss_i_size: 500.26 loss_j_size: 595.92\n",
      "Loss: 516.42 | loss_i: 299.51 loss_j: 566.33 loss_edge_type: 541.80 loss_i_size: 532.90 loss_j_size: 641.56\n",
      "Loss: 634.28 | loss_i: 437.40 loss_j: 860.47 loss_edge_type: 650.00 loss_i_size: 561.00 loss_j_size: 662.55\n",
      "Loss: 596.89 | loss_i: 372.07 loss_j: 754.76 loss_edge_type: 623.56 loss_i_size: 566.79 loss_j_size: 667.28\n",
      "Loss: 578.35 | loss_i: 394.03 loss_j: 735.49 loss_edge_type: 581.99 loss_i_size: 543.99 loss_j_size: 636.24\n",
      "Loss: 524.89 | loss_i: 285.74 loss_j: 552.14 loss_edge_type: 530.13 loss_i_size: 578.15 loss_j_size: 678.29\n",
      "Loss: 551.52 | loss_i: 368.02 loss_j: 678.73 loss_edge_type: 576.34 loss_i_size: 520.85 loss_j_size: 613.67\n",
      "Loss: 626.71 | loss_i: 381.74 loss_j: 728.83 loss_edge_type: 621.89 loss_i_size: 653.59 loss_j_size: 747.50\n",
      "Loss: 604.91 | loss_i: 360.28 loss_j: 656.29 loss_edge_type: 582.88 loss_i_size: 667.52 loss_j_size: 757.56\n",
      "Loss: 540.16 | loss_i: 285.20 loss_j: 578.31 loss_edge_type: 535.93 loss_i_size: 600.48 loss_j_size: 700.88\n",
      "Loss: 586.90 | loss_i: 346.45 loss_j: 695.42 loss_edge_type: 623.65 loss_i_size: 597.46 loss_j_size: 671.54\n",
      "Loss: 621.96 | loss_i: 398.89 loss_j: 699.15 loss_edge_type: 640.54 loss_i_size: 628.98 loss_j_size: 742.26\n",
      "Loss: 461.06 | loss_i: 218.91 loss_j: 420.75 loss_edge_type: 477.30 loss_i_size: 553.90 loss_j_size: 634.46\n",
      "Loss: 448.34 | loss_i: 219.41 loss_j: 439.21 loss_edge_type: 462.26 loss_i_size: 514.83 loss_j_size: 606.00\n",
      "Loss: 606.27 | loss_i: 384.70 loss_j: 666.13 loss_edge_type: 623.50 loss_i_size: 618.46 loss_j_size: 738.55\n",
      "Loss: 539.12 | loss_i: 314.97 loss_j: 601.06 loss_edge_type: 567.58 loss_i_size: 529.84 loss_j_size: 682.12\n",
      "Loss: 583.45 | loss_i: 332.32 loss_j: 643.98 loss_edge_type: 578.61 loss_i_size: 634.37 loss_j_size: 727.95\n",
      "Loss: 542.54 | loss_i: 293.95 loss_j: 603.68 loss_edge_type: 579.05 loss_i_size: 565.91 loss_j_size: 670.14\n",
      "Loss: 699.77 | loss_i: 418.95 loss_j: 901.31 loss_edge_type: 703.20 loss_i_size: 689.86 loss_j_size: 785.51\n",
      "Loss: 545.62 | loss_i: 327.73 loss_j: 655.11 loss_edge_type: 565.52 loss_i_size: 539.31 loss_j_size: 640.41\n",
      "Loss: 458.86 | loss_i: 299.71 loss_j: 540.01 loss_edge_type: 500.96 loss_i_size: 441.95 loss_j_size: 511.66\n",
      "Loss: 628.62 | loss_i: 415.39 loss_j: 794.64 loss_edge_type: 617.82 loss_i_size: 609.94 loss_j_size: 705.29\n",
      "Loss: 503.78 | loss_i: 271.07 loss_j: 489.41 loss_edge_type: 568.94 loss_i_size: 540.53 loss_j_size: 648.94\n",
      "Loss: 580.18 | loss_i: 397.01 loss_j: 740.26 loss_edge_type: 598.41 loss_i_size: 542.50 loss_j_size: 622.72\n",
      "Loss: 556.86 | loss_i: 338.48 loss_j: 623.06 loss_edge_type: 586.54 loss_i_size: 560.32 loss_j_size: 675.89\n",
      "Loss: 461.21 | loss_i: 256.42 loss_j: 459.68 loss_edge_type: 512.83 loss_i_size: 483.20 loss_j_size: 593.94\n",
      "Loss: 635.48 | loss_i: 393.97 loss_j: 690.61 loss_edge_type: 602.33 loss_i_size: 701.72 loss_j_size: 788.78\n",
      "Loss: 561.23 | loss_i: 345.52 loss_j: 714.61 loss_edge_type: 591.02 loss_i_size: 538.03 loss_j_size: 616.99\n",
      "Loss: 586.35 | loss_i: 321.31 loss_j: 666.19 loss_edge_type: 575.68 loss_i_size: 634.40 loss_j_size: 734.18\n",
      "Loss: 667.56 | loss_i: 482.53 loss_j: 894.55 loss_edge_type: 642.58 loss_i_size: 610.84 loss_j_size: 707.31\n",
      "Loss: 520.68 | loss_i: 310.24 loss_j: 626.70 loss_edge_type: 570.60 loss_i_size: 511.37 loss_j_size: 584.52\n",
      "Loss: 560.71 | loss_i: 345.44 loss_j: 634.50 loss_edge_type: 566.83 loss_i_size: 579.19 loss_j_size: 677.58\n",
      "Loss: 586.53 | loss_i: 342.69 loss_j: 649.16 loss_edge_type: 617.16 loss_i_size: 597.13 loss_j_size: 726.50\n",
      "Loss: 542.03 | loss_i: 377.57 loss_j: 652.20 loss_edge_type: 590.28 loss_i_size: 501.27 loss_j_size: 588.83\n",
      "Loss: 631.57 | loss_i: 448.85 loss_j: 890.68 loss_edge_type: 653.27 loss_i_size: 543.71 loss_j_size: 621.34\n",
      "Loss: 512.18 | loss_i: 314.75 loss_j: 618.90 loss_edge_type: 509.82 loss_i_size: 518.82 loss_j_size: 598.60\n",
      "Loss: 550.82 | loss_i: 342.27 loss_j: 724.80 loss_edge_type: 583.01 loss_i_size: 496.83 loss_j_size: 607.22\n",
      "Loss: 494.34 | loss_i: 306.53 loss_j: 525.09 loss_edge_type: 544.58 loss_i_size: 504.80 loss_j_size: 590.70\n",
      "Loss: 616.18 | loss_i: 374.27 loss_j: 689.64 loss_edge_type: 637.97 loss_i_size: 645.32 loss_j_size: 733.70\n",
      "Loss: 648.89 | loss_i: 437.12 loss_j: 867.28 loss_edge_type: 680.40 loss_i_size: 577.54 loss_j_size: 682.10\n",
      "Loss: 762.98 | loss_i: 652.20 loss_j: 1275.79 loss_edge_type: 664.68 loss_i_size: 543.90 loss_j_size: 678.32\n",
      "Loss: 572.89 | loss_i: 355.05 loss_j: 649.07 loss_edge_type: 586.82 loss_i_size: 585.28 loss_j_size: 688.19\n",
      "Loss: 728.42 | loss_i: 532.77 loss_j: 1024.86 loss_edge_type: 721.32 loss_i_size: 618.95 loss_j_size: 744.22\n",
      "Loss: 687.89 | loss_i: 501.91 loss_j: 1014.77 loss_edge_type: 639.12 loss_i_size: 597.19 loss_j_size: 686.46\n",
      "Loss: 554.77 | loss_i: 303.57 loss_j: 573.10 loss_edge_type: 584.20 loss_i_size: 609.57 loss_j_size: 703.39\n",
      "Loss: 669.23 | loss_i: 541.66 loss_j: 920.02 loss_edge_type: 643.83 loss_i_size: 569.52 loss_j_size: 671.13\n",
      "Loss: 123.00 | loss_i: 82.68 loss_j: 165.80 loss_edge_type: 101.92 loss_i_size: 125.24 loss_j_size: 139.34\n",
      "Validation Loss: 477.63 | loss_i: 262.14 loss_j: 531.47 loss_edge_type: 503.58 loss_i_size: 478.76 loss_j_size: 612.23\n",
      "Validation Loss: 718.09 | loss_i: 572.63 loss_j: 1020.24 loss_edge_type: 658.93 loss_i_size: 613.00 loss_j_size: 725.68\n",
      "Validation Loss: 576.06 | loss_i: 413.54 loss_j: 774.30 loss_edge_type: 571.85 loss_i_size: 519.85 loss_j_size: 600.75\n",
      "Validation Loss: 463.26 | loss_i: 224.58 loss_j: 444.84 loss_edge_type: 482.98 loss_i_size: 529.00 loss_j_size: 634.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:27<01:02, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 508.78 | loss_i: 351.69 loss_j: 655.92 loss_edge_type: 527.55 loss_i_size: 450.82 loss_j_size: 557.92\n",
      "Validation Loss: 374.31 | loss_i: 297.83 loss_j: 559.44 loss_edge_type: 386.78 loss_i_size: 282.79 loss_j_size: 344.73\n",
      "Training loss: 566.2970\n",
      "Validation loss: 519.6895\n",
      "\n",
      "------------------------- Epoch 15 -------------------------\n",
      "Loss: 559.54 | loss_i: 368.65 loss_j: 740.52 loss_edge_type: 572.39 loss_i_size: 500.19 loss_j_size: 615.97\n",
      "Loss: 634.95 | loss_i: 475.49 loss_j: 798.78 loss_edge_type: 589.38 loss_i_size: 622.51 loss_j_size: 688.61\n",
      "Loss: 466.41 | loss_i: 256.86 loss_j: 465.44 loss_edge_type: 530.28 loss_i_size: 491.80 loss_j_size: 587.67\n",
      "Loss: 446.46 | loss_i: 247.18 loss_j: 467.13 loss_edge_type: 500.21 loss_i_size: 469.16 loss_j_size: 548.60\n",
      "Loss: 537.13 | loss_i: 291.90 loss_j: 569.27 loss_edge_type: 581.97 loss_i_size: 577.90 loss_j_size: 664.61\n",
      "Loss: 624.01 | loss_i: 388.28 loss_j: 786.98 loss_edge_type: 612.59 loss_i_size: 604.19 loss_j_size: 728.02\n",
      "Loss: 569.54 | loss_i: 345.43 loss_j: 597.36 loss_edge_type: 580.90 loss_i_size: 610.97 loss_j_size: 713.03\n",
      "Loss: 528.26 | loss_i: 321.13 loss_j: 640.98 loss_edge_type: 557.20 loss_i_size: 508.54 loss_j_size: 613.44\n",
      "Loss: 562.53 | loss_i: 350.56 loss_j: 698.00 loss_edge_type: 581.60 loss_i_size: 538.19 loss_j_size: 644.29\n",
      "Loss: 653.24 | loss_i: 395.00 loss_j: 703.14 loss_edge_type: 673.98 loss_i_size: 682.47 loss_j_size: 811.62\n",
      "Loss: 513.32 | loss_i: 288.00 loss_j: 545.66 loss_edge_type: 529.09 loss_i_size: 554.74 loss_j_size: 649.12\n",
      "Loss: 644.41 | loss_i: 419.24 loss_j: 880.17 loss_edge_type: 600.83 loss_i_size: 588.64 loss_j_size: 733.17\n",
      "Loss: 616.27 | loss_i: 398.51 loss_j: 892.84 loss_edge_type: 634.05 loss_i_size: 536.86 loss_j_size: 619.10\n",
      "Loss: 444.17 | loss_i: 203.99 loss_j: 381.59 loss_edge_type: 465.86 loss_i_size: 544.52 loss_j_size: 624.91\n",
      "Loss: 651.53 | loss_i: 440.53 loss_j: 925.85 loss_edge_type: 657.82 loss_i_size: 576.53 loss_j_size: 656.93\n",
      "Loss: 565.51 | loss_i: 356.44 loss_j: 686.94 loss_edge_type: 595.41 loss_i_size: 549.00 loss_j_size: 639.75\n",
      "Loss: 517.10 | loss_i: 267.89 loss_j: 516.02 loss_edge_type: 529.52 loss_i_size: 581.80 loss_j_size: 690.28\n",
      "Loss: 522.56 | loss_i: 318.84 loss_j: 601.20 loss_edge_type: 543.89 loss_i_size: 524.65 loss_j_size: 624.23\n",
      "Loss: 535.23 | loss_i: 266.47 loss_j: 475.81 loss_edge_type: 580.82 loss_i_size: 643.66 loss_j_size: 709.39\n",
      "Loss: 602.88 | loss_i: 337.76 loss_j: 641.70 loss_edge_type: 674.29 loss_i_size: 617.47 loss_j_size: 743.17\n",
      "Loss: 476.03 | loss_i: 266.32 loss_j: 505.99 loss_edge_type: 496.66 loss_i_size: 497.80 loss_j_size: 613.37\n",
      "Loss: 561.30 | loss_i: 385.65 loss_j: 696.01 loss_edge_type: 586.06 loss_i_size: 529.56 loss_j_size: 609.22\n",
      "Loss: 442.58 | loss_i: 249.18 loss_j: 482.13 loss_edge_type: 482.07 loss_i_size: 454.47 loss_j_size: 545.06\n",
      "Loss: 450.79 | loss_i: 231.40 loss_j: 431.21 loss_edge_type: 477.18 loss_i_size: 514.16 loss_j_size: 600.00\n",
      "Loss: 423.03 | loss_i: 252.71 loss_j: 425.38 loss_edge_type: 485.39 loss_i_size: 427.60 loss_j_size: 524.07\n",
      "Loss: 581.18 | loss_i: 358.74 loss_j: 725.98 loss_edge_type: 609.42 loss_i_size: 547.48 loss_j_size: 664.28\n",
      "Loss: 618.75 | loss_i: 391.00 loss_j: 701.96 loss_edge_type: 620.12 loss_i_size: 641.01 loss_j_size: 739.68\n",
      "Loss: 570.75 | loss_i: 337.93 loss_j: 651.92 loss_edge_type: 559.00 loss_i_size: 597.56 loss_j_size: 707.35\n",
      "Loss: 399.55 | loss_i: 185.67 loss_j: 332.43 loss_edge_type: 450.34 loss_i_size: 491.21 loss_j_size: 538.09\n",
      "Loss: 498.70 | loss_i: 266.06 loss_j: 549.02 loss_edge_type: 490.93 loss_i_size: 546.22 loss_j_size: 641.30\n",
      "Loss: 502.39 | loss_i: 275.55 loss_j: 524.89 loss_edge_type: 552.34 loss_i_size: 530.57 loss_j_size: 628.62\n",
      "Loss: 561.02 | loss_i: 341.51 loss_j: 696.75 loss_edge_type: 564.81 loss_i_size: 548.08 loss_j_size: 653.91\n",
      "Loss: 699.89 | loss_i: 475.02 loss_j: 1013.74 loss_edge_type: 686.80 loss_i_size: 603.33 loss_j_size: 720.57\n",
      "Loss: 642.85 | loss_i: 478.83 loss_j: 844.92 loss_edge_type: 644.28 loss_i_size: 566.67 loss_j_size: 679.51\n",
      "Loss: 556.88 | loss_i: 313.78 loss_j: 645.79 loss_edge_type: 559.12 loss_i_size: 585.78 loss_j_size: 679.92\n",
      "Loss: 512.39 | loss_i: 284.04 loss_j: 525.72 loss_edge_type: 574.84 loss_i_size: 539.05 loss_j_size: 638.31\n",
      "Loss: 574.15 | loss_i: 387.22 loss_j: 679.86 loss_edge_type: 599.01 loss_i_size: 548.40 loss_j_size: 656.28\n",
      "Loss: 506.88 | loss_i: 285.00 loss_j: 549.34 loss_edge_type: 536.24 loss_i_size: 522.88 loss_j_size: 640.94\n",
      "Loss: 645.66 | loss_i: 478.34 loss_j: 907.88 loss_edge_type: 601.87 loss_i_size: 579.83 loss_j_size: 660.39\n",
      "Loss: 541.45 | loss_i: 317.84 loss_j: 575.80 loss_edge_type: 556.64 loss_i_size: 580.33 loss_j_size: 676.63\n",
      "Loss: 538.42 | loss_i: 294.30 loss_j: 605.35 loss_edge_type: 554.88 loss_i_size: 569.63 loss_j_size: 667.93\n",
      "Loss: 472.55 | loss_i: 272.29 loss_j: 503.38 loss_edge_type: 516.76 loss_i_size: 487.09 loss_j_size: 583.25\n",
      "Loss: 535.00 | loss_i: 338.49 loss_j: 615.01 loss_edge_type: 539.18 loss_i_size: 538.45 loss_j_size: 643.86\n",
      "Loss: 399.78 | loss_i: 190.78 loss_j: 326.82 loss_edge_type: 431.70 loss_i_size: 496.96 loss_j_size: 552.62\n",
      "Loss: 601.30 | loss_i: 396.04 loss_j: 802.60 loss_edge_type: 560.54 loss_i_size: 570.46 loss_j_size: 676.89\n",
      "Loss: 599.16 | loss_i: 407.77 loss_j: 798.88 loss_edge_type: 640.90 loss_i_size: 533.23 loss_j_size: 615.01\n",
      "Loss: 698.67 | loss_i: 422.54 loss_j: 787.34 loss_edge_type: 693.83 loss_i_size: 748.53 loss_j_size: 841.09\n",
      "Loss: 457.71 | loss_i: 240.84 loss_j: 453.27 loss_edge_type: 505.06 loss_i_size: 500.95 loss_j_size: 588.41\n",
      "Loss: 615.16 | loss_i: 478.44 loss_j: 909.48 loss_edge_type: 583.55 loss_i_size: 499.37 loss_j_size: 604.98\n",
      "Loss: 622.12 | loss_i: 382.08 loss_j: 782.17 loss_edge_type: 662.69 loss_i_size: 578.26 loss_j_size: 705.43\n",
      "Loss: 542.03 | loss_i: 358.32 loss_j: 675.65 loss_edge_type: 568.44 loss_i_size: 502.00 loss_j_size: 605.71\n",
      "Loss: 80.48 | loss_i: 56.16 loss_j: 92.32 loss_edge_type: 76.39 loss_i_size: 83.07 loss_j_size: 94.45\n",
      "Validation Loss: 566.27 | loss_i: 411.70 loss_j: 757.76 loss_edge_type: 585.19 loss_i_size: 475.61 loss_j_size: 601.09\n",
      "Validation Loss: 499.19 | loss_i: 344.34 loss_j: 590.93 loss_edge_type: 480.46 loss_i_size: 482.17 loss_j_size: 598.06\n",
      "Validation Loss: 794.08 | loss_i: 664.12 loss_j: 1238.79 loss_edge_type: 682.52 loss_i_size: 627.15 loss_j_size: 757.82\n",
      "Validation Loss: 441.38 | loss_i: 281.67 loss_j: 507.47 loss_edge_type: 523.40 loss_i_size: 415.49 loss_j_size: 478.88\n",
      "Validation Loss: 396.52 | loss_i: 211.89 loss_j: 379.00 loss_edge_type: 455.75 loss_i_size: 420.89 loss_j_size: 515.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:38<00:52, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 345.97 | loss_i: 191.98 loss_j: 388.93 loss_edge_type: 354.07 loss_i_size: 360.98 loss_j_size: 433.89\n",
      "Training loss: 540.8396\n",
      "Validation loss: 507.2360\n",
      "\n",
      "------------------------- Epoch 16 -------------------------\n",
      "Loss: 554.35 | loss_i: 335.29 loss_j: 611.83 loss_edge_type: 588.97 loss_i_size: 568.45 loss_j_size: 667.19\n",
      "Loss: 683.50 | loss_i: 394.02 loss_j: 760.51 loss_edge_type: 645.01 loss_i_size: 769.46 loss_j_size: 848.48\n",
      "Loss: 634.36 | loss_i: 483.63 loss_j: 845.79 loss_edge_type: 607.87 loss_i_size: 556.84 loss_j_size: 677.68\n",
      "Loss: 437.84 | loss_i: 207.98 loss_j: 416.07 loss_edge_type: 454.94 loss_i_size: 515.14 loss_j_size: 595.07\n",
      "Loss: 543.96 | loss_i: 335.69 loss_j: 612.94 loss_edge_type: 521.64 loss_i_size: 569.64 loss_j_size: 679.90\n",
      "Loss: 487.30 | loss_i: 294.90 loss_j: 522.08 loss_edge_type: 540.97 loss_i_size: 498.63 loss_j_size: 579.93\n",
      "Loss: 453.36 | loss_i: 267.59 loss_j: 456.70 loss_edge_type: 490.28 loss_i_size: 487.02 loss_j_size: 565.21\n",
      "Loss: 608.74 | loss_i: 396.08 loss_j: 735.87 loss_edge_type: 603.25 loss_i_size: 615.81 loss_j_size: 692.67\n",
      "Loss: 410.64 | loss_i: 206.66 loss_j: 416.59 loss_edge_type: 437.93 loss_i_size: 456.20 loss_j_size: 535.83\n",
      "Loss: 639.08 | loss_i: 395.81 loss_j: 767.09 loss_edge_type: 676.11 loss_i_size: 618.40 loss_j_size: 737.99\n",
      "Loss: 530.40 | loss_i: 310.55 loss_j: 584.98 loss_edge_type: 554.54 loss_i_size: 553.07 loss_j_size: 648.88\n",
      "Loss: 628.39 | loss_i: 352.77 loss_j: 717.88 loss_edge_type: 608.12 loss_i_size: 682.56 loss_j_size: 780.60\n",
      "Loss: 464.04 | loss_i: 237.50 loss_j: 433.51 loss_edge_type: 515.57 loss_i_size: 521.36 loss_j_size: 612.26\n",
      "Loss: 467.62 | loss_i: 244.40 loss_j: 485.86 loss_edge_type: 512.79 loss_i_size: 503.26 loss_j_size: 591.80\n",
      "Loss: 493.86 | loss_i: 312.60 loss_j: 516.01 loss_edge_type: 532.29 loss_i_size: 504.39 loss_j_size: 604.00\n",
      "Loss: 508.37 | loss_i: 357.70 loss_j: 669.61 loss_edge_type: 566.18 loss_i_size: 425.96 loss_j_size: 522.39\n",
      "Loss: 707.31 | loss_i: 519.64 loss_j: 1105.24 loss_edge_type: 669.32 loss_i_size: 574.75 loss_j_size: 667.62\n",
      "Loss: 649.89 | loss_i: 438.02 loss_j: 803.13 loss_edge_type: 664.48 loss_i_size: 620.58 loss_j_size: 723.24\n",
      "Loss: 489.34 | loss_i: 248.55 loss_j: 504.76 loss_edge_type: 527.14 loss_i_size: 537.31 loss_j_size: 628.93\n",
      "Loss: 556.91 | loss_i: 330.77 loss_j: 632.76 loss_edge_type: 600.36 loss_i_size: 565.71 loss_j_size: 654.98\n",
      "Loss: 530.98 | loss_i: 310.63 loss_j: 621.52 loss_edge_type: 581.15 loss_i_size: 521.63 loss_j_size: 619.95\n",
      "Loss: 566.68 | loss_i: 354.77 loss_j: 664.86 loss_edge_type: 609.94 loss_i_size: 552.30 loss_j_size: 651.52\n",
      "Loss: 382.85 | loss_i: 203.55 loss_j: 345.81 loss_edge_type: 451.65 loss_i_size: 408.47 loss_j_size: 504.77\n",
      "Loss: 493.36 | loss_i: 319.95 loss_j: 545.36 loss_edge_type: 530.09 loss_i_size: 476.35 loss_j_size: 595.06\n",
      "Loss: 577.20 | loss_i: 400.90 loss_j: 737.61 loss_edge_type: 584.68 loss_i_size: 536.61 loss_j_size: 626.22\n",
      "Loss: 501.79 | loss_i: 285.25 loss_j: 575.22 loss_edge_type: 498.38 loss_i_size: 533.61 loss_j_size: 616.50\n",
      "Loss: 436.76 | loss_i: 223.32 loss_j: 416.02 loss_edge_type: 489.65 loss_i_size: 481.44 loss_j_size: 573.36\n",
      "Loss: 517.93 | loss_i: 320.17 loss_j: 574.50 loss_edge_type: 538.56 loss_i_size: 522.35 loss_j_size: 634.06\n",
      "Loss: 572.55 | loss_i: 314.38 loss_j: 625.02 loss_edge_type: 585.84 loss_i_size: 609.50 loss_j_size: 728.01\n",
      "Loss: 486.83 | loss_i: 305.28 loss_j: 543.56 loss_edge_type: 504.91 loss_i_size: 490.23 loss_j_size: 590.16\n",
      "Loss: 472.44 | loss_i: 269.68 loss_j: 545.07 loss_edge_type: 522.15 loss_i_size: 468.82 loss_j_size: 556.48\n",
      "Loss: 555.09 | loss_i: 325.73 loss_j: 612.45 loss_edge_type: 581.66 loss_i_size: 575.10 loss_j_size: 680.52\n",
      "Loss: 512.98 | loss_i: 294.58 loss_j: 577.86 loss_edge_type: 556.97 loss_i_size: 513.88 loss_j_size: 621.61\n",
      "Loss: 486.77 | loss_i: 249.52 loss_j: 516.93 loss_edge_type: 511.80 loss_i_size: 536.63 loss_j_size: 618.97\n",
      "Loss: 500.86 | loss_i: 310.73 loss_j: 561.49 loss_edge_type: 537.58 loss_i_size: 488.59 loss_j_size: 605.91\n",
      "Loss: 490.85 | loss_i: 290.53 loss_j: 584.19 loss_edge_type: 540.94 loss_i_size: 481.88 loss_j_size: 556.70\n",
      "Loss: 448.56 | loss_i: 242.77 loss_j: 412.41 loss_edge_type: 446.10 loss_i_size: 533.65 loss_j_size: 607.89\n",
      "Loss: 476.84 | loss_i: 272.91 loss_j: 533.19 loss_edge_type: 528.87 loss_i_size: 482.60 loss_j_size: 566.65\n",
      "Loss: 520.20 | loss_i: 320.38 loss_j: 648.50 loss_edge_type: 563.27 loss_i_size: 474.99 loss_j_size: 593.87\n",
      "Loss: 546.47 | loss_i: 310.29 loss_j: 647.37 loss_edge_type: 591.52 loss_i_size: 539.04 loss_j_size: 644.13\n",
      "Loss: 643.41 | loss_i: 438.71 loss_j: 845.47 loss_edge_type: 677.00 loss_i_size: 576.41 loss_j_size: 679.47\n",
      "Loss: 528.13 | loss_i: 297.31 loss_j: 561.56 loss_edge_type: 540.23 loss_i_size: 564.48 loss_j_size: 677.05\n",
      "Loss: 570.97 | loss_i: 355.43 loss_j: 734.96 loss_edge_type: 573.76 loss_i_size: 552.65 loss_j_size: 638.07\n",
      "Loss: 405.87 | loss_i: 208.40 loss_j: 404.51 loss_edge_type: 472.51 loss_i_size: 423.21 loss_j_size: 520.72\n",
      "Loss: 636.31 | loss_i: 415.55 loss_j: 787.05 loss_edge_type: 633.81 loss_i_size: 617.73 loss_j_size: 727.39\n",
      "Loss: 417.54 | loss_i: 207.40 loss_j: 407.99 loss_edge_type: 454.20 loss_i_size: 469.87 loss_j_size: 548.26\n",
      "Loss: 546.98 | loss_i: 394.38 loss_j: 787.05 loss_edge_type: 548.09 loss_i_size: 451.01 loss_j_size: 554.35\n",
      "Loss: 512.27 | loss_i: 270.85 loss_j: 483.14 loss_edge_type: 498.30 loss_i_size: 618.31 loss_j_size: 690.77\n",
      "Loss: 459.42 | loss_i: 282.47 loss_j: 474.67 loss_edge_type: 531.62 loss_i_size: 464.23 loss_j_size: 544.10\n",
      "Loss: 463.60 | loss_i: 229.31 loss_j: 475.82 loss_edge_type: 505.41 loss_i_size: 514.48 loss_j_size: 592.98\n",
      "Loss: 548.55 | loss_i: 362.61 loss_j: 704.76 loss_edge_type: 554.63 loss_i_size: 526.83 loss_j_size: 593.93\n",
      "Loss: 65.10 | loss_i: 19.30 loss_j: 36.54 loss_edge_type: 58.95 loss_i_size: 99.35 loss_j_size: 111.34\n",
      "Validation Loss: 447.33 | loss_i: 271.58 loss_j: 502.26 loss_edge_type: 465.36 loss_i_size: 439.62 loss_j_size: 557.83\n",
      "Validation Loss: 472.38 | loss_i: 278.52 loss_j: 596.98 loss_edge_type: 485.82 loss_i_size: 459.93 loss_j_size: 540.64\n",
      "Validation Loss: 384.32 | loss_i: 210.88 loss_j: 336.41 loss_edge_type: 441.67 loss_i_size: 412.14 loss_j_size: 520.52\n",
      "Validation Loss: 781.24 | loss_i: 650.33 loss_j: 1204.37 loss_edge_type: 681.98 loss_i_size: 641.98 loss_j_size: 727.55\n",
      "Validation Loss: 590.60 | loss_i: 479.21 loss_j: 795.84 loss_edge_type: 601.50 loss_i_size: 488.07 loss_j_size: 588.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [02:48<00:41, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 218.95 | loss_i: 103.01 loss_j: 170.36 loss_edge_type: 256.37 loss_i_size: 251.41 loss_j_size: 313.62\n",
      "Training loss: 515.8732\n",
      "Validation loss: 482.4711\n",
      "\n",
      "------------------------- Epoch 17 -------------------------\n",
      "Loss: 465.69 | loss_i: 276.26 loss_j: 489.43 loss_edge_type: 516.14 loss_i_size: 474.44 loss_j_size: 572.19\n",
      "Loss: 491.83 | loss_i: 301.62 loss_j: 496.23 loss_edge_type: 536.16 loss_i_size: 514.96 loss_j_size: 610.19\n",
      "Loss: 665.50 | loss_i: 408.07 loss_j: 838.04 loss_edge_type: 672.72 loss_i_size: 647.10 loss_j_size: 761.57\n",
      "Loss: 489.90 | loss_i: 297.94 loss_j: 534.69 loss_edge_type: 518.00 loss_i_size: 507.86 loss_j_size: 591.02\n",
      "Loss: 487.51 | loss_i: 288.30 loss_j: 542.18 loss_edge_type: 520.76 loss_i_size: 505.03 loss_j_size: 581.30\n",
      "Loss: 491.21 | loss_i: 230.03 loss_j: 502.79 loss_edge_type: 487.02 loss_i_size: 566.86 loss_j_size: 669.34\n",
      "Loss: 392.81 | loss_i: 201.18 loss_j: 336.45 loss_edge_type: 423.08 loss_i_size: 456.60 loss_j_size: 546.74\n",
      "Loss: 542.76 | loss_i: 331.07 loss_j: 669.21 loss_edge_type: 605.45 loss_i_size: 504.64 loss_j_size: 603.44\n",
      "Loss: 553.45 | loss_i: 412.58 loss_j: 778.73 loss_edge_type: 545.85 loss_i_size: 467.23 loss_j_size: 562.88\n",
      "Loss: 407.41 | loss_i: 180.70 loss_j: 302.50 loss_edge_type: 443.41 loss_i_size: 519.39 loss_j_size: 591.05\n",
      "Loss: 549.55 | loss_i: 300.55 loss_j: 555.42 loss_edge_type: 579.10 loss_i_size: 619.73 loss_j_size: 692.93\n",
      "Loss: 526.95 | loss_i: 317.47 loss_j: 639.11 loss_edge_type: 584.02 loss_i_size: 509.57 loss_j_size: 584.61\n",
      "Loss: 560.34 | loss_i: 337.87 loss_j: 642.50 loss_edge_type: 615.41 loss_i_size: 551.01 loss_j_size: 654.93\n",
      "Loss: 633.09 | loss_i: 548.73 loss_j: 986.40 loss_edge_type: 616.06 loss_i_size: 457.64 loss_j_size: 556.59\n",
      "Loss: 485.47 | loss_i: 300.09 loss_j: 562.42 loss_edge_type: 505.41 loss_i_size: 477.68 loss_j_size: 581.76\n",
      "Loss: 487.45 | loss_i: 247.69 loss_j: 460.49 loss_edge_type: 498.58 loss_i_size: 570.54 loss_j_size: 659.93\n",
      "Loss: 427.36 | loss_i: 200.90 loss_j: 374.81 loss_edge_type: 455.90 loss_i_size: 503.18 loss_j_size: 602.02\n",
      "Loss: 504.45 | loss_i: 274.15 loss_j: 553.54 loss_edge_type: 532.90 loss_i_size: 543.89 loss_j_size: 617.76\n",
      "Loss: 517.96 | loss_i: 360.55 loss_j: 676.35 loss_edge_type: 564.77 loss_i_size: 442.88 loss_j_size: 545.24\n",
      "Loss: 446.19 | loss_i: 258.52 loss_j: 516.81 loss_edge_type: 484.72 loss_i_size: 433.24 loss_j_size: 537.64\n",
      "Loss: 418.25 | loss_i: 210.42 loss_j: 390.42 loss_edge_type: 442.88 loss_i_size: 477.23 loss_j_size: 570.33\n",
      "Loss: 498.62 | loss_i: 316.26 loss_j: 555.98 loss_edge_type: 505.73 loss_i_size: 514.44 loss_j_size: 600.70\n",
      "Loss: 420.16 | loss_i: 203.90 loss_j: 370.12 loss_edge_type: 482.34 loss_i_size: 475.53 loss_j_size: 568.87\n",
      "Loss: 610.52 | loss_i: 383.41 loss_j: 810.92 loss_edge_type: 629.87 loss_i_size: 581.82 loss_j_size: 646.56\n",
      "Loss: 611.59 | loss_i: 426.09 loss_j: 845.93 loss_edge_type: 582.94 loss_i_size: 553.29 loss_j_size: 649.73\n",
      "Loss: 511.32 | loss_i: 310.60 loss_j: 610.87 loss_edge_type: 566.77 loss_i_size: 493.86 loss_j_size: 574.49\n",
      "Loss: 403.41 | loss_i: 201.69 loss_j: 388.05 loss_edge_type: 451.53 loss_i_size: 446.55 loss_j_size: 529.24\n",
      "Loss: 439.64 | loss_i: 232.54 loss_j: 461.36 loss_edge_type: 510.71 loss_i_size: 442.93 loss_j_size: 550.68\n",
      "Loss: 418.90 | loss_i: 238.93 loss_j: 407.71 loss_edge_type: 456.80 loss_i_size: 439.21 loss_j_size: 551.84\n",
      "Loss: 459.18 | loss_i: 245.77 loss_j: 468.79 loss_edge_type: 518.76 loss_i_size: 493.62 loss_j_size: 568.96\n",
      "Loss: 415.24 | loss_i: 206.93 loss_j: 369.68 loss_edge_type: 448.07 loss_i_size: 482.66 loss_j_size: 568.85\n",
      "Loss: 451.83 | loss_i: 270.97 loss_j: 487.99 loss_edge_type: 493.60 loss_i_size: 448.80 loss_j_size: 557.77\n",
      "Loss: 418.43 | loss_i: 198.91 loss_j: 337.50 loss_edge_type: 469.56 loss_i_size: 491.48 loss_j_size: 594.69\n",
      "Loss: 640.73 | loss_i: 374.19 loss_j: 777.57 loss_edge_type: 646.89 loss_i_size: 642.93 loss_j_size: 762.07\n",
      "Loss: 473.04 | loss_i: 246.30 loss_j: 465.82 loss_edge_type: 516.92 loss_i_size: 533.91 loss_j_size: 602.25\n",
      "Loss: 420.10 | loss_i: 237.06 loss_j: 421.32 loss_edge_type: 494.54 loss_i_size: 434.35 loss_j_size: 513.25\n",
      "Loss: 422.23 | loss_i: 223.77 loss_j: 426.48 loss_edge_type: 448.02 loss_i_size: 467.75 loss_j_size: 545.14\n",
      "Loss: 610.63 | loss_i: 495.27 loss_j: 944.31 loss_edge_type: 577.16 loss_i_size: 488.78 loss_j_size: 547.61\n",
      "Loss: 506.55 | loss_i: 313.20 loss_j: 586.08 loss_edge_type: 551.65 loss_i_size: 487.69 loss_j_size: 594.11\n",
      "Loss: 536.81 | loss_i: 296.09 loss_j: 637.86 loss_edge_type: 542.68 loss_i_size: 550.12 loss_j_size: 657.30\n",
      "Loss: 433.70 | loss_i: 229.94 loss_j: 419.70 loss_edge_type: 482.11 loss_i_size: 482.78 loss_j_size: 553.95\n",
      "Loss: 472.66 | loss_i: 296.19 loss_j: 567.53 loss_edge_type: 509.29 loss_i_size: 457.41 loss_j_size: 532.88\n",
      "Loss: 519.16 | loss_i: 328.28 loss_j: 632.20 loss_edge_type: 535.00 loss_i_size: 494.71 loss_j_size: 605.61\n",
      "Loss: 641.10 | loss_i: 387.70 loss_j: 687.87 loss_edge_type: 649.96 loss_i_size: 687.31 loss_j_size: 792.69\n",
      "Loss: 588.04 | loss_i: 375.89 loss_j: 764.44 loss_edge_type: 564.33 loss_i_size: 576.61 loss_j_size: 658.94\n",
      "Loss: 441.18 | loss_i: 212.93 loss_j: 400.15 loss_edge_type: 472.41 loss_i_size: 508.17 loss_j_size: 612.22\n",
      "Loss: 536.46 | loss_i: 305.00 loss_j: 588.59 loss_edge_type: 564.54 loss_i_size: 568.31 loss_j_size: 655.84\n",
      "Loss: 442.66 | loss_i: 238.35 loss_j: 474.55 loss_edge_type: 485.55 loss_i_size: 465.19 loss_j_size: 549.66\n",
      "Loss: 487.24 | loss_i: 243.31 loss_j: 460.66 loss_edge_type: 508.42 loss_i_size: 564.75 loss_j_size: 659.03\n",
      "Loss: 491.60 | loss_i: 242.19 loss_j: 456.06 loss_edge_type: 522.73 loss_i_size: 585.07 loss_j_size: 651.94\n",
      "Loss: 540.80 | loss_i: 351.16 loss_j: 696.37 loss_edge_type: 594.16 loss_i_size: 487.39 loss_j_size: 574.92\n",
      "Loss: 38.82 | loss_i: 22.43 loss_j: 37.84 loss_edge_type: 50.66 loss_i_size: 33.34 loss_j_size: 49.82\n",
      "Validation Loss: 428.44 | loss_i: 275.46 loss_j: 471.64 loss_edge_type: 446.06 loss_i_size: 442.08 loss_j_size: 506.94\n",
      "Validation Loss: 373.54 | loss_i: 218.09 loss_j: 415.54 loss_edge_type: 415.29 loss_i_size: 370.14 loss_j_size: 448.64\n",
      "Validation Loss: 659.23 | loss_i: 512.61 loss_j: 950.42 loss_edge_type: 584.15 loss_i_size: 582.89 loss_j_size: 666.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [02:58<00:31, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 488.40 | loss_i: 349.31 loss_j: 667.84 loss_edge_type: 522.53 loss_i_size: 418.81 loss_j_size: 483.52\n",
      "Validation Loss: 427.96 | loss_i: 234.92 loss_j: 413.33 loss_edge_type: 456.30 loss_i_size: 475.13 loss_j_size: 560.13\n",
      "Validation Loss: 345.88 | loss_i: 243.80 loss_j: 448.89 loss_edge_type: 359.58 loss_i_size: 312.11 loss_j_size: 365.01\n",
      "Training loss: 489.3742\n",
      "Validation loss: 453.9075\n",
      "\n",
      "------------------------- Epoch 18 -------------------------\n",
      "Loss: 514.36 | loss_i: 304.53 loss_j: 537.12 loss_edge_type: 531.85 loss_i_size: 551.13 loss_j_size: 647.19\n",
      "Loss: 411.71 | loss_i: 223.41 loss_j: 359.65 loss_edge_type: 516.32 loss_i_size: 435.81 loss_j_size: 523.36\n",
      "Loss: 462.21 | loss_i: 246.38 loss_j: 449.15 loss_edge_type: 507.91 loss_i_size: 502.40 loss_j_size: 605.20\n",
      "Loss: 436.47 | loss_i: 247.78 loss_j: 467.22 loss_edge_type: 488.50 loss_i_size: 461.10 loss_j_size: 517.74\n",
      "Loss: 440.13 | loss_i: 253.18 loss_j: 433.30 loss_edge_type: 496.85 loss_i_size: 476.70 loss_j_size: 540.62\n",
      "Loss: 393.60 | loss_i: 222.20 loss_j: 410.36 loss_edge_type: 404.41 loss_i_size: 421.45 loss_j_size: 509.59\n",
      "Loss: 522.90 | loss_i: 322.32 loss_j: 579.13 loss_edge_type: 550.61 loss_i_size: 538.64 loss_j_size: 623.78\n",
      "Loss: 455.71 | loss_i: 277.20 loss_j: 532.33 loss_edge_type: 515.17 loss_i_size: 431.18 loss_j_size: 522.69\n",
      "Loss: 484.38 | loss_i: 382.40 loss_j: 744.83 loss_edge_type: 533.29 loss_i_size: 338.95 loss_j_size: 422.43\n",
      "Loss: 491.06 | loss_i: 279.93 loss_j: 531.51 loss_edge_type: 524.54 loss_i_size: 516.08 loss_j_size: 603.26\n",
      "Loss: 537.90 | loss_i: 302.33 loss_j: 607.90 loss_edge_type: 528.36 loss_i_size: 573.07 loss_j_size: 677.83\n",
      "Loss: 616.71 | loss_i: 461.40 loss_j: 960.32 loss_edge_type: 615.14 loss_i_size: 477.14 loss_j_size: 569.56\n",
      "Loss: 447.47 | loss_i: 226.15 loss_j: 374.84 loss_edge_type: 489.50 loss_i_size: 529.72 loss_j_size: 617.14\n",
      "Loss: 420.45 | loss_i: 209.60 loss_j: 380.13 loss_edge_type: 497.37 loss_i_size: 460.84 loss_j_size: 554.33\n",
      "Loss: 496.64 | loss_i: 320.49 loss_j: 586.82 loss_edge_type: 548.94 loss_i_size: 462.85 loss_j_size: 564.09\n",
      "Loss: 569.46 | loss_i: 410.38 loss_j: 763.98 loss_edge_type: 610.97 loss_i_size: 495.16 loss_j_size: 566.83\n",
      "Loss: 422.65 | loss_i: 216.99 loss_j: 475.37 loss_edge_type: 459.72 loss_i_size: 445.57 loss_j_size: 515.60\n",
      "Loss: 448.11 | loss_i: 239.90 loss_j: 438.09 loss_edge_type: 512.74 loss_i_size: 483.32 loss_j_size: 566.48\n",
      "Loss: 430.03 | loss_i: 275.50 loss_j: 468.75 loss_edge_type: 476.38 loss_i_size: 440.78 loss_j_size: 488.75\n",
      "Loss: 430.89 | loss_i: 217.75 loss_j: 404.19 loss_edge_type: 478.09 loss_i_size: 476.93 loss_j_size: 577.50\n",
      "Loss: 526.69 | loss_i: 359.37 loss_j: 803.20 loss_edge_type: 511.58 loss_i_size: 444.12 loss_j_size: 515.19\n",
      "Loss: 427.36 | loss_i: 204.12 loss_j: 420.89 loss_edge_type: 427.08 loss_i_size: 493.65 loss_j_size: 591.04\n",
      "Loss: 515.62 | loss_i: 299.64 loss_j: 644.68 loss_edge_type: 553.44 loss_i_size: 513.38 loss_j_size: 566.97\n",
      "Loss: 436.03 | loss_i: 228.43 loss_j: 440.15 loss_edge_type: 538.97 loss_i_size: 458.54 loss_j_size: 514.07\n",
      "Loss: 466.41 | loss_i: 247.69 loss_j: 483.26 loss_edge_type: 499.31 loss_i_size: 517.78 loss_j_size: 584.00\n",
      "Loss: 408.68 | loss_i: 192.45 loss_j: 342.34 loss_edge_type: 478.23 loss_i_size: 470.83 loss_j_size: 559.57\n",
      "Loss: 405.86 | loss_i: 217.05 loss_j: 427.27 loss_edge_type: 454.26 loss_i_size: 429.42 loss_j_size: 501.29\n",
      "Loss: 582.71 | loss_i: 327.68 loss_j: 681.34 loss_edge_type: 592.68 loss_i_size: 611.83 loss_j_size: 700.00\n",
      "Loss: 494.86 | loss_i: 265.47 loss_j: 564.94 loss_edge_type: 517.45 loss_i_size: 538.01 loss_j_size: 588.44\n",
      "Loss: 435.60 | loss_i: 240.37 loss_j: 509.60 loss_edge_type: 488.71 loss_i_size: 429.87 loss_j_size: 509.44\n",
      "Loss: 476.21 | loss_i: 297.84 loss_j: 480.83 loss_edge_type: 510.44 loss_i_size: 514.30 loss_j_size: 577.65\n",
      "Loss: 492.69 | loss_i: 279.96 loss_j: 540.66 loss_edge_type: 477.30 loss_i_size: 545.53 loss_j_size: 619.99\n",
      "Loss: 485.59 | loss_i: 295.25 loss_j: 567.86 loss_edge_type: 516.96 loss_i_size: 477.42 loss_j_size: 570.46\n",
      "Loss: 467.19 | loss_i: 295.20 loss_j: 543.08 loss_edge_type: 524.35 loss_i_size: 440.33 loss_j_size: 532.96\n",
      "Loss: 469.55 | loss_i: 279.76 loss_j: 549.55 loss_edge_type: 497.81 loss_i_size: 464.15 loss_j_size: 556.46\n",
      "Loss: 460.66 | loss_i: 270.25 loss_j: 489.46 loss_edge_type: 495.41 loss_i_size: 474.72 loss_j_size: 573.47\n",
      "Loss: 506.31 | loss_i: 271.87 loss_j: 507.20 loss_edge_type: 493.87 loss_i_size: 590.65 loss_j_size: 667.98\n",
      "Loss: 360.77 | loss_i: 174.88 loss_j: 311.83 loss_edge_type: 442.74 loss_i_size: 393.18 loss_j_size: 481.23\n",
      "Loss: 400.41 | loss_i: 245.07 loss_j: 442.84 loss_edge_type: 437.29 loss_i_size: 395.43 loss_j_size: 481.42\n",
      "Loss: 406.00 | loss_i: 226.31 loss_j: 456.48 loss_edge_type: 477.15 loss_i_size: 403.78 loss_j_size: 466.30\n",
      "Loss: 491.93 | loss_i: 285.49 loss_j: 484.16 loss_edge_type: 550.03 loss_i_size: 532.44 loss_j_size: 607.51\n",
      "Loss: 528.69 | loss_i: 368.22 loss_j: 729.65 loss_edge_type: 494.90 loss_i_size: 486.67 loss_j_size: 564.01\n",
      "Loss: 440.52 | loss_i: 274.00 loss_j: 600.89 loss_edge_type: 483.73 loss_i_size: 385.54 loss_j_size: 458.46\n",
      "Loss: 503.32 | loss_i: 265.41 loss_j: 543.52 loss_edge_type: 514.99 loss_i_size: 544.96 loss_j_size: 647.70\n",
      "Loss: 537.13 | loss_i: 316.98 loss_j: 566.69 loss_edge_type: 554.96 loss_i_size: 584.45 loss_j_size: 662.60\n",
      "Loss: 482.10 | loss_i: 232.31 loss_j: 407.50 loss_edge_type: 520.33 loss_i_size: 580.90 loss_j_size: 669.44\n",
      "Loss: 410.06 | loss_i: 183.33 loss_j: 353.71 loss_edge_type: 464.59 loss_i_size: 486.98 loss_j_size: 561.70\n",
      "Loss: 367.67 | loss_i: 182.85 loss_j: 315.34 loss_edge_type: 426.95 loss_i_size: 419.74 loss_j_size: 493.45\n",
      "Loss: 525.95 | loss_i: 300.00 loss_j: 599.84 loss_edge_type: 546.96 loss_i_size: 550.68 loss_j_size: 632.26\n",
      "Loss: 479.87 | loss_i: 277.80 loss_j: 488.63 loss_edge_type: 483.19 loss_i_size: 533.41 loss_j_size: 616.31\n",
      "Loss: 532.66 | loss_i: 306.16 loss_j: 555.60 loss_edge_type: 584.68 loss_i_size: 570.37 loss_j_size: 646.48\n",
      "Loss: 55.36 | loss_i: 21.53 loss_j: 49.22 loss_edge_type: 52.56 loss_i_size: 67.82 loss_j_size: 85.66\n",
      "Validation Loss: 266.43 | loss_i: 126.59 loss_j: 187.64 loss_edge_type: 335.31 loss_i_size: 321.79 loss_j_size: 360.84\n",
      "Validation Loss: 529.65 | loss_i: 354.39 loss_j: 733.17 loss_edge_type: 489.05 loss_i_size: 495.50 loss_j_size: 576.14\n",
      "Validation Loss: 564.19 | loss_i: 465.88 loss_j: 809.67 loss_edge_type: 566.21 loss_i_size: 443.40 loss_j_size: 535.81\n",
      "Validation Loss: 368.26 | loss_i: 180.65 loss_j: 368.16 loss_edge_type: 429.48 loss_i_size: 405.10 loss_j_size: 457.93\n",
      "Validation Loss: 504.72 | loss_i: 398.66 loss_j: 712.65 loss_edge_type: 519.67 loss_i_size: 413.95 loss_j_size: 478.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:09<00:20, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 301.94 | loss_i: 132.85 loss_j: 290.84 loss_edge_type: 340.54 loss_i_size: 352.11 loss_j_size: 393.35\n",
      "Training loss: 461.7943\n",
      "Validation loss: 422.5337\n",
      "\n",
      "------------------------- Epoch 19 -------------------------\n",
      "Loss: 433.02 | loss_i: 226.90 loss_j: 417.90 loss_edge_type: 492.15 loss_i_size: 489.64 loss_j_size: 538.50\n",
      "Loss: 470.95 | loss_i: 255.11 loss_j: 463.95 loss_edge_type: 493.66 loss_i_size: 534.65 loss_j_size: 607.41\n",
      "Loss: 360.19 | loss_i: 177.34 loss_j: 359.18 loss_edge_type: 437.77 loss_i_size: 384.43 loss_j_size: 442.23\n",
      "Loss: 470.01 | loss_i: 265.14 loss_j: 506.78 loss_edge_type: 484.86 loss_i_size: 514.82 loss_j_size: 578.42\n",
      "Loss: 494.47 | loss_i: 291.75 loss_j: 605.34 loss_edge_type: 521.68 loss_i_size: 485.07 loss_j_size: 568.52\n",
      "Loss: 379.68 | loss_i: 180.77 loss_j: 315.89 loss_edge_type: 450.76 loss_i_size: 445.15 loss_j_size: 505.83\n",
      "Loss: 553.17 | loss_i: 325.42 loss_j: 669.09 loss_edge_type: 584.33 loss_i_size: 569.16 loss_j_size: 617.84\n",
      "Loss: 405.02 | loss_i: 224.55 loss_j: 415.82 loss_edge_type: 475.42 loss_i_size: 413.93 loss_j_size: 495.36\n",
      "Loss: 405.45 | loss_i: 229.62 loss_j: 356.84 loss_edge_type: 460.89 loss_i_size: 474.08 loss_j_size: 505.82\n",
      "Loss: 415.87 | loss_i: 203.75 loss_j: 414.19 loss_edge_type: 486.45 loss_i_size: 453.92 loss_j_size: 521.06\n",
      "Loss: 363.88 | loss_i: 195.70 loss_j: 359.27 loss_edge_type: 403.77 loss_i_size: 391.94 loss_j_size: 468.71\n",
      "Loss: 525.08 | loss_i: 294.70 loss_j: 587.09 loss_edge_type: 524.29 loss_i_size: 578.14 loss_j_size: 641.18\n",
      "Loss: 479.99 | loss_i: 260.97 loss_j: 490.75 loss_edge_type: 539.75 loss_i_size: 514.66 loss_j_size: 593.83\n",
      "Loss: 391.02 | loss_i: 203.82 loss_j: 400.31 loss_edge_type: 449.63 loss_i_size: 417.85 loss_j_size: 483.50\n",
      "Loss: 377.55 | loss_i: 180.55 loss_j: 361.29 loss_edge_type: 417.37 loss_i_size: 415.98 loss_j_size: 512.57\n",
      "Loss: 406.11 | loss_i: 165.58 loss_j: 313.19 loss_edge_type: 449.93 loss_i_size: 520.59 loss_j_size: 581.26\n",
      "Loss: 520.61 | loss_i: 288.61 loss_j: 601.13 loss_edge_type: 532.68 loss_i_size: 555.69 loss_j_size: 624.94\n",
      "Loss: 513.26 | loss_i: 314.76 loss_j: 548.14 loss_edge_type: 546.00 loss_i_size: 535.00 loss_j_size: 622.38\n",
      "Loss: 426.51 | loss_i: 223.13 loss_j: 463.00 loss_edge_type: 486.62 loss_i_size: 443.30 loss_j_size: 516.51\n",
      "Loss: 408.02 | loss_i: 198.96 loss_j: 409.79 loss_edge_type: 489.18 loss_i_size: 426.63 loss_j_size: 515.52\n",
      "Loss: 465.25 | loss_i: 267.06 loss_j: 529.70 loss_edge_type: 527.55 loss_i_size: 458.73 loss_j_size: 543.22\n",
      "Loss: 419.28 | loss_i: 225.86 loss_j: 428.60 loss_edge_type: 521.36 loss_i_size: 419.61 loss_j_size: 500.95\n",
      "Loss: 382.93 | loss_i: 164.00 loss_j: 301.00 loss_edge_type: 446.68 loss_i_size: 465.27 loss_j_size: 537.69\n",
      "Loss: 541.46 | loss_i: 372.65 loss_j: 726.49 loss_edge_type: 559.16 loss_i_size: 490.45 loss_j_size: 558.52\n",
      "Loss: 401.37 | loss_i: 215.65 loss_j: 412.30 loss_edge_type: 442.35 loss_i_size: 428.69 loss_j_size: 507.84\n",
      "Loss: 515.98 | loss_i: 275.90 loss_j: 521.39 loss_edge_type: 566.52 loss_i_size: 574.32 loss_j_size: 641.76\n",
      "Loss: 563.41 | loss_i: 406.95 loss_j: 809.65 loss_edge_type: 603.81 loss_i_size: 481.34 loss_j_size: 515.31\n",
      "Loss: 446.89 | loss_i: 220.44 loss_j: 470.76 loss_edge_type: 499.07 loss_i_size: 486.81 loss_j_size: 557.37\n",
      "Loss: 365.49 | loss_i: 190.26 loss_j: 322.80 loss_edge_type: 419.20 loss_i_size: 415.63 loss_j_size: 479.58\n",
      "Loss: 435.17 | loss_i: 245.63 loss_j: 426.16 loss_edge_type: 501.06 loss_i_size: 474.36 loss_j_size: 528.63\n",
      "Loss: 408.73 | loss_i: 214.80 loss_j: 439.41 loss_edge_type: 489.00 loss_i_size: 407.40 loss_j_size: 493.04\n",
      "Loss: 363.97 | loss_i: 162.69 loss_j: 347.74 loss_edge_type: 380.60 loss_i_size: 422.49 loss_j_size: 506.35\n",
      "Loss: 480.47 | loss_i: 276.35 loss_j: 508.05 loss_edge_type: 546.23 loss_i_size: 491.30 loss_j_size: 580.44\n",
      "Loss: 429.49 | loss_i: 265.30 loss_j: 500.04 loss_edge_type: 477.91 loss_i_size: 404.20 loss_j_size: 500.00\n",
      "Loss: 384.47 | loss_i: 217.81 loss_j: 386.65 loss_edge_type: 438.74 loss_i_size: 409.98 loss_j_size: 469.17\n",
      "Loss: 542.87 | loss_i: 418.18 loss_j: 828.63 loss_edge_type: 525.51 loss_i_size: 441.14 loss_j_size: 500.91\n",
      "Loss: 431.88 | loss_i: 229.36 loss_j: 410.32 loss_edge_type: 485.80 loss_i_size: 476.37 loss_j_size: 557.54\n",
      "Loss: 552.59 | loss_i: 384.09 loss_j: 688.11 loss_edge_type: 587.27 loss_i_size: 529.97 loss_j_size: 573.49\n",
      "Loss: 391.41 | loss_i: 181.39 loss_j: 356.59 loss_edge_type: 450.47 loss_i_size: 453.88 loss_j_size: 514.72\n",
      "Loss: 331.90 | loss_i: 159.94 loss_j: 258.02 loss_edge_type: 383.55 loss_i_size: 399.98 loss_j_size: 458.04\n",
      "Loss: 484.92 | loss_i: 338.48 loss_j: 710.16 loss_edge_type: 500.38 loss_i_size: 421.31 loss_j_size: 454.26\n",
      "Loss: 452.93 | loss_i: 262.01 loss_j: 511.77 loss_edge_type: 497.40 loss_i_size: 462.89 loss_j_size: 530.56\n",
      "Loss: 584.24 | loss_i: 360.86 loss_j: 692.30 loss_edge_type: 628.38 loss_i_size: 579.10 loss_j_size: 660.55\n",
      "Loss: 535.65 | loss_i: 336.85 loss_j: 687.03 loss_edge_type: 618.69 loss_i_size: 469.14 loss_j_size: 566.52\n",
      "Loss: 396.84 | loss_i: 209.68 loss_j: 402.96 loss_edge_type: 444.16 loss_i_size: 427.27 loss_j_size: 500.10\n",
      "Loss: 421.48 | loss_i: 263.90 loss_j: 519.44 loss_edge_type: 492.01 loss_i_size: 385.40 loss_j_size: 446.63\n",
      "Loss: 402.69 | loss_i: 209.28 loss_j: 415.36 loss_edge_type: 461.27 loss_i_size: 430.97 loss_j_size: 496.54\n",
      "Loss: 380.29 | loss_i: 232.54 loss_j: 435.40 loss_edge_type: 419.48 loss_i_size: 364.27 loss_j_size: 449.77\n",
      "Loss: 417.54 | loss_i: 212.76 loss_j: 382.80 loss_edge_type: 462.26 loss_i_size: 474.79 loss_j_size: 555.09\n",
      "Loss: 487.00 | loss_i: 290.44 loss_j: 551.34 loss_edge_type: 525.43 loss_i_size: 490.51 loss_j_size: 577.30\n",
      "Loss: 387.55 | loss_i: 182.64 loss_j: 345.50 loss_edge_type: 459.94 loss_i_size: 447.95 loss_j_size: 501.72\n",
      "Loss: 44.99 | loss_i: 15.62 loss_j: 40.03 loss_edge_type: 43.36 loss_i_size: 59.70 loss_j_size: 66.23\n",
      "Validation Loss: 395.00 | loss_i: 212.19 loss_j: 439.03 loss_edge_type: 452.94 loss_i_size: 400.53 loss_j_size: 470.33\n",
      "Validation Loss: 376.89 | loss_i: 218.37 loss_j: 433.68 loss_edge_type: 376.66 loss_i_size: 409.77 loss_j_size: 445.99\n",
      "Validation Loss: 645.63 | loss_i: 576.89 loss_j: 1043.63 loss_edge_type: 618.14 loss_i_size: 467.15 loss_j_size: 522.36\n",
      "Validation Loss: 442.08 | loss_i: 322.37 loss_j: 548.84 loss_edge_type: 506.25 loss_i_size: 390.27 loss_j_size: 442.67\n",
      "Validation Loss: 305.17 | loss_i: 142.04 loss_j: 243.75 loss_edge_type: 376.97 loss_i_size: 362.04 loss_j_size: 401.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:19<00:10, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 256.64 | loss_i: 118.73 loss_j: 241.46 loss_edge_type: 288.96 loss_i_size: 304.92 loss_j_size: 329.13\n",
      "Training loss: 435.5956\n",
      "Validation loss: 403.5694\n",
      "\n",
      "------------------------- Epoch 20 -------------------------\n",
      "Loss: 395.73 | loss_i: 213.29 loss_j: 388.99 loss_edge_type: 462.77 loss_i_size: 418.04 loss_j_size: 495.54\n",
      "Loss: 470.39 | loss_i: 244.77 loss_j: 504.11 loss_edge_type: 566.73 loss_i_size: 478.61 loss_j_size: 557.71\n",
      "Loss: 418.49 | loss_i: 226.28 loss_j: 444.65 loss_edge_type: 462.55 loss_i_size: 453.04 loss_j_size: 505.93\n",
      "Loss: 420.94 | loss_i: 233.31 loss_j: 398.59 loss_edge_type: 471.17 loss_i_size: 471.70 loss_j_size: 529.90\n",
      "Loss: 375.05 | loss_i: 189.10 loss_j: 330.09 loss_edge_type: 453.99 loss_i_size: 411.36 loss_j_size: 490.71\n",
      "Loss: 398.53 | loss_i: 181.30 loss_j: 359.59 loss_edge_type: 441.15 loss_i_size: 476.02 loss_j_size: 534.58\n",
      "Loss: 409.31 | loss_i: 233.00 loss_j: 459.87 loss_edge_type: 457.61 loss_i_size: 425.70 loss_j_size: 470.36\n",
      "Loss: 458.34 | loss_i: 254.29 loss_j: 507.76 loss_edge_type: 525.86 loss_i_size: 474.02 loss_j_size: 529.79\n",
      "Loss: 370.95 | loss_i: 149.44 loss_j: 300.19 loss_edge_type: 412.22 loss_i_size: 468.31 loss_j_size: 524.57\n",
      "Loss: 498.65 | loss_i: 270.40 loss_j: 531.97 loss_edge_type: 542.30 loss_i_size: 538.25 loss_j_size: 610.33\n",
      "Loss: 469.73 | loss_i: 284.26 loss_j: 535.31 loss_edge_type: 507.67 loss_i_size: 482.44 loss_j_size: 538.94\n",
      "Loss: 400.49 | loss_i: 201.31 loss_j: 361.19 loss_edge_type: 445.77 loss_i_size: 470.26 loss_j_size: 523.94\n",
      "Loss: 407.23 | loss_i: 179.84 loss_j: 357.88 loss_edge_type: 448.52 loss_i_size: 501.72 loss_j_size: 548.21\n",
      "Loss: 484.11 | loss_i: 245.58 loss_j: 516.97 loss_edge_type: 557.74 loss_i_size: 523.76 loss_j_size: 576.48\n",
      "Loss: 387.08 | loss_i: 225.64 loss_j: 371.59 loss_edge_type: 442.03 loss_i_size: 427.08 loss_j_size: 469.07\n",
      "Loss: 368.73 | loss_i: 167.41 loss_j: 311.04 loss_edge_type: 471.39 loss_i_size: 407.00 loss_j_size: 486.79\n",
      "Loss: 484.99 | loss_i: 346.20 loss_j: 670.75 loss_edge_type: 500.79 loss_i_size: 426.35 loss_j_size: 480.87\n",
      "Loss: 377.88 | loss_i: 196.86 loss_j: 369.68 loss_edge_type: 417.13 loss_i_size: 423.69 loss_j_size: 482.05\n",
      "Loss: 491.39 | loss_i: 341.00 loss_j: 600.64 loss_edge_type: 515.87 loss_i_size: 470.06 loss_j_size: 529.36\n",
      "Loss: 418.22 | loss_i: 285.78 loss_j: 508.10 loss_edge_type: 471.90 loss_i_size: 387.62 loss_j_size: 437.71\n",
      "Loss: 393.97 | loss_i: 214.07 loss_j: 393.61 loss_edge_type: 444.75 loss_i_size: 432.34 loss_j_size: 485.07\n",
      "Loss: 417.87 | loss_i: 241.20 loss_j: 415.53 loss_edge_type: 475.25 loss_i_size: 454.09 loss_j_size: 503.25\n",
      "Loss: 340.17 | loss_i: 191.01 loss_j: 355.71 loss_edge_type: 423.08 loss_i_size: 340.43 loss_j_size: 390.59\n",
      "Loss: 467.09 | loss_i: 305.30 loss_j: 644.79 loss_edge_type: 535.42 loss_i_size: 390.01 loss_j_size: 459.96\n",
      "Loss: 362.97 | loss_i: 199.81 loss_j: 361.71 loss_edge_type: 413.36 loss_i_size: 387.64 loss_j_size: 452.33\n",
      "Loss: 548.31 | loss_i: 394.36 loss_j: 796.46 loss_edge_type: 553.35 loss_i_size: 464.99 loss_j_size: 532.39\n",
      "Loss: 345.23 | loss_i: 225.27 loss_j: 458.49 loss_edge_type: 384.06 loss_i_size: 307.90 loss_j_size: 350.39\n",
      "Loss: 413.27 | loss_i: 260.28 loss_j: 504.56 loss_edge_type: 457.80 loss_i_size: 387.44 loss_j_size: 456.26\n",
      "Loss: 383.59 | loss_i: 196.57 loss_j: 362.58 loss_edge_type: 433.53 loss_i_size: 437.87 loss_j_size: 487.40\n",
      "Loss: 398.71 | loss_i: 186.95 loss_j: 326.82 loss_edge_type: 442.56 loss_i_size: 487.62 loss_j_size: 549.62\n",
      "Loss: 348.13 | loss_i: 162.91 loss_j: 358.42 loss_edge_type: 420.64 loss_i_size: 371.99 loss_j_size: 426.70\n",
      "Loss: 425.82 | loss_i: 221.92 loss_j: 487.23 loss_edge_type: 452.56 loss_i_size: 452.50 loss_j_size: 514.87\n",
      "Loss: 389.08 | loss_i: 202.20 loss_j: 368.66 loss_edge_type: 471.77 loss_i_size: 427.03 loss_j_size: 475.76\n",
      "Loss: 409.26 | loss_i: 230.29 loss_j: 488.13 loss_edge_type: 495.88 loss_i_size: 392.02 loss_j_size: 439.96\n",
      "Loss: 428.98 | loss_i: 286.58 loss_j: 465.78 loss_edge_type: 494.84 loss_i_size: 418.03 loss_j_size: 479.67\n",
      "Loss: 381.25 | loss_i: 241.47 loss_j: 449.70 loss_edge_type: 418.84 loss_i_size: 377.64 loss_j_size: 418.57\n",
      "Loss: 442.20 | loss_i: 235.92 loss_j: 464.12 loss_edge_type: 484.43 loss_i_size: 478.08 loss_j_size: 548.43\n",
      "Loss: 418.59 | loss_i: 231.79 loss_j: 466.55 loss_edge_type: 452.50 loss_i_size: 465.46 loss_j_size: 476.64\n",
      "Loss: 392.79 | loss_i: 190.94 loss_j: 370.02 loss_edge_type: 426.37 loss_i_size: 469.60 loss_j_size: 507.01\n",
      "Loss: 460.25 | loss_i: 257.96 loss_j: 544.95 loss_edge_type: 520.07 loss_i_size: 474.70 loss_j_size: 503.60\n",
      "Loss: 382.86 | loss_i: 240.26 loss_j: 461.84 loss_edge_type: 435.16 loss_i_size: 376.96 loss_j_size: 400.08\n",
      "Loss: 411.03 | loss_i: 234.14 loss_j: 484.74 loss_edge_type: 452.75 loss_i_size: 416.28 loss_j_size: 467.24\n",
      "Loss: 402.73 | loss_i: 210.96 loss_j: 421.07 loss_edge_type: 523.07 loss_i_size: 411.58 loss_j_size: 446.96\n",
      "Loss: 400.29 | loss_i: 215.16 loss_j: 354.90 loss_edge_type: 463.24 loss_i_size: 441.68 loss_j_size: 526.47\n",
      "Loss: 398.43 | loss_i: 166.98 loss_j: 388.15 loss_edge_type: 452.74 loss_i_size: 464.94 loss_j_size: 519.33\n",
      "Loss: 427.48 | loss_i: 205.81 loss_j: 373.17 loss_edge_type: 483.16 loss_i_size: 504.60 loss_j_size: 570.66\n",
      "Loss: 488.94 | loss_i: 277.66 loss_j: 505.59 loss_edge_type: 526.07 loss_i_size: 548.29 loss_j_size: 587.10\n",
      "Loss: 367.94 | loss_i: 170.79 loss_j: 290.80 loss_edge_type: 446.84 loss_i_size: 444.17 loss_j_size: 487.09\n",
      "Loss: 444.59 | loss_i: 248.78 loss_j: 540.35 loss_edge_type: 502.45 loss_i_size: 435.54 loss_j_size: 495.85\n",
      "Loss: 417.91 | loss_i: 248.77 loss_j: 503.89 loss_edge_type: 489.00 loss_i_size: 389.47 loss_j_size: 458.45\n",
      "Loss: 400.25 | loss_i: 233.00 loss_j: 451.37 loss_edge_type: 459.40 loss_i_size: 406.09 loss_j_size: 451.37\n",
      "Loss: 31.61 | loss_i: 19.29 loss_j: 33.79 loss_edge_type: 39.28 loss_i_size: 33.19 loss_j_size: 32.48\n",
      "Validation Loss: 359.76 | loss_i: 228.86 loss_j: 393.88 loss_edge_type: 407.75 loss_i_size: 372.70 loss_j_size: 395.63\n",
      "Validation Loss: 419.81 | loss_i: 274.86 loss_j: 542.91 loss_edge_type: 497.67 loss_i_size: 368.91 loss_j_size: 414.68\n",
      "Validation Loss: 269.70 | loss_i: 122.28 loss_j: 222.84 loss_edge_type: 327.41 loss_i_size: 326.72 loss_j_size: 349.27\n",
      "Validation Loss: 406.01 | loss_i: 286.85 loss_j: 507.32 loss_edge_type: 425.67 loss_i_size: 385.03 loss_j_size: 425.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:30<00:00, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 568.88 | loss_i: 447.95 loss_j: 818.44 loss_edge_type: 528.10 loss_i_size: 500.79 loss_j_size: 549.13\n",
      "Validation Loss: 232.29 | loss_i: 111.37 loss_j: 256.36 loss_edge_type: 304.44 loss_i_size: 237.38 loss_j_size: 251.90\n",
      "Training loss: 408.6111\n",
      "Validation loss: 376.0758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "N_LAYER = 4\n",
    "N_HEAD = 4\n",
    "D_MODEL = 256\n",
    "D_FEEDFW = 1024\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = AutoregressiveTransformer(n_token).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model, optimizer, loss_fn, dataloader_train, dataloader_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c8f05a5-8fd4-4984-86c6-b970de3812c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ad98492e-5e2b-4cb4-832c-066b035755e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy and loss\n",
    "plt.figure()\n",
    "plt.plot(train_loss_list, label='Train loss')\n",
    "plt.plot(validation_loss_list, label='Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e0ea71ab-604f-461d-aad7-3e6d3af75667",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoregressiveTransformer(\n",
       "  (emb_i): Embeddings(\n",
       "    (lut): Embedding(43, 128, padding_idx=0)\n",
       "  )\n",
       "  (emb_j): Embeddings(\n",
       "    (lut): Embedding(43, 128, padding_idx=0)\n",
       "  )\n",
       "  (emb_edge_type): Embeddings(\n",
       "    (lut): Embedding(10, 12, padding_idx=0)\n",
       "  )\n",
       "  (emb_i_size): Embeddings(\n",
       "    (lut): Embedding(12, 16, padding_idx=0)\n",
       "  )\n",
       "  (emb_j_size): Embeddings(\n",
       "    (lut): Embedding(12, 16, padding_idx=0)\n",
       "  )\n",
       "  (pos_emb): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (in_linear): Linear(in_features=300, out_features=256, bias=True)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_i): Linear(in_features=256, out_features=43, bias=True)\n",
       "  (proj_j): Linear(in_features=256, out_features=43, bias=True)\n",
       "  (proj_edge_type): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (proj_i_size): Linear(in_features=256, out_features=12, bias=True)\n",
       "  (proj_j_size): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed5b73-dfbd-4a94-9ece-f619b3bc6d2a",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b8cf6352-cf83-4f49-9ee5-dc514e40b79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def weighted_sampling(probs):\n",
    "    probs /= sum(probs)\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "    sorted_index = np.argsort(probs)[::-1]\n",
    "    word = np.random.choice(sorted_index, size=1, p=sorted_probs)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "# -- nucleus -- #\n",
    "def nucleus(probs, p):\n",
    "    probs /= (sum(probs) + 1e-5)\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "    sorted_index = np.argsort(probs)[::-1]\n",
    "    cusum_sorted_probs = np.cumsum(sorted_probs)\n",
    "    after_threshold = cusum_sorted_probs > p\n",
    "    if sum(after_threshold) > 0:\n",
    "        last_index = np.where(after_threshold)[0][0] + 1\n",
    "        candi_index = sorted_index[:last_index]\n",
    "    else:\n",
    "        candi_index = sorted_index[:]\n",
    "    candi_probs = [probs[i] for i in candi_index]\n",
    "    candi_probs /= sum(candi_probs)\n",
    "    word = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "def sampling(logit, p=None, t=1.0):\n",
    "    logit = logit.squeeze().cpu().numpy()\n",
    "    probs = softmax_with_temperature(logits=logit, temperature=t)\n",
    "    \n",
    "    if p is not None:\n",
    "        cur_word = nucleus(probs, p=p)\n",
    "    else:\n",
    "        cur_word = weighted_sampling(probs)\n",
    "    return cur_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "691f23fb-9e12-4c0b-9126-dfdb3c4d3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Temperature\n",
    "\n",
    "def predict(model, input_sequence, max_length=150):\n",
    "    model.eval()\n",
    "    \n",
    "    SOS_token = (40, 40, 8, 10, 10)\n",
    "    EOS_token = (41, 41, 9, 11, 11)\n",
    "            \n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Get source mask\n",
    "            tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "\n",
    "            y_i, y_j, y_edge_type, y_i_size, y_j_size = model(input_sequence, y_input, tgt_mask)\n",
    "\n",
    "            # # sampling gen_cond\n",
    "            # print(y_i.shape)\n",
    "            # print(y_i[-1])\n",
    "            curr_i = sampling(y_i[-1], t=0.5)\n",
    "            cur_j = sampling(y_j[-1], t=0.5)\n",
    "            cur_edge_type = sampling(y_edge_type[-1], t=2, p=0.8)\n",
    "            cur_i_size = sampling(y_i_size[-1], t=0.25)\n",
    "            cur_j_size = sampling(y_j_size[-1], t=0.25)\n",
    "\n",
    "            # pred = []\n",
    "            # for token in pred_tokens:\n",
    "            #     next_item = token.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "            #     pred.append(next_item)\n",
    "\n",
    "            next_item = torch.tensor([[(curr_i, cur_j, cur_edge_type, cur_i_size, cur_j_size)]], device=device)\n",
    "            # print(next_item)\n",
    "\n",
    "            # Concatenate previous input with predicted best word\n",
    "            y_input = torch.cat((y_input, next_item), dim=1)\n",
    "            # print(y_input)\n",
    "\n",
    "            # Stop if model predicts end of sentence\n",
    "            # print(next_item.view(-1))\n",
    "            # print(torch.tensor(EOS_token))\n",
    "            if curr_i == torch.tensor(EOS_token[0]):\n",
    "                break\n",
    "    return y_input\n",
    "    # return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a3dcd2b5-fdef-4e09-9e74-1f08dea159cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: tensor([[[40, 40,  8, 10, 10]]], device='cuda:0')\n",
      "Continuation: tensor([[[40, 40,  8, 10, 10],\n",
      "         [ 1,  2,  1,  4,  4],\n",
      "         [ 2,  9,  3,  4,  4],\n",
      "         [ 3,  4,  6,  4,  4],\n",
      "         [ 3,  5,  3,  4,  4],\n",
      "         [ 4,  5,  5,  4,  4],\n",
      "         [ 4,  9,  3,  4,  4],\n",
      "         [ 4, 10,  3,  4,  4],\n",
      "         [ 4, 11,  3,  4,  4],\n",
      "         [ 5,  6,  3,  4,  4],\n",
      "         [ 5,  9,  3,  4,  4],\n",
      "         [ 5, 10,  3,  4,  4],\n",
      "         [ 5,  9,  3,  4,  4],\n",
      "         [ 5, 10,  4,  4,  4],\n",
      "         [ 5, 11,  3,  4,  4],\n",
      "         [ 5,  7,  3,  4,  4],\n",
      "         [ 5,  9,  3,  4,  4],\n",
      "         [ 5, 10,  3,  4,  4],\n",
      "         [ 5,  7,  3,  4,  4],\n",
      "         [ 5,  9,  6,  4,  4],\n",
      "         [ 5, 10,  3,  4,  4],\n",
      "         [ 5, 15,  3,  4,  4],\n",
      "         [ 6,  7,  3,  4,  4],\n",
      "         [ 7,  9,  3,  4,  4],\n",
      "         [ 7,  9,  3,  4,  4],\n",
      "         [ 8,  9,  3,  4,  4],\n",
      "         [ 8, 10,  3,  4,  4],\n",
      "         [ 8, 10,  4,  4,  4],\n",
      "         [ 9,  9,  6,  4,  4],\n",
      "         [ 9, 10,  3,  4,  4],\n",
      "         [ 9, 11,  4,  4,  4],\n",
      "         [ 9, 21,  3,  4,  4],\n",
      "         [ 9, 11,  4,  4,  4],\n",
      "         [10, 15,  6,  4,  4],\n",
      "         [41, 19,  3,  4,  4]]], device='cuda:0')\n",
      "Example 1\n",
      "Input: tensor([[[40, 40,  8, 10, 10],\n",
      "         [ 1,  2,  1,  2,  4],\n",
      "         [ 2,  3,  2,  4,  4]]], device='cuda:0')\n",
      "Continuation: tensor([[[40, 40,  8, 10, 10],\n",
      "         [ 1,  2,  1,  4,  4],\n",
      "         [ 2,  3,  4,  4,  4],\n",
      "         [ 2,  4,  3,  4,  4],\n",
      "         [ 2,  4,  3,  4,  4],\n",
      "         [ 2,  4,  5,  4,  4],\n",
      "         [ 2,  4,  3,  4,  4],\n",
      "         [ 3,  4,  3,  4,  4],\n",
      "         [ 4,  5,  3,  4,  4],\n",
      "         [ 4,  9,  3,  4,  4],\n",
      "         [ 5,  6,  3,  4,  4],\n",
      "         [ 5, 10,  3,  4,  4],\n",
      "         [ 5, 11,  3,  4,  4],\n",
      "         [ 6,  7,  5,  4,  4],\n",
      "         [ 7,  8,  3,  4,  4],\n",
      "         [ 8,  9,  3,  4,  4],\n",
      "         [ 8, 10,  4,  4,  4],\n",
      "         [ 9, 10,  3,  4,  4],\n",
      "         [10, 11,  4,  4,  4],\n",
      "         [11, 12,  6,  4,  4],\n",
      "         [12, 13,  3,  4,  4],\n",
      "         [13, 41,  4,  4,  4],\n",
      "         [41, 41,  9, 11, 11]]], device='cuda:0')\n",
      "Example 2\n",
      "Input: tensor([[[40, 40,  8, 10, 10],\n",
      "         [ 1,  2,  1,  8,  7],\n",
      "         [ 2,  3,  4,  7,  8]]], device='cuda:0')\n",
      "Continuation: tensor([[[40, 40,  8, 10, 10],\n",
      "         [ 1,  2,  1,  4,  4],\n",
      "         [ 2,  3,  3,  4,  4],\n",
      "         [ 2,  4,  3,  4,  4],\n",
      "         [ 3,  4,  3,  4,  4],\n",
      "         [ 4,  5,  3,  4,  4],\n",
      "         [ 4,  9,  3,  4,  4],\n",
      "         [ 5,  6,  4,  4,  8],\n",
      "         [ 5,  8,  3,  9,  4],\n",
      "         [ 5,  9,  4,  4,  8],\n",
      "         [ 6,  7,  3,  8,  4],\n",
      "         [ 7,  8,  3,  4,  8],\n",
      "         [ 8,  9,  4,  8,  8],\n",
      "         [ 8, 10,  3,  8,  8],\n",
      "         [ 9, 11,  4,  8,  8],\n",
      "         [ 9, 11,  6,  8,  8],\n",
      "         [10, 11,  6,  8,  8],\n",
      "         [11, 12,  4,  8,  2],\n",
      "         [41, 41,  2, 11, 11]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([[(40, 40, 8, 10, 10)]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[(40, 40, 8, 10, 10), (1, 2, 1, 2, 4), (2, 3, 2, 4, 4)]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[(40, 40, 8, 10, 10), (1, 2, 1, 8, 7), (2, 3, 4, 7, 8)]], dtype=torch.long, device=device),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example}\")\n",
    "    print(f\"Continuation: {result}\")\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419bdd0-2a71-4c46-ae05-f9ff0f8f1496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmugen",
   "language": "python",
   "name": "graphmugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
