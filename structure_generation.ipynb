{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31060737-6028-46dc-aa2a-133659cfbdae",
   "metadata": {},
   "source": [
    "# Train a model to generate structure of a piece (MELONS-inspired)\n",
    "\n",
    "1. Read structure dataset from POP909_structure\n",
    "2. Pre-process str into graph format\n",
    "3. Setup transformer model\n",
    "4. Train-val split, data loader\n",
    "5. Evaluate model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b181400b-6fb3-463b-966d-9ea578a735f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf9ab62-b7d7-4a94-9fdb-1d2c3213741b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_path = \"POP909_structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d754b377-807f-495b-9476-c15a0ee36cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for folder in os.listdir(structure_path):\n",
    "    try:\n",
    "        f = open(f\"{structure_path}/{folder}/human_label1.txt\", \"r\")\n",
    "        # print(f.read())\n",
    "        labels.append(f.read())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e56d4ea-6cce-430c-9881-7191eb709955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8A8A8B8C4C4b4b4x2A8B8C4C4C4C4X1o1\\n',\n",
       " 'i4A4A4B4B4C4C4C4D4x4B4B4C4C4C4D4X3\\n',\n",
       " 'i4A8A8B4C4b5x1b5A8B4x1C4C4\\n',\n",
       " 'i18A8A8A9x14B8A10B8A10o4\\n',\n",
       " 'X4b4A8B12b4A8B12b4b4B12o4\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af654142-478d-4553-b81f-80db89181124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_string(s):\n",
    "    # This regex pattern matches a letter followed by one or more digits\n",
    "    pattern = re.compile(r'[a-zA-Z]\\d+')\n",
    "    # Find all matches in the string\n",
    "    matches = pattern.findall(s)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4269dc3a-ae35-4536-a820-94429e7e0598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i18', 'A8', 'A8', 'A9', 'x14', 'B8', 'A10', 'B8', 'A10', 'o4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_string(labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff696da-721c-412e-83d6-a33abe2dac0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def split_into_pairs(input_string):\n",
    "#     # Initialize an empty list to hold the pairs\n",
    "#     pairs = []\n",
    "    \n",
    "#     # Iterate over the string in steps of 2\n",
    "#     for i in range(0, len(input_string), 2):\n",
    "#         next_pair = input_string[i:i+2]\n",
    "#         if next_pair != \"\\n\":\n",
    "#             # Append the substring of the next two characters to the list\n",
    "#             pairs.append(next_pair)\n",
    "    \n",
    "#     return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082857cf-4f2f-4752-9277-63be87d37d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split_into_pairs(\"i4A4A4B9b4A4B9b4B9X5o2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f9ed7a-de88-430d-9841-bc9424cd349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_phrases = []\n",
    "for label in labels:\n",
    "    all_phrases.append(split_string(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ef2418-1d51-47d2-8f52-c38c1e83798f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i8',\n",
       "  'A8',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'b4',\n",
       "  'b4',\n",
       "  'x2',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'X1',\n",
       "  'o1'],\n",
       " ['i4',\n",
       "  'A4',\n",
       "  'A4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'x4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'X3'],\n",
       " ['i4',\n",
       "  'A8',\n",
       "  'A8',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'b5',\n",
       "  'x1',\n",
       "  'b5',\n",
       "  'A8',\n",
       "  'B4',\n",
       "  'x1',\n",
       "  'C4',\n",
       "  'C4']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe7daf0-5a56-4074-8520-567520056d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_phrase_edge_type(prev_phrase, curr_phrase, prev_phrase_idx, curr_phrase_idx):\n",
    "    \"\"\"\n",
    "    Edge types:\n",
    "    0: Intro to Any\n",
    "    1: Any to Outro\n",
    "    2: Repeated phrase\n",
    "    3: Melody to Melody\n",
    "    4: Melody to Non-Melody\n",
    "    5: Non-Melody to Melody\n",
    "    6: Non-Melody to Non-Melody\n",
    "    \"\"\"\n",
    "    # print(prev_phrase_idx, curr_phrase_idx)\n",
    "    \n",
    "    prev_phrase_type = prev_phrase[0]\n",
    "    curr_phrase_type = curr_phrase[0]\n",
    "    \n",
    "    if prev_phrase == curr_phrase:\n",
    "            return 2\n",
    "    \n",
    "    if prev_phrase_idx + 1 == curr_phrase_idx:\n",
    "        # print(prev_phrase_type)\n",
    "    \n",
    "        if prev_phrase_type == \"i\":\n",
    "            return 0\n",
    "        elif curr_phrase_type == \"o\":\n",
    "            return 1\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.isupper():\n",
    "            return 3\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.islower():\n",
    "            return 4\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.isupper():\n",
    "            return 5\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.islower():\n",
    "            return 6\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "952d7a6c-f667-43f0-a2df-a32539748a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_phrase_edge_type(\"B4\", \"B9\", 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad01669-06ee-49b9-a0dc-9e563baa9618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8',\n",
       " 'A8',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'b4',\n",
       " 'b4',\n",
       " 'x2',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'X1',\n",
       " 'o1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf67c9-a9c3-4a07-a779-660fa860d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get max size of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f6e135-ceef-4b6b-b9cc-696341e3ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(phrases):\n",
    "    # Create sequence of edges from phrase, where each item is a tuple (i, j, edge type, num bars in i, num bars in j)\n",
    "    seq = []\n",
    "    max_phrase_len = 0\n",
    "    for i, phrase_from in enumerate(phrases):\n",
    "        for j, phrase_to in enumerate(phrases[i+1:]):\n",
    "            phrase_to_idx = j+i+1\n",
    "            edge_type = get_phrase_edge_type(phrase_from, phrase_to, i, phrase_to_idx)\n",
    "            if edge_type is not None:\n",
    "                phrase_from_len = int(phrase_from[1])\n",
    "                phrase_to_len = int(phrase_to[1])\n",
    "                max_phrase_len = max(max_phrase_len, max(phrase_from_len, phrase_to_len))\n",
    "                seq.append((i, phrase_to_idx, edge_type, phrase_from_len, phrase_to_len))\n",
    "    \n",
    "    # Append END token\n",
    "    seq.append((len(phrases)-1, len(phrases), 7, 0, 0))\n",
    "    return seq, max_phrase_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27d96fc-78de-4fd5-a714-477437e814a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "max_phrase_len = 0\n",
    "max_num_nodes = 0\n",
    "for phrases in all_phrases:\n",
    "    # print(phrases)\n",
    "    num_nodes = len(phrases)\n",
    "    seq, max_phrase_len_indiv = create_sequence(phrases)\n",
    "    seqs.append(seq)\n",
    "    max_phrase_len = max(max_phrase_len, max_phrase_len_indiv)\n",
    "    max_num_nodes = max(max_num_nodes, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08eda1d0-6b73-417b-83cb-d53f4eec6e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 8, 8),\n",
       " (1, 2, 2, 8, 8),\n",
       " (1, 9, 2, 8, 8),\n",
       " (2, 3, 3, 8, 8),\n",
       " (2, 9, 2, 8, 8),\n",
       " (3, 4, 3, 8, 4),\n",
       " (3, 10, 2, 8, 8),\n",
       " (4, 5, 2, 4, 4),\n",
       " (4, 11, 2, 4, 4),\n",
       " (4, 12, 2, 4, 4),\n",
       " (4, 13, 2, 4, 4),\n",
       " (4, 14, 2, 4, 4),\n",
       " (5, 6, 4, 4, 4),\n",
       " (5, 11, 2, 4, 4),\n",
       " (5, 12, 2, 4, 4),\n",
       " (5, 13, 2, 4, 4),\n",
       " (5, 14, 2, 4, 4),\n",
       " (6, 7, 2, 4, 4),\n",
       " (7, 8, 6, 4, 2),\n",
       " (8, 9, 5, 2, 8),\n",
       " (9, 10, 3, 8, 8),\n",
       " (10, 11, 3, 8, 4),\n",
       " (11, 12, 2, 4, 4),\n",
       " (11, 13, 2, 4, 4),\n",
       " (11, 14, 2, 4, 4),\n",
       " (12, 13, 2, 4, 4),\n",
       " (12, 14, 2, 4, 4),\n",
       " (13, 14, 2, 4, 4),\n",
       " (14, 15, 3, 4, 1),\n",
       " (15, 16, 1, 1, 1),\n",
       " (16, 17, 7, 0, 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ede387e-42da-4182-975f-5f7ca62ade74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_phrase_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d79d2b30-a39d-4423-a0e0-d50c03f6161b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c0b370f-1c45-43ef-a8ac-69dcd642bfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 5, 4),\n",
       " (1, 2, 3, 4, 5),\n",
       " (1, 6, 2, 4, 4),\n",
       " (2, 3, 3, 5, 3),\n",
       " (2, 7, 2, 5, 5),\n",
       " (3, 4, 3, 3, 9),\n",
       " (3, 8, 2, 3, 3),\n",
       " (4, 5, 4, 9, 3),\n",
       " (4, 9, 2, 9, 9),\n",
       " (4, 11, 2, 9, 9),\n",
       " (5, 6, 5, 3, 4),\n",
       " (6, 7, 3, 4, 5),\n",
       " (7, 8, 3, 5, 3),\n",
       " (8, 9, 3, 3, 9),\n",
       " (9, 10, 3, 9, 3),\n",
       " (9, 11, 2, 9, 9),\n",
       " (10, 11, 3, 3, 9),\n",
       " (11, 12, 3, 9, 6),\n",
       " (12, 13, 1, 6, 1),\n",
       " (13, 14, 7, 0, 0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f52e5a6-6b3c-427d-a8c9-ce95cc2cca8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_input_output_pairs(seq):\n",
    "    # Create input and output pairs\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "    for idx in range(1, len(seq)):\n",
    "        input_seqs.append(seq[:idx])\n",
    "        output_seqs.append(seq[idx:])\n",
    "    return input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced290f2-dd22-4c1d-aacf-2cbcf6859837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for seq in seqs:\n",
    "    input_seqs, output_seqs = create_input_output_pairs(seq)\n",
    "    inputs.append(input_seqs)\n",
    "    outputs.append(output_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb06f913-0b29-4af9-ad75-e0ecc42576dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 8, 8), (1, 2, 2, 8, 8), (1, 9, 2, 8, 8), (2, 3, 3, 8, 8)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "679d6ff9-0367-4cac-a44f-a073730ca171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 9, 2, 8, 8),\n",
       " (3, 4, 3, 8, 4),\n",
       " (3, 10, 2, 8, 8),\n",
       " (4, 5, 2, 4, 4),\n",
       " (4, 11, 2, 4, 4),\n",
       " (4, 12, 2, 4, 4),\n",
       " (4, 13, 2, 4, 4),\n",
       " (4, 14, 2, 4, 4),\n",
       " (5, 6, 4, 4, 4),\n",
       " (5, 11, 2, 4, 4),\n",
       " (5, 12, 2, 4, 4),\n",
       " (5, 13, 2, 4, 4),\n",
       " (5, 14, 2, 4, 4),\n",
       " (6, 7, 2, 4, 4),\n",
       " (7, 8, 6, 4, 2),\n",
       " (8, 9, 5, 2, 8),\n",
       " (9, 10, 3, 8, 8),\n",
       " (10, 11, 3, 8, 4),\n",
       " (11, 12, 2, 4, 4),\n",
       " (11, 13, 2, 4, 4),\n",
       " (11, 14, 2, 4, 4),\n",
       " (12, 13, 2, 4, 4),\n",
       " (12, 14, 2, 4, 4),\n",
       " (13, 14, 2, 4, 4),\n",
       " (14, 15, 3, 4, 1),\n",
       " (15, 16, 1, 1, 1),\n",
       " (16, 17, 7, 0, 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87603922-e6a1-4a8e-9fe4-a0741963bc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_flat = [seq for seqs in inputs for seq in seqs]\n",
    "outputs_flat = [seq for seqs in outputs for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7980d661-dd99-4ff3-9340-8fedf098e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19982"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ba826d-6e76-44cb-96a1-fa230dd4e738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in inputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc2405c8-9a64-454e-9c5e-5ff62f42ca70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19982"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "811ba1ac-ad2e-4b39-a50c-853b38465a66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in outputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e0752f6-0d88-4cc9-8416-ec14bc5deb21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([token for tokens in inputs_flat for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c23f50c-543e-43df-bf0a-a60f114989b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2897"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([token for tokens in outputs_flat for token in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150ab47-96b6-42b7-97a0-1c42e4a0eec5",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5849d65-260e-407d-afc6-407dd399785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def sequences_to_tensor(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Convert a list of sequences of different lengths to a padded tensor.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of list of tuples): List of sequences where each sequence is a list of tuples.\n",
    "        padding_value (int, optional): Value to use for padding. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_length, tuple_length)\n",
    "    \"\"\"\n",
    "    # Convert each sequence to a tensor\n",
    "    tensor_sequences = [torch.tensor(seq) for seq in sequences]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f74f9544-f310-4ec0-ab5a-fb21e63eb179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input shape: torch.Size([19982, 150, 5])\n",
      "Padded output shape: torch.Size([19982, 150, 5])\n"
     ]
    }
   ],
   "source": [
    "padded_input = sequences_to_tensor(inputs_flat, padding_value=0)\n",
    "padded_output = sequences_to_tensor(outputs_flat, padding_value=0)\n",
    "\n",
    "print(\"Padded input shape:\", padded_input.shape)\n",
    "print(\"Padded output shape:\", padded_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7997c132-f8d8-4e38-9f86-cb1f029f1319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 8, 8),\n",
       " (1, 2, 2, 8, 8),\n",
       " (1, 9, 2, 8, 8),\n",
       " (2, 3, 3, 8, 8),\n",
       " (2, 9, 2, 8, 8),\n",
       " (3, 4, 3, 8, 4),\n",
       " (3, 10, 2, 8, 8),\n",
       " (4, 5, 2, 4, 4),\n",
       " (4, 11, 2, 4, 4),\n",
       " (4, 12, 2, 4, 4),\n",
       " (4, 13, 2, 4, 4),\n",
       " (4, 14, 2, 4, 4),\n",
       " (5, 6, 4, 4, 4),\n",
       " (5, 11, 2, 4, 4),\n",
       " (5, 12, 2, 4, 4),\n",
       " (5, 13, 2, 4, 4),\n",
       " (5, 14, 2, 4, 4),\n",
       " (6, 7, 2, 4, 4),\n",
       " (7, 8, 6, 4, 2),\n",
       " (8, 9, 5, 2, 8),\n",
       " (9, 10, 3, 8, 8),\n",
       " (10, 11, 3, 8, 4),\n",
       " (11, 12, 2, 4, 4),\n",
       " (11, 13, 2, 4, 4),\n",
       " (11, 14, 2, 4, 4),\n",
       " (12, 13, 2, 4, 4),\n",
       " (12, 14, 2, 4, 4),\n",
       " (13, 14, 2, 4, 4),\n",
       " (14, 15, 3, 4, 1),\n",
       " (15, 16, 1, 1, 1),\n",
       " (16, 17, 7, 0, 0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8ad429b-2c99-4f3a-a5ae-66a26e0cb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "909"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a881149-7308-4791-83b1-07b5803370e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded seq shape: torch.Size([909, 151, 5])\n"
     ]
    }
   ],
   "source": [
    "padded_seq = sequences_to_tensor(seqs, padding_value=0)\n",
    "print(\"Padded seq shape:\", padded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "30a435a3-e7a5-45c3-94f2-a8f1eee2df88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77c686f5-f192-4832-ba0f-8bb75c267828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into Train and Test sets of size 818 and 91 respectively.\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_test = round(len(seqs) * test_ratio)\n",
    "train_split, test_split = random_split(padded_seq, [len(seqs)-num_test, num_test])\n",
    "print(f\"Split data into Train and Test sets of size {len(train_split)} and {len(test_split)} respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79cff4a2-fc97-4370-a33c-028224190555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the custom dataset\n",
    "class TupleSequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, output_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.input_sequences[idx], self.output_sequences[idx]]\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "# Create the dataset\n",
    "dataset_train = TupleSequenceDataset(train_split, train_split)\n",
    "dataset_test = TupleSequenceDataset(test_split, test_split)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fbe783fb-8daf-46b4-9d34-0bba2b75f5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_token = [max_num_nodes, max_num_nodes, 7, max_phrase_len, max_phrase_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7a446fdd-f302-4285-8d43-af446268bec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 39, 7, 9, 9]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd777-1572-428c-a42d-0968bc2d287d",
   "metadata": {},
   "source": [
    "## Autoregression transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "678c3f45-6f55-420b-aae3-ff12a76a8e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "576dc225-92c2-4b10-bc10-628ecbe7868b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/YatingMusic/compound-word-transformer/blob/main/workspace/uncond/cp-linear/main-cp.py\n",
    "# https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "# https://gist.github.com/danimelchor/bcad4d7f79b98464c4d4481d62d27622\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Get embeddings for edge tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, n_token, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # print(n_token)\n",
    "        self.lut = nn.Embedding(n_token+1, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(n_token)\n",
    "        # print(x.shape)\n",
    "        # print(self.d_model)\n",
    "        # print(self.lut(x))\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Get positional encodings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=20000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    def __init__(self, n_token):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "        \n",
    "        # --- params config --- #\n",
    "        self.n_token = n_token   \n",
    "        self.d_model = D_MODEL \n",
    "        self.d_feedforward = D_FEEDFW\n",
    "        self.n_layer = N_LAYER #\n",
    "        self.dropout = 0.1\n",
    "        self.n_head = N_HEAD\n",
    "        self.d_head = D_MODEL // N_HEAD\n",
    "        self.d_inner = 1024\n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.emb_sizes = [128, 128, 12, 16, 16]\n",
    "        \n",
    "        \n",
    "        # --- modules config --- #\n",
    "        # embeddings\n",
    "        print('>>>>>:', self.n_token)\n",
    "        self.emb_i = Embeddings(self.n_token[0], self.emb_sizes[0])\n",
    "        self.emb_j = Embeddings(self.n_token[1], self.emb_sizes[1])\n",
    "        self.emb_edge_type = Embeddings(self.n_token[2], self.emb_sizes[2])\n",
    "        self.emb_i_size = Embeddings(self.n_token[3], self.emb_sizes[3])\n",
    "        self.emb_j_size = Embeddings(self.n_token[4], self.emb_sizes[4])\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout)\n",
    "\n",
    "        # linear \n",
    "        self.in_linear = nn.Linear(np.sum(self.emb_sizes), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.n_head,\n",
    "            num_encoder_layers=self.n_layer,\n",
    "            num_decoder_layers=self.n_layer,\n",
    "            dim_feedforward=self.d_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        # individual output\n",
    "        self.proj_i    = nn.Linear(self.d_model, self.n_token[0])        \n",
    "        self.proj_j    = nn.Linear(self.d_model, self.n_token[1])\n",
    "        self.proj_edge_type  = nn.Linear(self.d_model, self.n_token[2])\n",
    "        self.proj_i_size     = nn.Linear(self.d_model, self.n_token[3])\n",
    "        self.proj_j_size    = nn.Linear(self.d_model, self.n_token[4])\n",
    "        \n",
    "    def compute_loss(self, predict, target, loss_mask):\n",
    "        loss = self.loss_func(predict, target)\n",
    "        loss = loss * loss_mask\n",
    "        loss = torch.sum(loss) / torch.sum(loss_mask)\n",
    "        return loss\n",
    "\n",
    "#     def train_step(self, x, target, loss_mask):\n",
    "#         h, y_i, y_j, y_edge_type, y_i_size, y_j_size = self.forward(x)\n",
    "         \n",
    "#         # reshape (b, s, f) -> (b, f, s)\n",
    "#         y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "#         y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "#         y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "#         y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "#         y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "        \n",
    "#         # loss\n",
    "#         loss_i = self.compute_loss(\n",
    "#                 y_i, target[..., 0], loss_mask)\n",
    "#         loss_j = self.compute_loss(\n",
    "#                 y_j, target[..., 1], loss_mask)\n",
    "#         loss_edge_type = self.compute_loss(\n",
    "#                 y_edge_type, target[..., 2], loss_mask)\n",
    "#         loss_i_size = self.compute_loss(\n",
    "#                 y_i_size,  target[..., 3], loss_mask)\n",
    "#         loss_j_size = self.compute_loss(\n",
    "#                 y_j_size, target[..., 4], loss_mask)\n",
    "\n",
    "#         return loss_i, loss_j, loss_edge_type, loss_i_size, loss_j_size\n",
    "    \n",
    "    \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        '''\n",
    "        linear transformer: b x s x f\n",
    "        x.shape=(bs, nf)\n",
    "        '''\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        # src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        # tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        # src = self.positional_encoder(src)\n",
    "        # tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        # # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # # to obtain size (sequence length, batch_size, dim_model),\n",
    "        # src = src.permute(1,0,2)\n",
    "        # tgt = tgt.permute(1,0,2)\n",
    "    \n",
    "        # src embeddings\n",
    "        emb_i_src =    self.emb_i(src[..., 0])\n",
    "        emb_j_src =    self.emb_j(src[..., 1])\n",
    "        emb_edge_type_src =  self.emb_edge_type(src[..., 2])\n",
    "        emb_i_size_src =     self.emb_i_size(src[..., 3])\n",
    "        emb_j_size_src =    self.emb_j_size(src[..., 4])\n",
    "\n",
    "        embs_src = torch.cat(\n",
    "            [\n",
    "                emb_i_src,\n",
    "                emb_j_src,\n",
    "                emb_edge_type_src,\n",
    "                emb_i_size_src,\n",
    "                emb_j_size_src,\n",
    "            ], dim=-1)\n",
    "\n",
    "        emb_linear_src = self.in_linear(embs_src)\n",
    "        pos_emb_src = self.pos_emb(emb_linear_src)\n",
    "        \n",
    "        \n",
    "        # tgt embeddings\n",
    "        emb_i_tgt =    self.emb_i(tgt[..., 0])\n",
    "        emb_j_tgt =    self.emb_j(tgt[..., 1])\n",
    "        emb_edge_type_tgt =  self.emb_edge_type(tgt[..., 2])\n",
    "        emb_i_size_tgt =     self.emb_i_size(tgt[..., 3])\n",
    "        emb_j_size_tgt =    self.emb_j_size(tgt[..., 4])\n",
    "\n",
    "        embs_tgt = torch.cat(\n",
    "            [\n",
    "                emb_i_tgt,\n",
    "                emb_j_tgt,\n",
    "                emb_edge_type_tgt,\n",
    "                emb_i_size_tgt,\n",
    "                emb_j_size_tgt,\n",
    "            ], dim=-1)\n",
    "\n",
    "        emb_linear_tgt = self.in_linear(embs_tgt)\n",
    "        pos_emb_tgt = self.pos_emb(emb_linear_tgt)\n",
    "        \n",
    "        # target embeddings\n",
    "    \n",
    "        # transformer\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        # print(pos_emb_src.shape)\n",
    "        # print(pos_emb_tgt.shape)\n",
    "        pos_emb_src = pos_emb_src.permute(1,0,2)\n",
    "        pos_emb_tgt = pos_emb_tgt.permute(1,0,2)\n",
    "        transformer_out = self.transformer(pos_emb_src, pos_emb_tgt, \n",
    "                                           tgt_mask=tgt_mask, \n",
    "                                           src_key_padding_mask=src_pad_mask, \n",
    "                                           tgt_key_padding_mask=tgt_pad_mask)\n",
    "        # out = self.out(transformer_out)\n",
    "        \n",
    "        \n",
    "#         if is_training:\n",
    "#             # mask\n",
    "#             attn_mask = TriangularCausalMask(pos_emb.size(1), device=x.device)\n",
    "#             h = self.transformer_encoder(pos_emb, attn_mask) # y: b x s x d_model\n",
    "\n",
    "#             # # project type\n",
    "#             # y_type = self.proj_type(h)\n",
    "#             # return h, y_type\n",
    "#         else:\n",
    "#             pos_emb = pos_emb.squeeze(0)\n",
    "#             h, memory = self.transformer_encoder(pos_emb, memory=memory) # y: s x d_model\n",
    "            \n",
    "#             # # project type\n",
    "#             # y_type = self.proj_type(h)\n",
    "#             # return h, y_type, memory\n",
    "\n",
    "        y_i    = self.proj_i(transformer_out)\n",
    "        y_j    = self.proj_j(transformer_out)\n",
    "        y_edge_type  = self.proj_edge_type(transformer_out)\n",
    "        y_i_size    = self.proj_i_size(transformer_out)\n",
    "        y_j_size = self.proj_j_size(transformer_out)\n",
    "\n",
    "        return  y_i, y_j, y_edge_type, y_i_size, y_j_size\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a square matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07f932-2723-4870-89f1-b963bfaf1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change input size to expand to vocab size?? I.e. if vocab size is 39, then it should be a 1-hot of 39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "16682939-e658-4fe2-aa32-03ec9f766eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_token[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7ba4b897-6858-4431-bc7e-37b565021219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(predict, target, loss_func, loss_mask):\n",
    "        predict = predict.permute(1, 2, 0)   \n",
    "        print(predict.shape)\n",
    "        print(target.shape)\n",
    "        loss = loss_func(predict, target)\n",
    "        loss = loss * loss_mask\n",
    "        loss = torch.sum(loss) / torch.sum(loss_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8e8f6bce-a686-4ec6-ab74-77e22874ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # X, y = batch[:, 0], batch[:, 1]\n",
    "        # X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "        X = batch_X.to(device)\n",
    "        y = batch_y.to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # # Standard training except we pass in y_input and tgt_mask\n",
    "        # pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        # pred = pred.permute(1, 2, 0)      \n",
    "        # loss = loss_fn(pred, y_expected)\n",
    "        \n",
    "        \n",
    "        y_i, y_j, y_edge_type, y_i_size, y_j_size = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # reshape (b, s, f) -> (b, f, s)\n",
    "        y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "        y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "        y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "        y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "        y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "        \n",
    "        print(y_i.shape)\n",
    "        print(y_j.shape)\n",
    "        print(y_expected.shape)\n",
    "        print(y_expected[..., 0].shape)\n",
    "\n",
    "        # loss\n",
    "        loss_i = compute_loss(\n",
    "                y_i, y_expected[..., 0], loss_fn, tgt_mask)\n",
    "        loss_j = compute_loss(\n",
    "                y_j, y_expected[..., 1], loss_fn, tgt_mask)\n",
    "        loss_edge_type = compute_loss(\n",
    "                y_edge_type, y_expected[..., 2], loss_fn, tgt_mask)\n",
    "        loss_i_size = compute_loss(\n",
    "                y_i_size,  y_expected[..., 3], loss_fn, tgt_mask)\n",
    "        loss_j_size = compute_loss(\n",
    "                y_j_size, y_expected[..., 4], loss_fn, tgt_mask)\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        batch_loss = (loss_i + loss_j + loss_edge_type + loss_i_size + loss_j_size) / 5\n",
    "        total_loss += batch_loss.detach().item()\n",
    "        # total_loss_i += loss_i.detach().item()\n",
    "\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "#             # Standard training except we pass in y_input and src_mask\n",
    "#             pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "#             # Permute pred to have batch size first again\n",
    "#             pred = pred.permute(1, 2, 0)      \n",
    "#             loss = loss_fn(pred, y_expected)\n",
    "#             total_loss += loss.detach().item()\n",
    "            \n",
    "            \n",
    "            y_i, y_j, y_edge_type, y_i_size, y_j_size = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # reshape (b, s, f) -> (b, f, s)\n",
    "            y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "            y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "            y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "            y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "            y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "            \n",
    "\n",
    "            # loss\n",
    "            loss_i = compute_loss(\n",
    "                    y_i, y_expected[..., 0], loss_fn, tgt_mask)\n",
    "            loss_j = compute_loss(\n",
    "                    y_j, y_expected[..., 1], loss_fn, tgt_mask)\n",
    "            loss_edge_type = compute_loss(\n",
    "                    y_edge_type, y_expected[..., 2], loss_fn, tgt_mask)\n",
    "            loss_i_size = compute_loss(\n",
    "                    y_i_size,  y_expected[..., 3], loss_fn, tgt_mask)\n",
    "            loss_j_size = compute_loss(\n",
    "                    y_j_size, y_expected[..., 4], loss_fn, tgt_mask)\n",
    "\n",
    "            batch_loss = (loss_i + loss_j + loss_edge_type + loss_i_size + loss_j_size) / 5\n",
    "            total_loss += batch_loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1b5e44fc-974f-4521-adbc-3b1e4bdbe401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5d63502e-0558-41a8-b2f5-443c1f789317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>: [39, 39, 7, 9, 9]\n",
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "torch.Size([150, 39, 32])\n",
      "torch.Size([150, 39, 32])\n",
      "torch.Size([32, 150, 5])\n",
      "torch.Size([32, 150])\n",
      "torch.Size([39, 32, 150])\n",
      "torch.Size([32, 150])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (39) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-f7b022b3a996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-252-de8fb670dc60>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, opt, loss_fn, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-e1a64a546bb2>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, opt, loss_fn, dataloader)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         loss_i = compute_loss(\n\u001b[0;32m---> 44\u001b[0;31m                 y_i, y_expected[..., 0], loss_fn, tgt_mask)\n\u001b[0m\u001b[1;32m     45\u001b[0m         loss_j = compute_loss(\n\u001b[1;32m     46\u001b[0m                 y_j, y_expected[..., 1], loss_fn, tgt_mask)\n",
      "\u001b[0;32m<ipython-input-246-50a7a0b0611a>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(predict, target, loss_func, loss_mask)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2262\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (39) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# Hyperparameters\n",
    "# vocab_size = 2932  # Example vocab size\n",
    "# embed_size = 6\n",
    "\n",
    "N_LAYER = 4\n",
    "N_HEAD = 4\n",
    "D_MODEL = 256\n",
    "D_FEEDFW = 1024\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_seq_length = 150\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "# model = AutoregressiveTransformer(vocab_size, embed_size, num_layers, num_heads, hidden_size, ff_size, dropout_rate)\n",
    "model = AutoregressiveTransformer(n_token).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model, optimizer, loss_fn, dataloader_train, dataloader_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59adc5-9fe0-4e34-a7cb-eeab50074130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f05a5-8fd4-4984-86c6-b970de3812c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmugen",
   "language": "python",
   "name": "graphmugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
