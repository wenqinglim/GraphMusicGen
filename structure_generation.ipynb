{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31060737-6028-46dc-aa2a-133659cfbdae",
   "metadata": {},
   "source": [
    "# Train a model to generate structure of a piece (MELONS-inspired)\n",
    "\n",
    "1. Read structure dataset from POP909_structure\n",
    "2. Pre-process str into graph format\n",
    "3. Setup transformer model\n",
    "4. Train-val split, data loader\n",
    "5. Evaluate model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b181400b-6fb3-463b-966d-9ea578a735f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccf9ab62-b7d7-4a94-9fdb-1d2c3213741b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_path = \"POP909_structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d754b377-807f-495b-9476-c15a0ee36cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for folder in os.listdir(structure_path):\n",
    "    try:\n",
    "        f = open(f\"{structure_path}/{folder}/human_label1.txt\", \"r\")\n",
    "        # print(f.read())\n",
    "        labels.append(f.read())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e56d4ea-6cce-430c-9881-7191eb709955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8A8A8B8C4C4b4b4x2A8B8C4C4C4C4X1o1\\n',\n",
       " 'i4A4A4B4B4C4C4C4D4x4B4B4C4C4C4D4X3\\n',\n",
       " 'i4A8A8B4C4b5x1b5A8B4x1C4C4\\n',\n",
       " 'i18A8A8A9x14B8A10B8A10o4\\n',\n",
       " 'X4b4A8B12b4A8B12b4b4B12o4\\n']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af654142-478d-4553-b81f-80db89181124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_string(s):\n",
    "    # This regex pattern matches a letter followed by one or more digits\n",
    "    pattern = re.compile(r'[a-zA-Z]\\d+')\n",
    "    # Find all matches in the string\n",
    "    matches = pattern.findall(s)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4269dc3a-ae35-4536-a820-94429e7e0598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i18', 'A8', 'A8', 'A9', 'x14', 'B8', 'A10', 'B8', 'A10', 'o4']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_string(labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ff696da-721c-412e-83d6-a33abe2dac0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def split_into_pairs(input_string):\n",
    "#     # Initialize an empty list to hold the pairs\n",
    "#     pairs = []\n",
    "    \n",
    "#     # Iterate over the string in steps of 2\n",
    "#     for i in range(0, len(input_string), 2):\n",
    "#         next_pair = input_string[i:i+2]\n",
    "#         if next_pair != \"\\n\":\n",
    "#             # Append the substring of the next two characters to the list\n",
    "#             pairs.append(next_pair)\n",
    "    \n",
    "#     return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "082857cf-4f2f-4752-9277-63be87d37d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i4', 'A4', 'A4', 'B9', 'b4', 'A4', 'B9', 'b4', 'B9', 'X5', 'o2']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_pairs(\"i4A4A4B9b4A4B9b4B9X5o2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "51f9ed7a-de88-430d-9841-bc9424cd349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_phrases = []\n",
    "for label in labels:\n",
    "    all_phrases.append(split_string(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "32ef2418-1d51-47d2-8f52-c38c1e83798f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i8',\n",
       "  'A8',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'b4',\n",
       "  'b4',\n",
       "  'x2',\n",
       "  'A8',\n",
       "  'B8',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'X1',\n",
       "  'o1'],\n",
       " ['i4',\n",
       "  'A4',\n",
       "  'A4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'x4',\n",
       "  'B4',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'C4',\n",
       "  'D4',\n",
       "  'X3'],\n",
       " ['i4',\n",
       "  'A8',\n",
       "  'A8',\n",
       "  'B4',\n",
       "  'C4',\n",
       "  'b5',\n",
       "  'x1',\n",
       "  'b5',\n",
       "  'A8',\n",
       "  'B4',\n",
       "  'x1',\n",
       "  'C4',\n",
       "  'C4']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe7daf0-5a56-4074-8520-567520056d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_phrase_edge_type(prev_phrase, curr_phrase, prev_phrase_idx, curr_phrase_idx):\n",
    "    \"\"\"\n",
    "    Edge types:\n",
    "    0: Intro to Any\n",
    "    1: Any to Outro\n",
    "    2: Repeated phrase\n",
    "    3: Melody to Melody\n",
    "    4: Melody to Non-Melody\n",
    "    5: Non-Melody to Melody\n",
    "    6: Non-Melody to Non-Melody\n",
    "    \"\"\"\n",
    "    # print(prev_phrase_idx, curr_phrase_idx)\n",
    "    \n",
    "    prev_phrase_type = prev_phrase[0]\n",
    "    curr_phrase_type = curr_phrase[0]\n",
    "    \n",
    "    if prev_phrase == curr_phrase:\n",
    "            return 2\n",
    "    \n",
    "    if prev_phrase_idx + 1 == curr_phrase_idx:\n",
    "        # print(prev_phrase_type)\n",
    "    \n",
    "        if prev_phrase_type == \"i\":\n",
    "            return 0\n",
    "        elif curr_phrase_type == \"o\":\n",
    "            return 1\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.isupper():\n",
    "            return 3\n",
    "        elif prev_phrase_type.isupper() & curr_phrase_type.islower():\n",
    "            return 4\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.isupper():\n",
    "            return 5\n",
    "        elif prev_phrase_type.islower() & curr_phrase_type.islower():\n",
    "            return 6\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "952d7a6c-f667-43f0-a2df-a32539748a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_phrase_edge_type(\"B4\", \"B9\", 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fad01669-06ee-49b9-a0dc-9e563baa9618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i8',\n",
       " 'A8',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'b4',\n",
       " 'b4',\n",
       " 'x2',\n",
       " 'A8',\n",
       " 'B8',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'C4',\n",
       " 'X1',\n",
       " 'o1']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "20f6e135-ceef-4b6b-b9cc-696341e3ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(phrases):\n",
    "    # Create sequence of edges from phrase, where each item is a tuple (i, j, edge type, num bars in i, num bars in j)\n",
    "    seq = []\n",
    "    for i, phrase_from in enumerate(phrases):\n",
    "        for j, phrase_to in enumerate(phrases[i+1:]):\n",
    "            phrase_to_idx = j+i+1\n",
    "            edge_type = get_phrase_edge_type(phrase_from, phrase_to, i, phrase_to_idx)\n",
    "            if edge_type is not None:\n",
    "                phrase_from_len = int(phrase_from[1])\n",
    "                phrase_to_len = int(phrase_to[1])\n",
    "                seq.append((i, phrase_to_idx, edge_type, phrase_from_len, phrase_to_len))\n",
    "    \n",
    "    # Append END token\n",
    "    seq.append((len(phrases)-1, len(phrases), 7, 0, 0))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d27d96fc-78de-4fd5-a714-477437e814a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for phrases in all_phrases:\n",
    "    # print(phrases)\n",
    "    seqs.append(create_sequence(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "08eda1d0-6b73-417b-83cb-d53f4eec6e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 8, 8),\n",
       " (1, 2, 2, 8, 8),\n",
       " (1, 9, 2, 8, 8),\n",
       " (2, 3, 3, 8, 8),\n",
       " (2, 9, 2, 8, 8),\n",
       " (3, 4, 3, 8, 4),\n",
       " (3, 10, 2, 8, 8),\n",
       " (4, 5, 2, 4, 4),\n",
       " (4, 11, 2, 4, 4),\n",
       " (4, 12, 2, 4, 4),\n",
       " (4, 13, 2, 4, 4),\n",
       " (4, 14, 2, 4, 4),\n",
       " (5, 6, 4, 4, 4),\n",
       " (5, 11, 2, 4, 4),\n",
       " (5, 12, 2, 4, 4),\n",
       " (5, 13, 2, 4, 4),\n",
       " (5, 14, 2, 4, 4),\n",
       " (6, 7, 2, 4, 4),\n",
       " (7, 8, 6, 4, 2),\n",
       " (8, 9, 5, 2, 8),\n",
       " (9, 10, 3, 8, 8),\n",
       " (10, 11, 3, 8, 4),\n",
       " (11, 12, 2, 4, 4),\n",
       " (11, 13, 2, 4, 4),\n",
       " (11, 14, 2, 4, 4),\n",
       " (12, 13, 2, 4, 4),\n",
       " (12, 14, 2, 4, 4),\n",
       " (13, 14, 2, 4, 4),\n",
       " (14, 15, 3, 4, 1),\n",
       " (15, 16, 1, 1, 1),\n",
       " (16, 17, 7, 0, 0)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c0b370f-1c45-43ef-a8ac-69dcd642bfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 4, 4),\n",
       " (1, 2, 2, 4, 4),\n",
       " (1, 5, 2, 4, 4),\n",
       " (2, 3, 3, 4, 9),\n",
       " (2, 5, 2, 4, 4),\n",
       " (3, 4, 4, 9, 4),\n",
       " (3, 6, 2, 9, 9),\n",
       " (3, 8, 2, 9, 9),\n",
       " (4, 5, 5, 4, 4),\n",
       " (4, 7, 2, 4, 4),\n",
       " (5, 6, 3, 4, 9),\n",
       " (6, 7, 4, 9, 4),\n",
       " (6, 8, 2, 9, 9),\n",
       " (7, 8, 5, 4, 9),\n",
       " (8, 9, 3, 9, 5),\n",
       " (9, 10, 1, 5, 2)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "036d2413-5fb2-4f98-9af6-61b8422e531b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add END token\n",
    "seq.append((10, 11, 7, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2f52e5a6-6b3c-427d-a8c9-ce95cc2cca8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_input_output_pairs(seq):\n",
    "    # Create input and output pairs\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "    for idx in range(1, len(seq)):\n",
    "        input_seqs.append(seq[:idx])\n",
    "        output_seqs.append(seq[idx:])\n",
    "    return input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ced290f2-dd22-4c1d-aacf-2cbcf6859837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for seq in seqs:\n",
    "    input_seqs, output_seqs = create_input_output_pairs(seq)\n",
    "    inputs.append(input_seqs)\n",
    "    outputs.append(output_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bb06f913-0b29-4af9-ad75-e0ecc42576dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0, 8, 8), (1, 2, 2, 8, 8), (1, 9, 2, 8, 8), (2, 3, 3, 8, 8)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "679d6ff9-0367-4cac-a44f-a073730ca171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 9, 2, 8, 8),\n",
       " (3, 4, 3, 8, 4),\n",
       " (3, 10, 2, 8, 8),\n",
       " (4, 5, 2, 4, 4),\n",
       " (4, 11, 2, 4, 4),\n",
       " (4, 12, 2, 4, 4),\n",
       " (4, 13, 2, 4, 4),\n",
       " (4, 14, 2, 4, 4),\n",
       " (5, 6, 4, 4, 4),\n",
       " (5, 11, 2, 4, 4),\n",
       " (5, 12, 2, 4, 4),\n",
       " (5, 13, 2, 4, 4),\n",
       " (5, 14, 2, 4, 4),\n",
       " (6, 7, 2, 4, 4),\n",
       " (7, 8, 6, 4, 2),\n",
       " (8, 9, 5, 2, 8),\n",
       " (9, 10, 3, 8, 8),\n",
       " (10, 11, 3, 8, 4),\n",
       " (11, 12, 2, 4, 4),\n",
       " (11, 13, 2, 4, 4),\n",
       " (11, 14, 2, 4, 4),\n",
       " (12, 13, 2, 4, 4),\n",
       " (12, 14, 2, 4, 4),\n",
       " (13, 14, 2, 4, 4),\n",
       " (14, 15, 3, 4, 1),\n",
       " (15, 16, 1, 1, 1),\n",
       " (16, 17, 7, 0, 0)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "87603922-e6a1-4a8e-9fe4-a0741963bc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_flat = [seq for seqs in inputs for seq in seqs]\n",
    "outputs_flat = [seq for seqs in outputs for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7980d661-dd99-4ff3-9340-8fedf098e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19982"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a2ba826d-6e76-44cb-96a1-fa230dd4e738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in inputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fc2405c8-9a64-454e-9c5e-5ff62f42ca70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19982"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "811ba1ac-ad2e-4b39-a50c-853b38465a66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in outputs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8f74d743-742b-428d-9090-aeaf7f9e1879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6e0752f6-0d88-4cc9-8416-ec14bc5deb21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2932"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([token for tokens in inputs_flat for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5c23f50c-543e-43df-bf0a-a60f114989b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2897"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([token for tokens in outputs_flat for token in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150ab47-96b6-42b7-97a0-1c42e4a0eec5",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b5849d65-260e-407d-afc6-407dd399785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def sequences_to_tensor(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Convert a list of sequences of different lengths to a padded tensor.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of list of tuples): List of sequences where each sequence is a list of tuples.\n",
    "        padding_value (int, optional): Value to use for padding. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_length, tuple_length)\n",
    "    \"\"\"\n",
    "    # Convert each sequence to a tensor\n",
    "    tensor_sequences = [torch.tensor(seq) for seq in sequences]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f74f9544-f310-4ec0-ab5a-fb21e63eb179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input shape: torch.Size([19982, 150, 5])\n",
      "Padded output shape: torch.Size([19982, 150, 5])\n"
     ]
    }
   ],
   "source": [
    "padded_input = sequences_to_tensor(inputs_flat, padding_value=0)\n",
    "padded_output = sequences_to_tensor(outputs_flat, padding_value=0)\n",
    "\n",
    "print(\"Padded input shape:\", padded_input.shape)\n",
    "print(\"Padded output shape:\", padded_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "79cff4a2-fc97-4370-a33c-028224190555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the custom dataset\n",
    "class TupleSequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, output_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.input_sequences[idx], self.output_sequences[idx]]\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TupleSequenceDataset(padded_input, padded_output)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(input_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd777-1572-428c-a42d-0968bc2d287d",
   "metadata": {},
   "source": [
    "## Autoregression transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "678c3f45-6f55-420b-aae3-ff12a76a8e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# from fast_transformers.builders import TransformerEncoderBuilder\n",
    "# from fast_transformers.builders import RecurrentEncoderBuilder\n",
    "# from fast_transformers.masking import TriangularCausalMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "576dc225-92c2-4b10-bc10-628ecbe7868b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/YatingMusic/compound-word-transformer/blob/main/workspace/uncond/cp-linear/main-cp.py\n",
    "# https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "# https://gist.github.com/danimelchor/bcad4d7f79b98464c4d4481d62d27622\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, n_token, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(n_token, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=20000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "\n",
    "class AutoregressiveTransformer(nn.Module):\n",
    "    def __init__(self, n_token):\n",
    "        super(AutoregressiveTransformer, self).__init__()\n",
    "        \n",
    "        # --- params config --- #\n",
    "        self.n_token = n_token   \n",
    "        self.d_model = D_MODEL \n",
    "        self.n_layer = N_LAYER #\n",
    "        self.dropout = 0.1\n",
    "        self.n_head = N_HEAD #\n",
    "        self.d_head = D_MODEL // N_HEAD\n",
    "        self.d_inner = 1024\n",
    "        self.loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.emb_sizes = [128, 128, 32, 16, 16]\n",
    "        \n",
    "        \n",
    "        # --- modules config --- #\n",
    "        # embeddings\n",
    "        print('>>>>>:', self.n_token)\n",
    "        self.emb_i = Embeddings(self.n_token[0], self.emb_sizes[0])\n",
    "        self.emb_j = Embeddings(self.n_token[1], self.emb_sizes[1])\n",
    "        self.emb_edge_type = Embeddings(self.n_token[2], self.emb_sizes[2])\n",
    "        self.emb_i_size = Embeddings(self.n_token[3], self.emb_sizes[3])\n",
    "        self.emb_j_size = Embeddings(self.n_token[4], self.emb_sizes[4])\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout)\n",
    "\n",
    "        # linear \n",
    "        self.in_linear = nn.Linear(np.sum(self.emb_sizes), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        if is_training:\n",
    "            # encoder (training)\n",
    "            self.transformer_encoder = TransformerEncoderBuilder.from_kwargs(\n",
    "                n_layers=self.n_layer,\n",
    "                n_heads=self.n_head,\n",
    "                query_dimensions=self.d_model//self.n_head,\n",
    "                value_dimensions=self.d_model//self.n_head,\n",
    "                feed_forward_dimensions=2048,\n",
    "                activation='gelu',\n",
    "                dropout=0.1,\n",
    "                attention_type=\"causal-linear\",\n",
    "            ).get()\n",
    "        else:\n",
    "            # encoder (inference)\n",
    "            print(' [o] using RNN backend.')\n",
    "            self.transformer_encoder = RecurrentEncoderBuilder.from_kwargs(\n",
    "                n_layers=self.n_layer,\n",
    "                n_heads=self.n_head,\n",
    "                query_dimensions=self.d_model//self.n_head,\n",
    "                value_dimensions=self.d_model//self.n_head,\n",
    "                feed_forward_dimensions=2048,\n",
    "                activation='gelu',\n",
    "                dropout=0.1,\n",
    "                attention_type=\"causal-linear\",\n",
    "            ).get()\n",
    "\n",
    "        # # blend with type\n",
    "        # self.project_concat_type = nn.Linear(self.d_model + 32, self.d_model)\n",
    "\n",
    "        # individual output\n",
    "        self.proj_i    = nn.Linear(self.d_model, self.n_token[0])        \n",
    "        self.proj_j    = nn.Linear(self.d_model, self.n_token[1])\n",
    "        self.proj_edge_type  = nn.Linear(self.d_model, self.n_token[2])\n",
    "        self.proj_i_size     = nn.Linear(self.d_model, self.n_token[3])\n",
    "        self.proj_j_size    = nn.Linear(self.d_model, self.n_token[4])\n",
    "        \n",
    "    def compute_loss(self, predict, target, loss_mask):\n",
    "        loss = self.loss_func(predict, target)\n",
    "        loss = loss * loss_mask\n",
    "        loss = torch.sum(loss) / torch.sum(loss_mask)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, x, target, loss_mask):\n",
    "        h, y_type  = self.forward_hidden(x)\n",
    "        y_i, y_j, y_edge_type, y_i_size, y_j_size = self.forward_output(h, target)\n",
    "         \n",
    "        # reshape (b, s, f) -> (b, f, s)\n",
    "        y_i = y_i[:, ...].permute(0, 2, 1)\n",
    "        y_j = y_j[:, ...].permute(0, 2, 1)\n",
    "        y_edge_type = y_edge_type[:, ...].permute(0, 2, 1)\n",
    "        y_i_size = y_i_size[:, ...].permute(0, 2, 1)\n",
    "        y_j_size = y_j_size[:, ...].permute(0, 2, 1)\n",
    "        \n",
    "        # loss\n",
    "        loss_i = self.compute_loss(\n",
    "                y_i, target[..., 0], loss_mask)\n",
    "        loss_j = self.compute_loss(\n",
    "                y_j, target[..., 1], loss_mask)\n",
    "        loss_edge_type = self.compute_loss(\n",
    "                y_edge_type, target[..., 2], loss_mask)\n",
    "        loss_i_size = self.compute_loss(\n",
    "                y_i_size,  target[..., 3], loss_mask)\n",
    "        loss_j_size = self.compute_loss(\n",
    "                y_j_size, target[..., 4], loss_mask)\n",
    "\n",
    "        return loss_i, loss_j, loss_edge_type, loss_i_size, loss_j_size\n",
    "    \n",
    "    \n",
    "    def forward_hidden(self, x, memory=None, is_training=True):\n",
    "        '''\n",
    "        linear transformer: b x s x f\n",
    "        x.shape=(bs, nf)\n",
    "        '''\n",
    "    \n",
    "        # embeddings\n",
    "        emb_i =    self.emb_i(x[..., 0])\n",
    "        emb_j =    self.emb_j(x[..., 1])\n",
    "        emb_edge_type =  self.emb_edge_type(x[..., 2])\n",
    "        emb_i_size =     self.emb_i_size(x[..., 3])\n",
    "        emb_j_size =    self.emb_j_size(x[..., 4])\n",
    "\n",
    "        embs = torch.cat(\n",
    "            [\n",
    "                emb_i,\n",
    "                emb_j,\n",
    "                emb_edge_type,\n",
    "                emb_i_size,\n",
    "                emb_j_size,\n",
    "            ], dim=-1)\n",
    "\n",
    "        emb_linear = self.in_linear(embs)\n",
    "        pos_emb = self.pos_emb(emb_linear)\n",
    "    \n",
    "        # transformer\n",
    "        if is_training:\n",
    "            # mask\n",
    "            attn_mask = TriangularCausalMask(pos_emb.size(1), device=x.device)\n",
    "            h = self.transformer_encoder(pos_emb, attn_mask) # y: b x s x d_model\n",
    "\n",
    "            # # project type\n",
    "            # y_type = self.proj_type(h)\n",
    "            # return h, y_type\n",
    "            return h\n",
    "        else:\n",
    "            pos_emb = pos_emb.squeeze(0)\n",
    "            h, memory = self.transformer_encoder(pos_emb, memory=memory) # y: s x d_model\n",
    "            \n",
    "            # # project type\n",
    "            # y_type = self.proj_type(h)\n",
    "            # return h, y_type, memory\n",
    "            return h, memory\n",
    "\n",
    "    def forward_output(self, h, y):\n",
    "        '''\n",
    "        for training\n",
    "        '''\n",
    "        # tf_skip_type = self.word_emb_type(y[..., 3])\n",
    "\n",
    "        # project other\n",
    "        # y_concat_type = torch.cat([h, tf_skip_type], dim=-1)\n",
    "        # y_  = self.project_concat_type(y_concat_type)\n",
    "\n",
    "        y_i    = self.proj_i(h)\n",
    "        y_j    = self.proj_j(h)\n",
    "        y_edge_type  = self.proj_edge_type(h)\n",
    "        y_i_size    = self.proj_i_size(h)\n",
    "        y_j_size = self.proj_j_size(h)\n",
    "\n",
    "        return  y_i, y_j, y_edge_type, y_i_size, y_j_size\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "600854aa-ffca-4fd9-b766-3110e1dc7cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 6])\n",
      "torch.Size([150, 1])\n",
      "torch.Size([3])\n",
      "torch.Size([150, 3])\n",
      "torch.Size([150, 3])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-c63e8681e65e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Initialize the model, optimizer, and loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoregressiveTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-e398708d206d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embed_size, num_layers, num_heads, hidden_size, ff_size, dropout_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mff_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Implementation of Feedforward model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GraphMusicGen/envs/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# vocab_size = 2932  # Example vocab size\n",
    "# embed_size = 6\n",
    "\n",
    "N_LAYER = 4\n",
    "N_HEAD = 4\n",
    "D_MODEL = 256\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_seq_length = 150\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = AutoregressiveTransformer(vocab_size, embed_size, num_layers, num_heads, hidden_size, ff_size, dropout_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e7be5fd0-a0b1-432f-b4fe-d2570fbc11ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO: Figure out shapes!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466ab66-47f3-4132-957d-7e28e12faed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input sequence\n",
    "input_sequence = torch.randint(0, vocab_size, (32, max_seq_length))  # batch size of 32 for illustration\n",
    "src_mask = model.generate_square_subsequent_mask(max_seq_length)\n",
    "\n",
    "# Training loop (simplified)\n",
    "model.train()\n",
    "for epoch in range(10):  # number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_sequence, src_mask)\n",
    "    loss = criterion(output.view(-1, vocab_size), input_sequence.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f6bce-a686-4ec6-ab74-77e22874ad06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmugen",
   "language": "python",
   "name": "graphmugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
